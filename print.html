<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>itsusinn-books</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="cpu-memory.html"><strong aria-hidden="true">1.</strong> 每个程序员都应该了解的内存知识</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="cpu-memory/1-introduction.html"><strong aria-hidden="true">1.1.</strong> 简介</a></li><li class="chapter-item expanded "><a href="cpu-memory/2-commodity-hardware-today.html"><strong aria-hidden="true">1.2.</strong> 商业硬件现状</a></li><li class="chapter-item expanded "><a href="cpu-memory/3-cpu-caches.html"><strong aria-hidden="true">1.3.</strong> CPU高速缓存</a></li><li class="chapter-item expanded "><a href="cpu-memory/4-virtual-memory.html"><strong aria-hidden="true">1.4.</strong> 虚拟缓存</a></li><li class="chapter-item expanded "><a href="cpu-memory/5-numa-support.html"><strong aria-hidden="true">1.5.</strong> NUMA系统</a></li><li class="chapter-item expanded "><a href="cpu-memory/6-bypassing-the-cache.html"><strong aria-hidden="true">1.6.</strong> 高速缓存的优化</a></li><li class="chapter-item expanded "><a href="cpu-memory/7-multi-thread-optimizations.html"><strong aria-hidden="true">1.7.</strong> 多线程的优化</a></li><li class="chapter-item expanded "><a href="cpu-memory/8-memory-performance-tools.html"><strong aria-hidden="true">1.8.</strong> 内存性能工具</a></li><li class="chapter-item expanded "><a href="cpu-memory/9-upcoming-technology.html"><strong aria-hidden="true">1.9.</strong> 未来的技术</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">itsusinn-books</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="每个程序员都应该了解的内存知识"><a class="header" href="#每个程序员都应该了解的内存知识">每个程序员都应该了解的内存知识</a></h1>
<blockquote>
<p>转载地址：</p>
<p>https://www.oschina.net/translate/what-every-programmer-should-know-about-memory-part1</p>
<p>https://www.oschina.net/translate/what-every-programmer-should-know-about-cpu-cache-part2</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h2 id="1-简介"><a class="header" href="#1-简介"><strong>1 简介</strong></a></h2>
<p>早期计算机比现在更为简单。系统的各种组件例如CPU，内存，大容量存储器和网口，由于被共同开发因而有非常均衡的表现。例如，内存和网口并不比CPU在提供数据的时候更（特别的）快。 </p>
<p>曾今计算机稳定的基本结构悄然改变，硬件开发人员开始致力于优化单个子系统。于是电脑一些组件的性能大大的落后因而成为了瓶颈。由于开销的原因，大容量存储器和内存子系统相对于其他组件来说改善得更为缓慢。 </p>
<p>大容量存储的性能问题往往靠软件来改善:  操作系统将常用(且最有可能被用)的数据放在主存中，因为后者的速度要快上几个数量级。或者将缓存加入存储设备中，这样就可以在不修改操作系统的前提下提升性能。{然而，为了在使用缓存时保证数据的完整性，仍然要作出一些修改。}这些内容不在本文的谈论范围之内，就不作赘述了。 </p>
<p>而解决内存的瓶颈更为困难，它与大容量存储不同，几乎每种方案都需要对硬件作出修改。目前，这些变更主要有以下这些方式: </p>
<ul>
<li>RAM的硬件设计(速度与并发度) </li>
<li>内存控制器的设计 </li>
<li>CPU缓存 </li>
<li>设备的直接内存访问(DMA) </li>
</ul>
<p>本文主要关心的是CPU缓存和内存控制器的设计。在讨论这些主题的过程中，我们还会研究DMA。不过，我们首先会从当今商用硬件的设计谈起。这有助于我们理解目前在使用内存子系统时可能遇到的问题和限制。我们还会详细介绍RAM的分类，说明为什么会存在这么多不同类型的内存。 </p>
<p>本文不会包括所有内容，也不会包括最终性质的内容。我们的讨论范围仅止于商用硬件，而且只限于其中的一小部分。另外，本文中的许多论题，我们只会点到为止，以达到本文目标为标准。对于这些论题，大家可以阅读其它文档，获得更详细的说明。 </p>
<p>当本文提到操作系统特定的细节和解决方案时，针对的都是Linux。无论何时都不会包含别的操作系统的任何信息，作者无意讨论其他操作系统的情况。如果读者认为他/她不得不使用别的操作系统，那么必须去要求供应商提供其操作系统类似于本文的文档。 </p>
<p>在开始之前最后的一点说明，本文包含大量出现的术语“经常”和别的类似的限定词。这里讨论的技术在现实中存在于很多不同的实现，所以本文只阐述使用得最广泛最主流的版本。在阐述中很少有地方能用到绝对的限定词。 </p>
<h3 id="11文档结构"><a class="header" href="#11文档结构"><strong>1.1文档结构</strong></a></h3>
<p>这个文档主要视为软件开发者而写的。本文不会涉及太多硬件细节，所以喜欢硬件的读者也许不会觉得有用。但是在我们讨论一些有用的细节之前，我们先要描述足够多的背景。 </p>
<p>在这个基础上，本文的第二部分将描述RAM（随机寄存器）。懂得这个部分的内容很好，但是此部分的内容并不是懂得其后内容必须部分。我们会在之后引用不少之前的部分，所以心急的读者可以跳过任何章节来读他们认为有用的部分。 </p>
<p>第三部分会谈到不少关于CPU缓存行为模式的内容。我们会列出一些图标，这样你们不至于觉得太枯燥。第三部分对于理解整个文章非常重要。第四部分将简短的描述虚拟内存是怎么被实现的。这也是你们需要理解全文其他部分的背景知识之一。 </p>
<p>第五部分会提到许多关于Non Uniform Memory Access (NUMA)系统。 </p>
<p>第六部分是本文的中心部分。在这个部分里面，我们将回顾其他许多部分中的信息，并且我们将给阅读本文的程序员许多在各种情况下的编程建议。如果你真的很心急，那么你可以直接阅读第六部分，并且我们建议你在必要的时候回到之前的章节回顾一下必要的背景知识。 </p>
<p>本文的第七部分将介绍一些能够帮助程序员更好的完成任务的工具。即便在彻底理解了某一项技术的情况下，距离彻底理解在非测试环境下的程序还是很遥远的。我们需要借助一些工具。 </p>
<p>第八部分，我们将展望一些在未来我们可能认为好用的科技。 </p>
<p><strong>1.2 反馈问题</strong> </p>
<p>作者会不定期更新本文档。这些更新既包括伴随技术进步而来的更新也包含更改错误。非常欢迎有志于反馈问题的读者发送电子邮件。 </p>
<p><strong>1.3 致谢</strong> </p>
<p>我首先需要感谢Johnray Fuller尤其是Jonathan Corbet，感谢他们将作者的英语转化成为更为规范的形式。Markus Armbruster提供大量本文中对于问题和缩写有价值的建议。 </p>
<p><strong>1.4 关于本文</strong> </p>
<p>本文题目对David Goldberg的经典文献《What Every Computer Scientist Should Know About Floating-Point Arithmetic》[goldberg]表示致敬。Goldberg的论文虽然不普及，但是对于任何有志于严格编程的人都会是一个先决条件。 </p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="2-商用硬件现状"><a class="header" href="#2-商用硬件现状"><strong>2 商用硬件现状</strong></a></h2>
<p>鉴于目前专业硬件正在逐渐淡出，理解商用硬件的现状变得十分重要。现如今，人们更多的采用水平扩展，也就是说，用大量小型、互联的商用计算机代替巨大、超快(但超贵)的系统。原因在于，快速而廉价的网络硬件已经崛起。那些大型的专用系统仍然有一席之地，但已被商用硬件后来居上。2007年，Red  Hat认为，未来构成数据中心的“积木”将会是拥有最多4个插槽的计算机，每个插槽插入一个四核CPU，这些CPU都是超线程的。{超线程使单个处理器核心能同时处理两个以上的任务，只需加入一点点额外硬件}。也就是说，这些数据中心中的标准系统拥有最多64个虚拟处理器。当然可以支持更大的系统，但人们认为4插槽、4核CPU是最佳配置，绝大多数的优化都针对这样的配置。 </p>
<p>在不同商用计算机之间，也存在着巨大的差异。不过，我们关注在主要的差异上，可以涵盖到超过90%以上的硬件。需要注意的是，这些技术上的细节往往日新月异，变化极快，因此大家在阅读的时候也需要注意本文的写作时间。 </p>
<p>这么多年来，个人计算机和小型服务器被标准化到了一个芯片组上，它由两部分组成: 北桥和南桥，见图2.1。 </p>
<blockquote>
<p><img src="cpu-memory/assets/06104430_73iC.png" alt="img" /> </p>
<p>图2.1 北桥和南桥组成的结构 </p>
</blockquote>
<p>CPU通过一条通用总线(前端总线，FSB)连接到北桥。北桥主要包括内存控制器和其它一些组件，内存控制器决定了RAM芯片的类型。不同的类型，包括DRAM、Rambus和SDRAM等等，要求不同的内存控制器。 </p>
<p>为了连通其它系统设备，北桥需要与南桥通信。南桥又叫I/O桥，通过多条不同总线与设备们通信。目前，比较重要的总线有PCI、PCI  Express、SATA和USB总线，除此以外，南桥还支持PATA、IEEE  1394、串行口和并行口等。比较老的系统上有连接北桥的AGP槽。那是由于南北桥间缺乏高速连接而采取的措施。现在的PCI-E都是直接连到南桥的。 </p>
<p>这种结构有一些需要注意的地方: </p>
<ul>
<li>从某个CPU到另一个CPU的数据需要走它与北桥通信的同一条总线。 </li>
<li>与RAM的通信需要经过北桥 </li>
<li>RAM只有一个端口。{本文不会介绍多端口RAM，因为商用硬件不采用这种内存，至少程序员无法访问到。这种内存一般在路由器等专用硬件中采用。} </li>
<li>CPU与南桥设备间的通信需要经过北桥 </li>
</ul>
<p>在上面这种设计中，瓶颈马上出现了。第一个瓶颈与设备对RAM的访问有关。早期，所有设备之间的通信都需要经过CPU，结果严重影响了整个系统的性能。为了解决这个问题，有些设备加入了直接内存访问(DMA)的能力。DMA允许设备在北桥的帮助下，无需CPU的干涉，直接读写RAM。到了今天，所有高性能的设备都可以使用DMA。虽然DMA大大降低了CPU的负担，却占用了北桥的带宽，与CPU形成了争用。</p>
<p>第二个瓶颈来自北桥与RAM间的总线。总线的具体情况与内存的类型有关。在早期的系统上，只有一条总线，因此不能实现并行访问。近期的RAM需要两条独立总线(或者说通道，DDR2就是这么叫的，见图2.8)，可以实现带宽加倍。北桥将内存访问交错地分配到两个通道上。更新的内存技术(如FB-DRAM)甚至加入了更多的通道。 </p>
<p>由于带宽有限，我们需要以一种使延迟最小化的方式来对内存访问进行调度。我们将会看到，处理器的速度比内存要快得多，需要等待内存。如果有多个超线程核心或CPU同时访问内存，等待时间则会更长。对于DMA也是同样。 </p>
<p>除了并发以外，访问模式也会极大地影响内存子系统、特别是多通道内存子系统的性能。关于访问模式，可参见2.2节。 </p>
<p>在一些比较昂贵的系统上，北桥自己不含内存控制器，而是连接到外部的多个内存控制器上(在下例中，共有4个)。 </p>
<blockquote>
<p><img src="cpu-memory/assets/06104430_V9Tz.png" alt="img" /> </p>
<p>图2.2 拥有外部控制器的北桥 </p>
</blockquote>
<p>这种架构的好处在于，多条内存总线的存在，使得总带宽也随之增加了。而且也可以支持更多的内存。通过同时访问不同内存区，还可以降低延时。对于像图2.2中这种多处理器直连北桥的设计来说，尤其有效。而这种架构的局限在于北桥的内部带宽，非常巨大(来自Intel)。{出于完整性的考虑，还需要补充一下，这样的内存控制器布局还可以用于其它用途，比如说「内存RAID」，它可以与热插拔技术一起使用。}</p>
<p>使用外部内存控制器并不是唯一的办法，另一个最近比较流行的方法是将控制器集成到CPU内部，将内存直连到每个CPU。这种架构的走红归功于基于AMD  Opteron处理器的SMP系统。图2.3展示了这种架构。Intel则会从Nehalem处理器开始支持通用系统接口(CSI)，基本上也是类似的思路——集成内存控制器，为每个处理器提供本地内存。 </p>
<blockquote>
<p><img src="cpu-memory/assets/06104431_mINo.png" alt="img" /> </p>
<p>图2.3 集成的内存控制器 </p>
</blockquote>
<p>通过采用这样的架构，系统里有几个处理器，就可以有几个内存库(memory bank)。比如，在4 CPU的计算机上，不需要一个拥有巨大带宽的复杂北桥，就可以实现4倍的内存带宽。另外，将内存控制器集成到CPU内部还有其它一些优点，这里就不赘述了。 </p>
<p>同样也有缺点。首先，系统仍然要让所有内存能被所有处理器所访问，导致内存不再是统一的资源(NUMA即得名于此)。处理器能以正常的速度访问本地内存(连接到该处理器的内存)。但它访问其它处理器的内存时，却需要使用处理器之间的互联通道。比如说，CPU 1如果要访问CPU 2的内存，则需要使用它们之间的互联通道。如果它需要访问CPU 4的内存，那么需要跨越两条互联通道。 </p>
<p>使用互联通道是有代价的。在讨论访问远端内存的代价时，我们用「NUMA因子」这个词。在图2.3中，每个CPU有两个层级:  相邻的CPU，以及两个互联通道外的CPU。在更加复杂的系统中，层级也更多。甚至有些机器有不止一种连接，比如说IBM的x445和SGI的Altix系列。CPU被归入节点，节点内的内存访问时间是一致的，或者只有很小的NUMA因子。而在节点之间的连接代价很大，而且有巨大的NUMA因子。 </p>
<p>目前，已经有商用的NUMA计算机，而且它们在未来应该会扮演更加重要的角色。人们预计，从2008年底开始，每台SMP机器都会使用NUMA。每个在NUMA上运行的程序都应该认识到NUMA的代价。在第5节中，我们将讨论更多的架构，以及Linux内核为这些程序提供的一些技术。 </p>
<p>除了本节中所介绍的技术之外，还有其它一些影响RAM性能的因素。它们无法被软件所左右，所以没有放在这里。如果大家有兴趣，可以在第2.1节中看一下。介绍这些技术，仅仅是因为它们能让我们绘制的RAM技术全图更为完整，或者是可能在大家购买计算机时能够提供一些帮助。 </p>
<p>以下的两节主要介绍一些入门级的硬件知识，同时讨论内存控制器与DRAM芯片间的访问协议。这些知识解释了内存访问的原理，程序员可能会得到一些启发。不过，这部分并不是必读的，心急的读者可以直接跳到第2.2.5节。 </p>
<h3 id="21-ram类型"><a class="header" href="#21-ram类型">2.1 RAM类型</a></h3>
<p>这些年来，出现了许多不同类型的RAM，各有差异，有些甚至有非常巨大的不同。那些很古老的类型已经乏人问津，我们就不仔细研究了。我们主要专注于几类现代RAM，剖开它们的表面，研究一下内核和应用开发人员们可以看到的一些细节。 </p>
<p>第一个有趣的细节是，为什么在同一台机器中有不同的RAM？或者说得更详细一点，为什么既有静态RAM(SRAM  {SRAM还可以表示「同步内存」。})，又有动态RAM(DRAM)。功能相同，前者更快。那么，为什么不全部使用SRAM？答案是，代价。无论在生产还是在使用上，SRAM都比DRAM要贵得多。生产和使用，这两个代价因子都很重要，后者则是越来越重要。为了理解这一点，我们分别看一下SRAM和DRAM一个位的存储的实现过程。 </p>
<p>在本节的余下部分，我们将讨论RAM实现的底层细节。我们将尽量控制细节的层面，比如，在「逻辑的层面」讨论信号，而不是硬件设计师那种层面，因为那毫无必要。 </p>
<h4 id="211-静态ram"><a class="header" href="#211-静态ram">2.1.1 静态RAM</a></h4>
<blockquote>
<p><img src="cpu-memory/assets/06104431_ys3P.png" alt="img" /> </p>
<p>图2.6 6-T静态RAM </p>
</blockquote>
<p>图2.4展示了6晶体管SRAM的一个单元。核心是4个晶体管M1-M4，它们组成两个交叉耦合的反相器。它们有两个稳定的状态，分别代表0和1。只要保持Vdd有电，状态就是稳定的。 </p>
<p>当需要访问单元的状态时，升起字访问线WL。BL和BL上就可以读取状态。如果需要覆盖状态，先将BL和BL设置为期望的值，然后升起WL。由于外部的驱动强于内部的4个晶体管，所以旧状态会被覆盖。 </p>
<p>更多详情，可以参考[sramwiki]。为了下文的讨论，需要注意以下问题: </p>
<p>一个单元需要6个晶体管。也有采用4个晶体管的SRAM，但有缺陷。 </p>
<p>维持状态需要恒定的电源。 </p>
<p>升起WL后立即可以读取状态。信号与其它晶体管控制的信号一样，是直角的(快速在两个状态间变化)。 </p>
<p>状态稳定，不需要刷新循环。 </p>
<p>SRAM也有其它形式，不那么费电，但比较慢。由于我们需要的是快速RAM，因此不在关注范围内。这些较慢的SRAM的主要优点在于接口简单，比动态RAM更容易使用。 </p>
<h4 id="212-动态ram"><a class="header" href="#212-动态ram">2.1.2 动态RAM</a></h4>
<p>动态RAM比静态RAM要简单得多。图2.5展示了一种普通DRAM的结构。它只含有一个晶体管和一个电容器。显然，这种复杂性上的巨大差异意味着功能上的迥异。 </p>
<blockquote>
<p><img src="cpu-memory/assets/06104431_i8Of.png" alt="img" /> </p>
<p>图2.5 1-T动态RAM </p>
</blockquote>
<p>动态RAM的状态是保持在电容器C中。晶体管M用来控制访问。如果要读取状态，升起访问线AL，这时，可能会有电流流到数据线DL上，也可能没有，取决于电容器是否有电。如果要写入状态，先设置DL，然后升起AL一段时间，直到电容器充电或放电完毕。 </p>
<p>动态RAM的设计有几个复杂的地方。由于读取状态时需要对电容器放电，所以这一过程不能无限重复，不得不在某个点上对它重新充电。 </p>
<p>更糟糕的是，为了容纳大量单元(现在一般在单个芯片上容纳10的9次方以上的RAM单元)，电容器的容量必须很小(0.000000000000001法拉以下)。这样，完整充电后大约持有几万个电子。即使电容器的电阻很大(若干兆欧姆)，仍然只需很短的时间就会耗光电荷，称为「泄漏」。 </p>
<p>这种泄露就是现在的大部分DRAM芯片每隔64ms就必须进行一次刷新的原因。在刷新期间，对于该芯片的访问是不可能的，这甚至会造成半数任务的延宕。（相关内容请察看【highperfdram】一章） </p>
<p>这个问题的另一个后果就是无法直接读取芯片单元中的信息，而必须通过信号放大器将0和1两种信号间的电势差增大。 </p>
<p>最后一个问题在于电容器的冲放电是需要时间的，这就导致了信号放大器读取的信号并不是典型的矩形信号。所以当放大器输出信号的时候就需要一个小小的延宕，相关公式如下 </p>
<blockquote>
<p><img src="cpu-memory/assets/06104432_i34C.png" alt="[Formulas]" /> </p>
</blockquote>
<p>这就意味着需要一些时间（时间长短取决于电容C和电阻R）来对电容进行冲放电。另一个负面作用是，信号放大器的输出电流不能立即就作为信号载体使用。图2.6显示了冲放电的曲线，x轴表示的是单位时间下的R*C </p>
<p><img src="cpu-memory/assets/06104432_ePJr.png" alt="img" /> </p>
<p>与静态RAM可以即刻读取数据不同的是，当要读取动态RAM的时候，必须花一点时间来等待电容的冲放电完全。这一点点的时间最终限制了DRAM的速度。 </p>
<p>当然了，这种读取方式也是有好处的。最大的好处在于缩小了规模。一个动态RAM的尺寸是小于静态RAM的。这种规模的减小不单单建立在动态RAM的简单结构之上，也是由于减少了静态RAM的各个单元独立的供电部分。以上也同时导致了动态RAM模具的简单化。 </p>
<p>综上所述，由于不可思议的成本差异，除了一些特殊的硬件（包括路由器什么的）之外，我们的硬件大多是使用DRAM的。这一点深深的影响了咱们这些程序员，后文将会对此进行讨论。在此之前，我们还是先了解下DRAM的更多细节。 </p>
<p><strong>2.1.3 DRAM 访问</strong> </p>
<p>一个程序选择了一个内存位置使用到了一个虚拟地址。处理器转换这个到物理地址最后将内存控制选择RAM芯片匹配了那个地址。在RAM芯片去选择单个内存单元，部分的物理地址以许多地址行的形式被传递。 </p>
<p>它单独地去处理来自于内存控制器的内存位置将完全不切实际：4G的RAM将需要 232 地址行。地址传递DRAM芯片的这种方式首先必须被路由器解析。一个路由器的N多地址行将有2<em>N</em> 输出行。这些输出行能被使用到选择内存单元。使用这个直接方法对于小容量芯片不再是个大问题 </p>
<p>但如果许多的单元生成这种方法不在适合。一个1G的芯片容量（我反感那些SI前缀，对于我一个*giga-bit将总是*230* 而不是*109**字节）将需要30地址行和230 选项行。一个路由器的大小及许多的输入行以指数方式递增当速度不被牺牲时。一个30地址行路由器需要一大堆芯片的真实身份另外路由器也就复杂起来了。更重要的是，传递30脉冲在地址行同步要比仅仅传递15脉冲困难的多。较少列能精确布局相同长度或恰当的时机（现代DRAM类型像DDR3能自动调整时序但这个限制能让他什么都能忍受） </p>
<p><img src="cpu-memory/assets/06104432_tXPs.png" alt="img" /> </p>
<p>图2.7展示了一个很高级别的一个DRAM芯片，DRAM被组织在行和列里。他们能在一行中对奇但DRAM芯片需要一个大的路由器。通过阵列方法设计能被一个路由器和一个半的multiplexer获得{多路复用器（multiplexer）和路由器是一样的，这的multiplexer需要以路由器身份工作当写数据时候。那么从现在开始我们开始讨论其区别.}这在所有方面会是一个大的存储。例如地址linesa0和a1通过行地址选择路由器来选择整个行的芯片的地址列，当读的时候，所有的芯片目录能使其纵列选择路由器可用，依据地址linesa2和a3一个纵列的目录用于数据DRAM芯片的接口类型。这发生了许多次在许多DRAM芯片产生一个总记录数的字节匹配给一个宽范围的数据总线。 </p>
<p>对于写操作，内存单元的数据新值被放到了数据总线，当使用RAS和CAS方式选中内存单元时，数据是存放在内存单元内的。这是一个相当直观的设计，在现实中——很显然——会复杂得多，对于读，需要规范从发出信号到数据在数据总线上变得可读的时延。电容不会像前面章节里面描述的那样立刻自动放电，从内存单元发出的信号是如此这微弱以至于它需要被放大。对于写，必须规范从数据RAS和CAS操作完成后到数据成功的被写入内存单元的时延（当然，电容不会立刻自动充电和放电）。这些时间常量对于DRAM芯片的性能是至关重要的，我们将在下章讨论它。 </p>
<p>另一个关于伸缩性的问题是，用30根地址线连接到每一个RAM芯片是行不通的。芯片的针脚是非常珍贵的资源，以至数据必须能并行传输就并行传输（比如：64位为一组）。内存控制器必须有能力解析每一个RAM模块（RAM芯片集合）。如果因为性能的原因要求并发行访问多个RAM模块并且每个RAM模块需要自己独占的30或多个地址线，那么对于8个RAM模块，仅仅是解析地址，内存控制器就需要240+之多的针脚。 </p>
<p>在很长一段时间里，地址线被复用以解决DRAM芯片的这些次要的可扩展性问题。这意味着地址被转换成两部分。第一部分由地址位a0和a1选择行（如图2.7）。这个选择保持有效直到撤销。然后是第二部分，地址位a2和a3选择列。关键差别在于：只需要两根外部地址线。需要一些很少的线指明RAS和CAS信号有效，但是把地址线的数目减半所付出的代价更小。可是地址复用也带来自身的一些问题。我们将在2.2章中提到。</p>
<p><strong>2.1.4 总结</strong> </p>
<p>如果这章节的内容有些难以应付，不用担心。纵观这章节的重点，有： </p>
<ul>
<li>为什么不是所有的存储器都是SRAM的原因 </li>
<li>存储单元需要单独选择来使用 </li>
<li>地址线数目直接负责存储控制器，主板，DRAM模块和DRAM芯片的成本 </li>
<li>在读或写操作结果之前需要占用一段时间是可行的 </li>
</ul>
<p>接下来的章节会涉及更多的有关访问DRAM存储器的实际操作的细节。我们不会提到更多有关访问SRAM的具体内容，它通常是直接寻址。这里是由于速度和有限的SRAM存储器的尺寸。SRAM现在应用在CPU的高速缓存和芯片，它们的连接件很小而且完全能在CPU设计师的掌控之下。我们以后会讨论到CPU高速缓存这个主题，但我们所需要知道的是SRAM存储单元是有确定的最大速度，这取决于花在SRAM上的艰难的尝试。这速度与CPU核心相比略慢一到两个数量级。</p>
<h3 id="22-dram访问细节"><a class="header" href="#22-dram访问细节">2.2 DRAM访问细节</a></h3>
<p>在上文介绍DRAM的时候，我们已经看到DRAM芯片为了节约资源，对地址进行了复用。而且，访问DRAM单元是需要一些时间的，因为电容器的放电并不是瞬时的。此外，我们还看到，DRAM需要不停地刷新。在这一节里，我们将把这些因素拼合起来，看看它们是如何决定DRAM的访问过程。 </p>
<p>我们将主要关注在当前的科技上，不会再去讨论异步DRAM以及它的各种变体。如果对它感兴趣，可以去参考[highperfdram]及[arstechtwo]。我们也不会讨论Rambus DRAM(RDRAM)，虽然它并不过时，但在系统内存领域应用不广。我们将主要介绍同步DRAM(SDRAM)及其后继者双倍速DRAM(DDR)。 </p>
<p>同步DRAM，顾名思义，是参照一个时间源工作的。由内存控制器提供一个时钟，时钟的频率决定了前端总线(FSB)的速度。FSB是内存控制器提供给DRAM芯片的接口。在我写作本文的时候，FSB已经达到800MHz、1066MHz，甚至1333MHz，并且下一代的1600MHz也已经宣布。但这并不表示时钟频率有这么高。实际上，目前的总线都是双倍或四倍传输的，每个周期传输2次或4次数据。报的越高，卖的越好，所以这些厂商们喜欢把四倍传输的200MHz总线宣传为“有效的”800MHz总线。 </p>
<p>以今天的SDRAM为例，每次数据传输包含64位，即8字节。所以FSB的传输速率应该是有效总线频率乘于8字节(对于4倍传输200MHz总线而言，传输速率为6.4GB/s)。听起来很高，但要知道这只是峰值速率，实际上无法达到的最高速率。我们将会看到，与RAM模块交流的协议有大量时间是处于非工作状态，不进行数据传输。我们必须对这些非工作时间有所了解，并尽量缩短它们，才能获得最佳的性能。 </p>
<h4 id="221-读访问协议"><a class="header" href="#221-读访问协议">2.2.1 读访问协议</a></h4>
<blockquote>
<p><img src="cpu-memory/assets/06104432_0ZDH.png" alt="img" /><br />
图2.8: SDRAM读访问的时序 </p>
</blockquote>
<p>图2.8展示了某个DRAM模块一些连接器上的活动，可分为三个阶段，图上以不同颜色表示。按惯例，时间为从左向右流逝。这里忽略了许多细节，我们只关注时钟频率、RAS与CAS信号、地址总线和数据总线。首先，内存控制器将行地址放在地址总线上，并降低RAS信号，读周期开始。所有信号都在时钟(CLK)的上升沿读取，因此，只要信号在读取的时间点上保持稳定，就算不是标准的方波也没有关系。设置行地址会促使RAM芯片锁住指定的行。 </p>
<p>CAS信号在tRCD(RAS到CAS时延)个时钟周期后发出。内存控制器将列地址放在地址总线上，降低CAS线。这里我们可以看到，地址的两个组成部分是怎么通过同一条总线传输的。 </p>
<p>至此，寻址结束，是时候传输数据了。但RAM芯片任然需要一些准备时间，这个时间称为CAS时延(CL)。在图2.8中CL为2。这个值可大可小，它取决于内存控制器、主板和DRAM模块的质量。CL还可能是半周期。假设CL为2.5，那么数据将在蓝色区域内的第一个下降沿准备就绪。 </p>
<p>既然数据的传输需要这么多的准备工作，仅仅传输一个字显然是太浪费了。因此，DRAM模块允许内存控制指定本次传输多少数据。可以是2、4或8个字。这样，就可以一次填满高速缓存的整条线，而不需要额外的RAS/CAS序列。另外，内存控制器还可以在不重置行选择的前提下发送新的CAS信号。这样，读取或写入连续的地址就可以变得非常快，因为不需要发送RAS信号，也不需要把行置为非激活状态(见下文)。是否要将行保持为“打开”状态是内存控制器判断的事情。让它一直保持打开的话，对真正的应用会有不好的影响(参见[highperfdram])。CAS信号的发送仅与RAM模块的命令速率(Command Rate)有关(常常记为T<em>x，<em>其中</em>x</em>为1或2，高性能的DRAM模块一般为1，表示在每个周期都可以接收新命令)。 </p>
<p>在上图中，SDRAM的每个周期输出一个字的数据。这是第一代的SDRAM。而DDR可以在一个周期中输出两个字。这种做法可以减少传输时间，但无法降低时延。DDR2尽管看上去不同，但在本质上也是相同的做法。对于DDR2，不需要再深入介绍了，我们只需要知道DDR2更快、更便宜、更可靠、更节能(参见[ddrtwo])就足够了。 </p>
<h4 id="222-预充电与激活"><a class="header" href="#222-预充电与激活">2.2.2 预充电与激活</a></h4>
<p>图2.8并不完整，它只画出了访问DRAM的完整循环的一部分。在发送RAS信号之前，必须先把当前锁住的行置为非激活状态，并对新行进行预充电。在这里，我们主要讨论由于显式发送指令而触发以上行为的情况。协议本身作了一些改进，在某些情况下是可以省略这个步骤的，但预充电带来的时延还是会影响整个操作。 </p>
<blockquote>
<p><img src="cpu-memory/assets/06104433_y5nt.png" alt="img" /><br />
图2.9: SDRAM的预充电与激活 </p>
</blockquote>
<p>图2.9显示的是两次CAS信号的时序图。第一次的数据在CL周期后准备就绪。图中的例子里，是在SDRAM上，用两个周期传输了两个字的数据。如果换成DDR的话，则可以传输4个字。 </p>
<p>即使是在一个命令速率为1的DRAM模块上，也无法立即发出预充电命令，而要等数据传输完成。在上图中，即为两个周期。刚好与CL相同，但只是巧合而已。预充电信号并没有专用线，某些实现是用同时降低写使能(WE)线和RAS线的方式来触发。这一组合方式本身没有特殊的意义(参见[micronddr])。 </p>
<p>发出预充电信命令后，还需等待tRP(行预充电时间)个周期之后才能使行被选中。在图2.9中，这个时间(紫色部分)大部分与内存传输的时间(淡蓝色部分)重合。不错。但tRP大于传输时间，因此下一个RAS信号只能等待一个周期。 </p>
<p>如果我们补充完整上图中的时间线，最后会发现下一次数据传输发生在前一次的5个周期之后。这意味着，数据总线的7个周期中只有2个周期才是真正在用的。再用它乘于FSB速度，结果就是，800MHz总线的理论速率6.4GB/s降到了1.8GB/s。真是太糟了。第6节将介绍一些技术，可以帮助我们提高总线有效速率。程序员们也需要尽自己的努力。 </p>
<p>SDRAM还有一些定时值，我们并没有谈到。在图2.9中，预充电命令仅受制于数据传输时间。除此之外，SDRAM模块在RAS信号之后，需要经过一段时间，才能进行预充电(记为tRAS)。它的值很大，一般达到tRP的2到3倍。如果在某个RAS信号之后，只有一个CAS信号，而且数据只传输很少几个周期，那么就有问题了。假设在图2.9中，第一个CAS信号是直接跟在一个RAS信号后免的，而tRAS为8个周期。那么预充电命令还需要被推迟一个周期，因为tRCD、CL和tRP加起来才7个周期。 </p>
<p>DDR模块往往用w-z-y-z-T来表示。例如，2-3-2-8-T1，意思是： </p>
<blockquote>
<p>w 2 CAS时延(CL)<br />
x 3 RAS-to-CAS时延(t  RCD)<br />
y 2 RAS预充电时间(t  RP)<br />
z 8 激活到预充电时间(t  RAS)<br />
T T1 命令速率 </p>
</blockquote>
<p>当然，除以上的参数外，还有许多其它参数影响命令的发送与处理。但以上5个参数已经足以确定模块的性能。 </p>
<p>在解读计算机性能参数时，这些信息可能会派上用场。而在购买计算机时，这些信息就更有用了，因为它们与FSB/SDRAM速度一起，都是决定计算机速度的关键因素。 </p>
<p>喜欢冒险的读者们还可以利用它们来调优系统。有些计算机的BIOS可以让你修改这些参数。SDRAM模块有一些可编程寄存器，可供设置参数。BIOS一般会挑选最佳值。如果RAM模块的质量足够好，我们可以在保持系统稳定的前提下将减小以上某个时延参数。互联网上有大量超频网站提供了相关的文档。不过，这是有风险的，需要大家自己承担，可别怪我没有事先提醒哟。 </p>
<h4 id="223-重充电"><a class="header" href="#223-重充电">2.2.3 重充电</a></h4>
<p>谈到DRAM的访问时，重充电是常常被忽略的一个主题。在2.1.2中曾经介绍，DRAM必须保持刷新。……行在充电时是无法访问的。[highperfdram]的研究发现，“令人吃惊，DRAM刷新对性能有着巨大的影响”。 </p>
<p>根据JEDEC规范，DRAM单元必须保持每64ms刷新一次。对于8192行的DRAM，这意味着内存控制器平均每7.8125µs就需要发出一个刷新命令(在实际情况下，由于刷新命令可以纳入队列，因此这个时间间隔可以更大一些)。刷新命令的调度由内存控制器负责。DRAM模块会记录上一次刷新行的地址，然后在下次刷新请求时自动对这个地址进行递增。 </p>
<p>对于刷新及发出刷新命令的时间点，程序员无法施加影响。但我们在解读性能参数时有必要知道，它也是DRAM生命周期的一个部分。如果系统需要读取某个重要的字，而刚好它所在的行正在刷新，那么处理器将会被延迟很长一段时间。刷新的具体耗时取决于DRAM模块本身。 </p>
<h4 id="224-内存类型"><a class="header" href="#224-内存类型">2.2.4 内存类型</a></h4>
<p>我们有必要花一些时间来了解一下目前流行的内存，以及那些即将流行的内存。首先从SDR(单倍速)SDRAM开始，因为它们是DDR(双倍速)SDRAM的基础。SDR非常简单，内存单元和数据传输率是相等的。 </p>
<blockquote>
<p><img src="cpu-memory/assets/06104433_JWAj.png" alt="img" /><br />
图2.10: SDR SDRAM的操作 </p>
</blockquote>
<p>在图2.10中，DRAM单元阵列能以等同于内存总线的速率输出内容。假设DRAM单元阵列工作在100MHz上，那么总线的数据传输率可以达到100Mb/s。所有组件的频率<em>f</em>保持相同。由于提高频率会导致耗电量增加，所以提高吞吐量需要付出很高的的代价。如果是很大规模的内存阵列，代价会非常巨大。{<em>功率 = 动态电容 x 电压**2</em> <em>x 频率</em>}。而且，提高频率还需要在保持系统稳定的情况下提高电压，这更是一个问题。因此，就有了DDR SDRAM(现在叫DDR1)，它可以在不提高频率的前提下提高吞吐量。 </p>
<blockquote>
<p><img src="cpu-memory/assets/06104433_A6Jf.png" alt="img" /><br />
图2.11 DDR1 SDRAM的操作 </p>
</blockquote>
<p>我们从图2.11上可以看出DDR1与SDR的不同之处，也可以从DDR1的名字里猜到那么几分，DDR1的每个周期可以传输两倍的数据，它的上升沿和下降沿都传输数据。有时又被称为“双泵(double-pumped)”总线。为了在不提升频率的前提下实现双倍传输，DDR引入了一个缓冲区。缓冲区的每条数据线都持有两位。它要求内存单元阵列的数据总线包含两条线。实现的方式很简单，用同一个列地址同时访问两个DRAM单元。对单元阵列的修改也很小。 </p>
<p>SDR DRAM是以频率来命名的(例如，对应于100MHz的称为PC100)。为了让DDR1听上去更好听，营销人员们不得不想了一种新的命名方案。这种新方案中含有DDR模块可支持的传输速率(DDR拥有64位总线): </p>
<blockquote>
<p>100MHz x 64位 x 2 = 1600MB/s </p>
</blockquote>
<p>于是，100MHz频率的DDR模块就被称为PC1600。由于1600 &gt; 100，营销方面的需求得到了满足，听起来非常棒，但实际上仅仅只是提升了两倍而已。{我接受两倍这个事实，但不喜欢类似的数字膨胀戏法。} </p>
<blockquote>
<p><img src="cpu-memory/assets/06104433_pulx.png" alt="img" /><br />
<strong>图2.12: DDR2 SDRAM的操作</strong> </p>
</blockquote>
<p>为了更进一步，DDR2有了更多的创新。在图2.12中，最明显的变化是，总线的频率加倍了。频率的加倍意味着带宽的加倍。如果对单元阵列的频率加倍，显然是不经济的，因此DDR2要求I/O缓冲区在每个时钟周期读取4位。也就是说，DDR2的变化仅在于使I/O缓冲区运行在更高的速度上。这是可行的，而且耗电也不会显著增加。DDR2的命名与DDR1相仿，只是将因子2替换成4(四泵总线)。图2.13显示了目前常用的一些模块的名称。 </p>
<blockquote>
<div class="table-wrapper"><table><thead><tr><th>阵列频率</th><th>总线频率</th><th>数据率</th><th>名称(速率)</th><th>名称  (FSB)</th></tr></thead><tbody>
<tr><td>133MHz</td><td>266MHz</td><td>4,256MB/s</td><td>PC2-4200</td><td>DDR2-533</td></tr>
<tr><td>166MHz</td><td>333MHz</td><td>5,312MB/s</td><td>PC2-5300</td><td>DDR2-667</td></tr>
<tr><td>200MHz</td><td>400MHz</td><td>6,400MB/s</td><td>PC2-6400</td><td>DDR2-800</td></tr>
<tr><td>250MHz</td><td>500MHz</td><td>8,000MB/s</td><td>PC2-8000</td><td>DDR2-1000</td></tr>
<tr><td>266MHz</td><td>533MHz</td><td>8,512MB/s</td><td>PC2-8500</td><td>DDR2-1066</td></tr>
</tbody></table>
</div>
<p>图2.13: DDR2模块名</p>
</blockquote>
<p>在命名方面还有一个拧巴的地方。FSB速度是用有效频率来标记的，即把上升、下降沿均传输数据的因素考虑进去，因此数字被撑大了。所以，拥有266MHz总线的133MHz模块有着533MHz的FSB“频率”。 </p>
<p>DDR3要求更多的改变(这里指真正的DDR3，而不是图形卡中假冒的GDDR3)。电压从1.8V下降到1.5V。由于耗电是与电压的平方成正比，因此可以节约30%的电力。加上管芯(die)的缩小和电气方面的其它进展，DDR3可以在保持相同频率的情况下，降低一半的电力消耗。或者，在保持相同耗电的情况下，达到更高的频率。又或者，在保持相同热量排放的情况下，实现容量的翻番。 </p>
<p>DDR3模块的单元阵列将运行在内部总线的四分之一速度上，DDR3的I/O缓冲区从DDR2的4位提升到8位。见图2.14。 </p>
<blockquote>
<p><img src="cpu-memory/assets/06104433_dVKN.png" alt="img" /><br />
<strong>图2.14: DDR3 SDRAM的操作</strong> </p>
</blockquote>
<p>一开始，DDR3可能会有较高的CAS时延，因为DDR2的技术相比之下更为成熟。由于这个原因，DDR3可能只会用于DDR2无法达到的高频率下，而且带宽比时延更重要的场景。此前，已经有讨论指出，1.3V的DDR3可以达到与DDR2相同的CAS时延。无论如何，更高速度带来的价值都会超过时延增加带来的影响。 </p>
<p>DDR3可能会有一个问题，即在1600Mb/s或更高速率下，每个通道的模块数可能会限制为1。在早期版本中，这一要求是针对所有频率的。我们希望这个要求可以提高一些，否则系统容量将会受到严重的限制。 </p>
<p>图2.15显示了我们预计中各DDR3模块的名称。JEDEC目前同意了前四种。由于Intel的45nm处理器是1600Mb/s的FSB，1866Mb/s可以用于超频市场。随着DDR3的发展，可能会有更多类型加入。 </p>
<blockquote>
<div class="table-wrapper"><table><thead><tr><th>阵列频率</th><th>总线频率</th><th>数据速率</th><th>名称(速率)</th><th>名称  (FSB)</th></tr></thead><tbody>
<tr><td>100MHz</td><td>400MHz</td><td>6,400MB/s</td><td>PC3-6400</td><td>DDR3-800</td></tr>
<tr><td>133MHz</td><td>533MHz</td><td>8,512MB/s</td><td>PC3-8500</td><td>DDR3-1066</td></tr>
<tr><td>166MHz</td><td>667MHz</td><td>10,667MB/s</td><td>PC3-10667</td><td>DDR3-1333</td></tr>
<tr><td>200MHz</td><td>800MHz</td><td>12,800MB/s</td><td>PC3-12800</td><td>DDR3-1600</td></tr>
<tr><td>233MHz</td><td>933MHz</td><td>14,933MB/s</td><td>PC3-14900</td><td>DDR3-1866</td></tr>
</tbody></table>
</div>
<p>图2.15: DDR3模块名</p>
</blockquote>
<p>所有的DDR内存都有一个问题：不断增加的频率使得建立并行数据总线变得十分困难。一个DDR2模块有240根引脚。所有到地址和数据引脚的连线必须被布置得差不多一样长。更大的问题是，如果多于一个DDR模块通过菊花链连接在同一个总线上，每个模块所接收到的信号随着模块的增加会变得越来越扭曲。DDR2规范允许每条总线（又称通道）连接最多两个模块，DDR3在高频率下只允许每个通道连接一个模块。每条总线多达240根引脚使得单个北桥无法以合理的方式驱动两个通道。替代方案是增加外部内存控制器（如图2.2），但这会提高成本。 </p>
<p>这意味着商品主板所搭载的DDR2或DDR3模块数将被限制在最多四条，这严重限制了系统的最大内存容量。即使是老旧的32位IA-32处理器也可以使用64GB内存。即使是家庭对内存的需求也在不断增长，所以，某些事必须开始做了。 </p>
<p>一种解法是，在处理器中加入内存控制器，我们在第2节中曾经介绍过。AMD的Opteron系列和Intel的CSI技术就是采用这种方法。只要我们能把处理器要求的内存连接到处理器上，这种解法就是有效的。如果不能，按照这种思路就会引入NUMA架构，当然同时也会引入它的缺点。而在有些情况下，我们需要其它解法。 </p>
<p>Intel针对大型服务器方面的解法(至少在未来几年)，是被称为全缓冲DRAM(FB-DRAM)的技术。FB-DRAM采用与DDR2相同的器件，因此造价低廉。不同之处在于它们与内存控制器的连接方式。FB-DRAM没有用并行总线，而用了串行总线(Rambus DRAM had this back when, too, 而SATA是PATA的继任者，就像PCI  Express是PCI/AGP的继承人一样)。串行总线可以达到更高的频率，串行化的负面影响，甚至可以增加带宽。使用串行总线后 </p>
<ol>
<li>每个通道可以使用更多的模块。 </li>
<li>每个北桥/内存控制器可以使用更多的通道。 </li>
<li>串行总线是全双工的(两条线)。 </li>
</ol>
<p>FB-DRAM只有69个脚。通过菊花链方式连接多个FB-DRAM也很简单。FB-DRAM规范允许每个通道连接最多8个模块。 </p>
<p>在对比下双通道北桥的连接性，采用FB-DRAM后，北桥可以驱动6个通道，而且脚数更少——6x69对比2x240。每个通道的布线也更为简单，有助于降低主板的成本。 </p>
<p>全双工的并行总线过于昂贵。而换成串行线后，这不再是一个问题，因此串行总线按全双工来设计的，这也意味着，在某些情况下，仅靠这一特性，总线的理论带宽已经翻了一倍。还不止于此。由于FB-DRAM控制器可同时连接6个通道，因此可以利用它来增加某些小内存系统的带宽。对于一个双通道、4模块的DDR2系统，我们可以用一个普通FB-DRAM控制器，用4通道来实现相同的容量。串行总线的实际带宽取决于在FB-DRAM模块中所使用的DDR2(或DDR3)芯片的类型。 </p>
<p>我们可以像这样总结这些优势： </p>
<blockquote>
<p>DDR2 FB-DRAM </p>
</blockquote>
<blockquote>
<div class="table-wrapper"><table><thead><tr><th></th><th>DDR2</th><th>FB-DRAM</th></tr></thead><tbody>
<tr><td>脚</td><td>240</td><td>69</td></tr>
<tr><td>通道</td><td>2</td><td>6</td></tr>
<tr><td>每通道DIMM数</td><td>2</td><td>8</td></tr>
<tr><td>最大内存</td><td>16GB</td><td>192GB</td></tr>
<tr><td>吞吐量</td><td>~10GB/s</td><td>~40GB/s</td></tr>
</tbody></table>
</div></blockquote>
<p>如果在单个通道上使用多个DIMM，会有一些问题。信号在每个DIMM上都会有延迟(尽管很小)，也就是说，延迟是递增的。不过，如果在相同频率和相同容量上进行比较，FB-DRAM总是能快过DDR2及DDR3，因为FB-DRAM只需要在每个通道上使用一个DIMM即可。而如果说到大型内存系统，那么DDR更是没有商用组件的解决方案。</p>
<h4 id="225-结论"><a class="header" href="#225-结论">2.2.5 结论</a></h4>
<p>通过本节，大家应该了解到访问DRAM的过程并不是一个快速的过程。至少与处理器的速度相比，或与处理器访问寄存器及缓存的速度相比，DRAM的访问不算快。大家还需要记住CPU和内存的频率是不同的。Intel Core 2处理器运行在2.933GHz，而1.066GHz FSB有11:1的时钟比率(注:  1.066GHz的总线为四泵总线)。那么，内存总线上延迟一个周期意味着处理器延迟11个周期。绝大多数机器使用的DRAM更慢，因此延迟更大。在后续的章节中，我们需要讨论延迟这个问题时，请把以上的数字记在心里。 </p>
<p>前文中读命令的时序图表明，DRAM模块可以支持高速数据传输。每个完整行可以被毫无延迟地传输。数据总线可以100%被占。对DDR而言，意味着每个周期传输2个64位字。对于DDR2-800模块和双通道而言，意味着12.8GB/s的速率。 </p>
<p>但是，除非是特殊设计，DRAM的访问并不总是串行的。访问不连续的内存区意味着需要预充电和RAS信号。于是，各种速度开始慢下来，DRAM模块急需帮助。预充电的时间越短，数据传输所受的惩罚越小。 </p>
<p>硬件和软件的预取(参见第6.3节)可以在时序中制造更多的重叠区，降低延迟。预取还可以转移内存操作的时间，从而减少争用。我们常常遇到的问题是，在这一轮中生成的数据需要被存储，而下一轮的数据需要被读出来。通过转移读取的时间，读和写就不需要同时发出了。 </p>
<h3 id="23-主存的其它用户"><a class="header" href="#23-主存的其它用户">2.3 主存的其它用户</a></h3>
<p>除了CPU外，系统中还有其它一些组件也可以访问主存。高性能网卡或大规模存储控制器是无法承受通过CPU来传输数据的，它们一般直接对内存进行读写(直接内存访问，DMA)。在图2.1中可以看到，它们可以通过南桥和北桥直接访问内存。另外，其它总线，比如USB等也需要FSB带宽，即使它们并不使用DMA，但南桥仍要通过FSB连接到北桥。 </p>
<p>DMA当然有很大的优点，但也意味着FSB带宽会有更多的竞争。在有大量DMA流量的情况下，CPU在访问内存时必然会有更大的延迟。我们可以用一些硬件来解决这个问题。例如，通过图2.3中的架构，我们可以挑选不受DMA影响的节点，让它们的内存为我们的计算服务。还可以在每个节点上连接一个南桥，将FSB的负荷均匀地分担到每个节点上。除此以外，还有许多其它方法。我们将在第6节中介绍一些技术和编程接口，它们能够帮助我们通过软件的方式改善这个问题。 </p>
<p>最后，还需要提一下某些廉价系统，它们的图形系统没有专用的显存，而是采用主存的一部分作为显存。由于对显存的访问非常频繁(例如，对于1024x768、16bpp、60Hz的显示设置来说，需要95MB/s的数据速率)，而主存并不像显卡上的显存，并没有两个端口，因此这种配置会对系统性能、尤其是时延造成一定的影响。如果大家对系统性能要求比较高，最好不要采用这种配置。这种系统带来的问题超过了本身的价值。人们在购买它们时已经做好了性能不佳的心理准备。 </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="3-cpu-高速缓存"><a class="header" href="#3-cpu-高速缓存">3 CPU 高速缓存</a></h1>
<p>现在的CPU比25年前要精密得多了。在那个年代，CPU的频率与内存总线的频率基本在同一层面上。内存的访问速度仅比寄存器慢那么一点点。但是，这一局面在上世纪90年代被打破了。CPU的频率大大提升，但内存总线的频率与内存芯片的性能却没有得到成比例的提升。并不是因为造不出更快的内存，只是因为太贵了。内存如果要达到目前CPU那样的速度，那么它的造价恐怕要贵上好几个数量级。</p>
<p>如果有两个选项让你选择，一个是速度非常快、但容量很小的内存，一个是速度还算快、但容量很多的内存，如果你的工作集比较大，超过了前一种情况，那么人们总是会选择第二个选项。原因在于辅存(一般为磁盘)的速度。由于工作集超过主存，那么必须用辅存来保存交换出去的那部分数据，而辅存的速度往往要比主存慢上好几个数量级。 </p>
<p>好在这问题也并不全然是非甲即乙的选择。在配置大量DRAM的同时，我们还可以配置少量SRAM。将地址空间的某个部分划给SRAM，剩下的部分划给DRAM。一般来说，SRAM可以当作扩展的寄存器来使用。 </p>
<p>上面的做法看起来似乎可以，但实际上并不可行。首先，将SRAM内存映射到进程的虚拟地址空间就是个非常复杂的工作，而且，在这种做法中，每个进程都需要管理这个SRAM区内存的分配。每个进程可能有大小完全不同的SRAM区，而组成程序的每个模块也需要索取属于自身的SRAM，更引入了额外的同步需求。简而言之，快速内存带来的好处完全被额外的管理开销给抵消了。因此，SRAM是作为CPU自动使用和管理的一个资源，而不是由OS或者用户管理的。在这种模式下，SRAM用来复制保存（或者叫缓存）主内存中有可能即将被CPU使用的数据。这意味着，在较短时间内，CPU很有可能重复运行某一段代码，或者重复使用某部分数据。从代码上看，这意味着CPU执行了一个循环，所以相同的代码一次又一次地执行（空间局部性的绝佳例子）。数据访问也相对局限在一个小的区间内。即使程序使用的物理内存不是相连的，在短期内程序仍然很有可能使用同样的数据（时间局部性）。这个在代码上表现为，程序在一个循环体内调用了入口一个位于另外的物理地址的函数。这个函数可能与当前指令的物理位置相距甚远，但是调用的时间差不大。在数据上表现为，程序使用的内存是有限的（相当于工作集的大小）。但是实际上由于RAM的随机访问特性，程序使用的物理内存并不是连续的。正是由于空间局部性和时间局部性的存在，我们才提炼出今天的CPU缓存概念。</p>
<p>我们先用一个简单的计算来展示一下高速缓存的效率。假设，访问主存需要200个周期，而访问高速缓存需要15个周期。如果使用100个数据元素100次，那么在没有高速缓存的情况下，需要2000000个周期，而在有高速缓存、而且所有数据都已被缓存的情况下，只需要168500个周期。节约了91.5%的时间。 </p>
<p>用作高速缓存的SRAM容量比主存小得多。以我的经验来说，高速缓存的大小一般是主存的千分之一左右(目前一般是4GB主存、4MB缓存)。这一点本身并不是什么问题。只是，计算机一般都会有比较大的主存，因此工作集的大小总是会大于缓存。特别是那些运行多进程的系统，它的工作集大小是所有进程加上内核的总和。 </p>
<p>处理高速缓存大小的限制需要制定一套很好的策略来决定在给定的时间内什么数据应该被缓存。由于不是所有数据的工作集都是在完全相同的时间段内被使用的，我们可以用一些技术手段将需要用到的数据临时替换那些当前并未使用的缓存数据。这种预取将会减少部分访问主存的成本，因为它与程序的执行是异步的。所有的这些技术将会使高速缓存在使用的时候看起来比实际更大。我们将在3.3节讨论这些问题。  我们将在第6章讨论如何让这些技术能很好地帮助程序员，让处理器更高效地工作。</p>
<h3 id="31-高速缓存的位置"><a class="header" href="#31-高速缓存的位置">3.1 高速缓存的位置</a></h3>
<p>在深入介绍高速缓存的技术细节之前，有必要说明一下它在现代计算机系统中所处的位置。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113713_tlJC.png" alt="img" /><br />
<strong>图3.1: 最简单的高速缓存配置图</strong> </p>
</blockquote>
<p>图3.1展示了最简单的高速缓存配置。早期的一些系统就是类似的架构。在这种架构中，CPU核心不再直连到主存。{在一些更早的系统中，高速缓存像CPU与主存一样连到系统总线上。那种做法更像是一种hack，而不是真正的解决方案。}数据的读取和存储都经过高速缓存。CPU核心与高速缓存之间是一条特殊的快速通道。在简化的表示法中，主存与高速缓存都连到系统总线上，这条总线同时还用于与其它组件通信。我们管这条总线叫“FSB”——就是现在称呼它的术语，参见第2.2节。在这一节里，我们将忽略北桥。 </p>
<pre><code>                                                                                 在过去的几十年，经验表明使用了冯诺伊曼结构的  计算机，将用于代码和数据的高速缓存分开是存在巨大优势的。自1993年以来，Intel  并且一直坚持使用独立的代码和数据高速缓存。由于所需的代码和数据的内存区域是几乎相互独立的，这就是为什么独立缓存工作得更完美的原因。近年来，独立缓存的另一个优势慢慢显现出来：常见处理器解码  指令的步骤  是缓慢的，尤其当管线为空的时候，往往会伴随着错误的预测或无法预测的分支的出现，  将高速缓存技术用于指令解码可以加快其执行速度。
</code></pre>
<p>在高速缓存出现后不久，系统变得更加复杂。高速缓存与主存之间的速度差异进一步拉大，直到加入了另一级缓存。新加入的这一级缓存比第一级缓存更大，但是更慢。由于加大一级缓存的做法从经济上考虑是行不通的，所以有了二级缓存，甚至现在的有些系统拥有三级缓存，如图3.2所示。随着单个CPU中核数的增加，未来甚至可能会出现更多层级的缓存。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113714_KQTE.png" alt="img" /><br />
<strong>图3.2: 三级缓存的处理器</strong> </p>
</blockquote>
<p>图3.2展示了三级缓存，并介绍了本文将使用的一些术语。L1d是一级数据缓存，L1i是一级指令缓存，等等。请注意，这只是示意图，真正的数据流并不需要流经上级缓存。CPU的设计者们在设计高速缓存的接口时拥有很大的自由。而程序员是看不到这些设计选项的。 </p>
<p>另外，我们有多核CPU，每个核心可以有多个“线程”。核心与线程的不同之处在于，核心拥有独立的硬件资源({早期的多核CPU甚至有独立的二级缓存。})。在不同时使用相同资源(比如，通往外界的连接)的情况下，核心可以完全独立地运行。而线程只是共享资源。Intel的线程只有独立的寄存器，而且还有限制——不是所有寄存器都独立，有些是共享的。综上，现代CPU的结构就像图3.3所示。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113714_1MPY.png" alt="img" /><br />
<strong>图3.3 多处理器、多核心、多线程</strong> </p>
</blockquote>
<p>在上图中，有两个处理器，每个处理器有两个核心，每个核心有两个线程。线程们共享一级缓存。核心(以深灰色表示)有独立的一级缓存，同时共享二级缓存。处理器(淡灰色)之间不共享任何缓存。这些信息很重要，特别是在讨论多进程和多线程情况下缓存的影响时尤为重要。 </p>
<h3 id="32-高级的缓存操作"><a class="header" href="#32-高级的缓存操作">3.2 高级的缓存操作</a></h3>
<p>了解成本和节约使用缓存，我们必须结合在第二节中讲到的关于计算机体系结构和RAM技术，以及前一节讲到的缓存描述来探讨。 </p>
<p>默认情况下，CPU核心所有的数据的读或写都存储在缓存中。当然，也有内存区域不能被缓存的，但是这种情况只发生在操作系统的实现者对数据考虑的前提下；对程序实现者来说，这是不可见的。这也说明，程序设计者可以故意绕过某些缓存，不过这将是第六节中讨论的内容了。 </p>
<p>如果CPU需要访问某个字(word)，先检索缓存。很显然，缓存不可能容纳主存所有内容(否则还需要主存干嘛)。系统用字的内存地址来对缓存条目进行标记。如果需要读写某个地址的字，那么根据标签来检索缓存即可。这里用到的地址可以是虚拟地址，也可以是物理地址，取决于缓存的具体实现。 </p>
<p>标签是需要额外空间的，用字作为缓存的粒度显然毫无效率。比如，在x86机器上，32位字的标签可能需要32位，甚至更长。另一方面，由于空间局部性的存在，与当前地址相邻的地址有很大可能会被一起访问。再回忆下2.2.1节——内存模块在传输位于同一行上的多份数据时，由于不需要发送新CAS信号，甚至不需要发送RAS信号，因此可以实现很高的效率。基于以上的原因，缓存条目并不存储单个字，而是存储若干连续字组成的“线”。在早期的缓存中，线长是32字节，现在一般是64字节。对于64位宽的内存总线，每条线需要8次传输。而DDR对于这种传输模式的支持更为高效。 </p>
<p>当处理器需要内存中的某块数据时，整条缓存线被装入L1d。缓存线的地址通过对内存地址进行掩码操作生成。对于64字节的缓存线，是将低6位置0。这些被丢弃的位作为线内偏移量。其它的位作为标签，并用于在缓存内定位。在实践中，我们将地址分为三个部分。32位地址的情况如下: </p>
<blockquote>
<p><img src="cpu-memory/assets/27113714_bLaw.png" alt="img" /> </p>
</blockquote>
<p>如果缓存线长度为2O，那么地址的低O位用作线内偏移量。上面的S位选择“缓存集”。后面我们会说明使用缓存集的原因。现在只需要明白一共有2S个缓存集就够了。剩下的32 - S - O = T位组成标签。它们用来区分别名相同的各条线{有相同S部分的缓存线被称为有相同的别名。}用于定位缓存集的S部分不需要存储，因为属于同一缓存集的所有线的S部分都是相同的。 </p>
<p>当某条指令修改内存时，仍然要先装入缓存线，因为任何指令都不可能同时修改整条线(只有一个例外——第6.1节中将会介绍的写合并(write-combine))。因此需要在写操作前先把缓存线装载进来。如果缓存线被写入，但还没有写回主存，那就是所谓的“脏了”。脏了的线一旦写回主存，脏标记即被清除。 </p>
<p>为了装入新数据，基本上总是要先在缓存中清理出位置。L1d将内容逐出L1d，推入L2(线长相同)。当然，L2也需要清理位置。于是L2将内容推入L3，最后L3将它推入主存。这种逐出操作一级比一级昂贵。这里所说的是现代AMD和VIA处理器所采用的<em>独占型缓存(exclusive cache)</em>。而Intel采用的是<em>包容型缓存(inclusive cache)，</em>{并不完全正确，Intel有些缓存是独占型的，还有一些缓存具有独占型缓存的特点。}L1d的每条线同时存在于L2里。对这种缓存，逐出操作就很快了。如果有足够L2，对于相同内容存在不同地方造成内存浪费的缺点可以降到最低，而且在逐出时非常有利。而独占型缓存在装载新数据时只需要操作L1d，不需要碰L2，因此会比较快。 </p>
<p>处理器体系结构中定义的作为存储器的模型只要还没有改变，那就允许多CPU按照自己的方式来管理高速缓存。这表示，例如，设计优良的处理器，利用很少或根本没有内存总线活动，并主动写回主内存脏高速缓存行。这种高速缓存架构在如x86和x86-64各种各样的处理器间存在。制造商之间，即使在同一制造商生产的产品中，证明了的内存模型抽象的力量。 </p>
<p>在对称多处理器（SMP）架构的系统中，CPU的高速缓存不能独立的工作。在任何时候，所有的处理器都应该拥有相同的内存内容。保证这样的统一的内存视图被称为“高速缓存一致性”。如果在其自己的高速缓存和主内存间，处理器设计简单，它将不会看到在其他处理器上的脏高速缓存行的内容。从一个处理器直接访问另一个处理器的高速缓存这种模型设计代价将是非常昂贵的，它是一个相当大的瓶颈。相反，当另一个处理器要读取或写入到高速缓存线上时，处理器会去检测。</p>
<p>如果CPU检测到一个写访问，而且该CPU的cache中已经缓存了一个cache line的原始副本，那么这个cache  line将被标记为无效的cache line。接下来在引用这个cache line之前，需要重新加载该cache  line。需要注意的是读访问并不会导致cache line被标记为无效的。 </p>
<p>更精确的cache实现需要考虑到其他更多的可能性，比如第二个CPU在读或者写他的cache line时，发现该cache  line在第一个CPU的cache中被标记为脏数据了，此时我们就需要做进一步的处理。在这种情况下，主存储器已经失效，第二个CPU需要读取第一个CPU的cache line。通过测试，我们知道在这种情况下第一个CPU会将自己的cache  line数据自动发送给第二个CPU。这种操作是绕过主存储器的，但是有时候存储控制器是可以直接将第一个CPU中的cache  line数据存储到主存储器中。对第一个CPU的cache的写访问会导致本地cache line的所有拷贝被标记为无效。 </p>
<p>随着时间的推移，一大批缓存一致性协议已经建立。其中，最重要的是MESI,我们将在第3.3.4节进行介绍。以上结论可以概括为几个简单的规则:</p>
<ul>
<li>一个脏缓存线不存在于任何其他处理器的缓存之中。 </li>
<li>同一缓存线中的干净拷贝可以驻留在任意多个其他缓存之中。 </li>
</ul>
<p>如果遵守这些规则,处理器甚至可以在多处理器系统中更加有效的使用它们的缓存。所有的处理器需要做的就是监控其他每一个写访问和比较本地缓存中的地址。在下一节中,我们将介绍更多细节方面的实现,尤其是存储开销方面的细节。</p>
<p>最后，我们至少应该关注高速缓存命中或未命中带来的消耗。下面是英特尔奔腾 M 的数据： </p>
<div class="table-wrapper"><table><thead><tr><th>To Where</th><th>Cycles</th></tr></thead><tbody>
<tr><td>Register</td><td>&lt;= 1</td></tr>
<tr><td>L1d</td><td>~3</td></tr>
<tr><td>L2</td><td>~14</td></tr>
<tr><td>Main Memory</td><td>~240</td></tr>
</tbody></table>
</div>
<p>这是在CPU周期中的实际访问时间。有趣的是，对于L2高速缓存的访问时间很大一部分（甚至是大部分）是由线路的延迟引起的。这是一个限制，增加高速缓存的大小变得更糟。只有当减小时（例如，从60纳米的Merom到45纳米Penryn处理器），可以提高这些数据。 </p>
<p>表格中的数字看起来很高，但是，幸运的是，整个成本不必须负担每次出现的缓存加载和缓存失效。某些部分的成本可以被隐藏。现在的处理器都使用不同长度的内部管道，在管道内指令被解码，并为准备执行。如果数据要传送到一个寄存器，那么部分的准备工作是从存储器（或高速缓存）加载数据。如果内存加载操作在管道中足够早的进行，它可以与其他操作并行发生，那么加载的全部发销可能会被隐藏。对L1D常常可能如此；某些有长管道的处理器的L2也可以。</p>
<p>提早启动内存的读取有许多障碍。它可能只是简单的不具有足够资源供内存访问，或者地址从另一个指令获取，然后加载的最终地址才变得可用。在这种情况下，加载成本是不能隐藏的（完全的）。</p>
<p>对于写操作，CPU并不需要等待数据被安全地放入内存。只要指令具有类似的效果，就没有什么东西可以阻止CPU走捷径了。它可以早早地执行下一条指令，甚至可以在影子寄存器(shadow register)的帮助下，更改这个写操作将要存储的数据。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113714_N5Op.png" alt="img" /><br />
<strong>图3.4: 随机写操作的访问时间</strong> </p>
</blockquote>
<p>图3.4展示了缓存的效果。关于产生图中数据的程序，我们会在稍后讨论。这里大致说下，这个程序是连续随机地访问某块大小可配的内存区域。每个数据项的大小是固定的。数据项的多少取决于选择的工作集大小。Y轴表示处理每个元素平均需要多少个CPU周期，注意它是对数刻度。X轴也是同样，工作集的大小都以2的n次方表示。 </p>
<p>图中有三个比较明显的不同阶段。很正常，这个处理器有L1d和L2，没有L3。根据经验可以推测出，L1d有213字节，而L2有220字节。因为，如果整个工作集都可以放入L1d，那么只需不到10个周期就可以完成操作。如果工作集超过L1d，处理器不得不从L2获取数据，于是时间飘升到28个周期左右。如果工作集更大，超过了L2，那么时间进一步暴涨到480个周期以上。这时候，许多操作将不得不从主存中获取数据。更糟糕的是，如果修改了数据，还需要将这些脏了的缓存线写回内存。 </p>
<p>看了这个图，大家应该会有足够的动力去检查代码、改进缓存的利用方式了吧？这里的性能改善可不只是微不足道的几个百分点，而是几个数量级呀。在第6节中，我们将介绍一些编写高效代码的技巧。而下一节将进一步深入缓存的设计。虽然精彩，但并不是必修课，大家可以选择性地跳过。 </p>
<h3 id="33-cpu缓存实现的细节"><a class="header" href="#33-cpu缓存实现的细节">3.3 CPU缓存实现的细节</a></h3>
<p>缓存的实现者们都要面对一个问题——主存中每一个单元都可能需被缓存。如果程序的工作集很大，就会有许多内存位置为了缓存而打架。前面我们曾经提过缓存与主存的容量比，1:1000也十分常见。 </p>
<h4 id="331-关联性"><a class="header" href="#331-关联性">3.3.1 关联性</a></h4>
<p>我们可以让缓存的每条线能存放任何内存地址的数据。这就是所谓的<em>全关联缓存(fully associative cache**)</em>。对于这种缓存，处理器为了访问某条线，将不得不检索所有线的标签。而标签则包含了整个地址，而不仅仅只是线内偏移量(也就意味着，图3.2中的S为0)。 </p>
<p>高速缓存有类似这样的实现，但是，看看在今天使用的L2的数目，表明这是不切实际的。给定4MB的高速缓存和64B的高速缓存段，高速缓存将有65,536个项。为了达到足够的性能，缓存逻辑必须能够在短短的几个时钟周期内，从所有这些项中，挑一个匹配给定的标签。实现这一点的工作将是巨大的。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113715_MkFo.png" alt="img" /></p>
<p><strong>Figure 3.5: 全关联高速缓存原理图</strong> </p>
</blockquote>
<p>对于每个高速缓存行，比较器是需要比较大标签（注意，S是零）。每个连接旁边的字母表示位的宽度。如果没有给出，它是一个单比特线。每个比较器都要比较两个T-位宽的值。然后，基于该结果，适当的高速缓存行的内容被选中，并使其可用。这需要合并多套O数据线，因为他们是缓存桶（译注：这里类似把O输出接入多选器，所以需要合并）。实现仅仅一个比较器，需要晶体管的数量就非常大，特别是因为它必须非常快。没有迭代比较器是可用的。节省比较器的数目的唯一途径是通过反复比较标签，以减少它们的数目。这是不适合的，出于同样的原因，迭代比较器不可用：它的时间太长。 </p>
<pre><code>                                                     全关联高速缓存对  小缓存是实用的（例如，在某些Intel处理器的TLB缓存是全关联的），但这些缓存都很小，非常小的。我们正在谈论的最多几十项。  
</code></pre>
<p>对于L1i，L1d和更高级别的缓存，需要采用不同的方法。可以做的就是是限制搜索。最极端的限制是，每个标签映射到一个明确的缓存条目。计算很简单：给定的4MB/64B缓存有65536项，我们可以使用地址的bit6到bit21（16位）来直接寻址高速缓存的每一个项。地址的低6位作为高速缓存段的索引。</p>
<blockquote>
<p><img src="cpu-memory/assets/27113715_FrW1.png" alt="img" /></p>
<p><strong>Figure 3.6: Direct-Mapped Cache Schematics</strong> </p>
</blockquote>
<p>在图3.6中可以看出，这种直接映射的高速缓存，速度快，比较容易实现。它只是需要一个比较器，一个多路复用器（在这个图中有两个，标记和数据是分离的，但是对于设计这不是一个硬性要求），和一些逻辑来选择只是有效的高速缓存行的内容。由于速度的要求，比较器是复杂的，但是现在只需要一个，结果是可以花更多的精力，让其变得快速。这种方法的复杂性在于在多路复用器。一个简单的多路转换器中的晶体管的数量增速是O（log N）的，其中N是高速缓存段的数目。这是可以容忍的，但可能会很慢，在某种情况下，速度可提升，通过增加多路复用器晶体管数量，来并行化的一些工作和自身增速。晶体管的总数只是随着快速增长的高速缓存缓慢的增加，这使得这种解决方案非常有吸引力。但它有一个缺点：只有用于直接映射地址的相关的地址位均匀分布，程序才能很好工作。如果分布的不均匀，而且这是常态，一些缓存项频繁的使用，并因此多次被换出，而另一些则几乎不被使用或一直是空的。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113715_hQpb.png" alt="img" /></p>
<p><strong>Figure 3.7: 组关联高速缓存原理图</strong> </p>
</blockquote>
<p>可以通过使高速缓存的组关联来解决此问题。组关联结合高速缓存的全关联和直接映射高速缓存特点，在很大程度上避免那些设计的弱点。图3.7显示了一个组关联高速缓存的设计。标签和数据存储分成不同的组并可以通过地址选择。这类似直接映射高速缓存。但是，小数目的值可以在同一个高速缓存组缓存，而不是一个缓存组只有一个元素，用于在高速缓存中的每个设定值是相同的一组值的缓存。所有组的成员的标签可以并行比较，这类似全关联缓存的功能。 </p>
<p>其结果是高速缓存，不容易被不幸或故意选择同属同一组编号的地址所击败，同时高速缓存的大小并不限于由比较器的数目，可以以并行的方式实现。如果高速缓存增长，只（在该图中）增加列的数目，而不增加行数。只有高速缓存之间的关联性增加，行数才会增加。今天，处理器的L2高速缓存或更高的高速缓存，使用的关联性高达16。 L1高速缓存通常使用8。 </p>
<div class="table-wrapper"><table><thead><tr><th>L2  Cache  Size</th><th>Associativity</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>Direct</td><td>2</td><td>4</td><td>8</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>CL=32</td><td>CL=64</td><td>CL=32</td><td>CL=64</td><td>CL=32</td><td>CL=64</td><td>CL=32</td><td>CL=64</td><td></td></tr>
<tr><td>512k</td><td>27,794,595</td><td>20,422,527</td><td>25,222,611</td><td>18,303,581</td><td>24,096,510</td><td>17,356,121</td><td>23,666,929</td><td>17,029,334</td></tr>
<tr><td>1M</td><td>19,007,315</td><td>13,903,854</td><td>16,566,738</td><td>12,127,174</td><td>15,537,500</td><td>11,436,705</td><td>15,162,895</td><td>11,233,896</td></tr>
<tr><td>2M</td><td>12,230,962</td><td>8,801,403</td><td>9,081,881</td><td>6,491,011</td><td>7,878,601</td><td>5,675,181</td><td>7,391,389</td><td>5,382,064</td></tr>
<tr><td>4M</td><td>7,749,986</td><td>5,427,836</td><td>4,736,187</td><td>3,159,507</td><td>3,788,122</td><td>2,418,898</td><td>3,430,713</td><td>2,125,103</td></tr>
<tr><td>8M</td><td>4,731,904</td><td>3,209,693</td><td>2,690,498</td><td>1,602,957</td><td>2,207,655</td><td>1,228,190</td><td>2,111,075</td><td>1,155,847</td></tr>
<tr><td>16M</td><td>2,620,587</td><td>1,528,592</td><td>1,958,293</td><td>1,089,580</td><td>1,704,878</td><td>883,530</td><td>1,671,541</td><td>862,324</td></tr>
</tbody></table>
</div>
<p><strong>Table 3.1: 高速缓存大小，关联行，段大小的影响</strong> </p>
<p>给定我们4MB/64B高速缓存，8路组关联，相关的缓存留给我们的有8192组，只用标签的13位，就可以寻址缓集。要确定哪些（如果有的话）的缓存组设置中的条目包含寻址的高速缓存行，8个标签都要进行比较。在很短的时间内做出来是可行的。通过一个实验，我们可以看到，这是有意义的。 </p>
<p>表3.1显示一个程序在改变缓存大小，缓存段大小和关联集大小，L2高速缓存的缓存失效数量（根据Linux内核相关的方面人的说法，GCC在这种情况下，是他们所有中最重要的标尺）。在7.2节中，我们将介绍工具来模拟此测试要求的高速缓存。 </p>
<p>万一这还不是很明显，所有这些值之间的关系是高速缓存的大小为： </p>
<blockquote>
<p>cache line size × associativity × number of sets </p>
</blockquote>
<p>地址被映射到高速缓存使用 </p>
<blockquote>
<p><strong>O</strong> = log  2 cache line size<br />
<strong>S</strong> = log  2 number of sets </p>
</blockquote>
<p>在第3.2节中的图显示的方式。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113715_JWar.png" alt="img" /></p>
<p><strong>Figure 3.8: 缓存段大小 vs 关联行 (CL=32)</strong> </p>
</blockquote>
<p>图3.8表中的数据更易于理解。它显示一个固定的32个字节大小的高速缓存行的数据。对于一个给定的高速缓存大小，我们可以看出，关联性，的确可以帮助明显减少高速缓存未命中的数量。对于8MB的缓存，从直接映射到2路组相联，可以减少近44％的高速缓存未命中。组相联高速缓存和直接映射缓存相比，该处理器可以把更多的工作集保持在缓存中。 </p>
<p>在文献中，偶尔可以读到，引入关联性，和加倍高速缓存的大小具有相同的效果。在从4M缓存跃升到8MB缓存的极端的情况下，这是正确的。关联性再提高一倍那就肯定不正确啦。正如我们所看到的数据，后面的收益要小得多。我们不应该完全低估它的效果，虽然。在示例程序中的内存使用的峰值是5.6M。因此，具有8MB缓存不太可能有很多（两个以上）使用相同的高速缓存的组。从较小的缓存的关联性的巨大收益可以看出，较大工作集可以节省更多。 </p>
<p>在一般情况下，增加8以上的高速缓存之间的关联性似乎对只有一个单线程工作量影响不大。随着介绍一个使用共享L2的多核处理器，形势发生了变化。现在你基本上有两个程序命中相同的缓存， 实际上导致高速缓存减半（对于四核处理器是1/4）。因此，可以预期，随着核的数目的增加，共享高速缓存的相关性也应增长。一旦这种方法不再可行（16 路组关联性已经很难）处理器设计者不得不开始使用共享的三级高速缓存和更高级别的，而L2高速缓存只被核的一个子集共享。 </p>
<p>从图3.8中，我们还可以研究缓存大小对性能的影响。这一数据需要了解工作集的大小才能进行解读。很显然，与主存相同的缓存比小缓存能产生更好的结果，因此，缓存通常是越大越好。 </p>
<p>上文已经说过，示例中最大的工作集为5.6M。它并没有给出最佳缓存大小值，但我们可以估算出来。问题主要在于内存的使用并不连续，因此，即使是16M的缓存，在处理5.6M的工作集时也会出现冲突(参见2路集合关联式16MB缓存vs直接映射式缓存的优点)。不管怎样，我们可以有把握地说，在同样5.6M的负载下，缓存从16MB升到32MB基本已没有多少提高的余地。但是，工作集是会变的。如果工作集不断增大，缓存也需要随之增大。在购买计算机时，如果需要选择缓存大小，一定要先衡量工作集的大小。原因可以参见图3.10。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113716_62uA.png" alt="img" /><br />
<strong>图3.9: 测试的内存分布情况</strong> </p>
</blockquote>
<p>我们执行两项测试。第一项测试是按顺序地访问所有元素。测试程序循着指针n进行访问，而所有元素是链接在一起的，从而使它们的被访问顺序与在内存中排布的顺序一致，如图3.9的下半部分所示，末尾的元素有一个指向首元素的引用。而第二项测试(见图3.9的上半部分)则是按随机顺序访问所有元素。在上述两个测试中，所有元素都构成一个单向循环链表。 </p>
<p><strong>3.3.2 Cache的性能测试</strong> </p>
<p>用于测试程序的数据可以模拟一个任意大小的工作集：包括读、写访问，随机、连续访问。在图3.4中我们可以看到，程序为工作集创建了一个与其大小和元素类型相同的数组： </p>
<pre><code>  struct l {
    struct l *n;
    long int pad[NPAD];
  };
</code></pre>
<p>n字段将所有节点随机得或者顺序的加入到环形链表中，用指针从当前节点进入到下一个节点。pad字段用来存储数据，其可以是任意大小。在一些测试程序中，pad字段是可以修改的, 在其他程序中，pad字段只可以进行读操作。 </p>
<p>在性能测试中，我们谈到工作集大小的问题，工作集使用结构体l定义的元素表示的。2N 字节的工作集包含 </p>
<blockquote>
<p>2  N/sizeof(struct l) </p>
</blockquote>
<p>个元素. 显然sizeof(struct l) 的值取决于NPAD的大小。在32位系统上，NPAD=7意味着数组的每个元素的大小为32字节，在64位系统上，NPAD=7意味着数组的每个元素的大小为64字节。 </p>
<h4 id="单线程顺序访问"><a class="header" href="#单线程顺序访问">单线程顺序访问</a></h4>
<p>最简单的情况就是遍历链表中顺序存储的节点。无论是从前向后处理，还是从后向前，对于处理器来说没有什么区别。下面的测试中，我们需要得到处理链表中一个元素所需要的时间，以CPU时钟周期最为计时单元。图3.10显示了测试结构。除非有特殊说明, 所有的测试都是在Pentium 4 64-bit 平台上进行的，因此结构体l中NPAD=0，大小为8字节。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113716_eaYQ.png" alt="img" /></p>
<p><strong>图 3.10: 顺序读访问, NPAD=0</strong> </p>
</blockquote>
<blockquote>
<p><img src="cpu-memory/assets/27113716_LLY3.png" alt="img" /></p>
<p><strong>图 3.11: 顺序读多个字节</strong> </p>
</blockquote>
<p>一开始的两个测试数据收到了噪音的污染。由于它们的工作负荷太小，无法过滤掉系统内其它进程对它们的影响。我们可以认为它们都是4个周期以内的。这样一来，整个图可以划分为比较明显的三个部分: </p>
<ul>
<li>工作集小于214字节的。 </li>
<li>工作集从215字节到220字节的。 </li>
<li>工作集大于221字节的。 </li>
</ul>
<p>这样的结果很容易解释——是因为处理器有16KB的L1d和1MB的L2。而在这三个部分之间，并没有非常锐利的边缘，这是因为系统的其它部分也在使用缓存，我们的测试程序并不能独占缓存的使用。尤其是L2，它是统一式的缓存，处理器的指令也会使用它(注: Intel使用的是包容式缓存)。 </p>
<p>测试的实际耗时可能会出乎大家的意料。L1d的部分跟我们预想的差不多，在一台P4上耗时为4个周期左右。但L2的结果则出乎意料。大家可能觉得需要14个周期以上，但实际只用了9个周期。这要归功于处理器先进的处理逻辑，当它使用连续的内存区时，会  <em>预先读取</em>下一条缓存线的数据。这样一来，当真正使用下一条线的时候，其实已经早已读完一半了，于是真正的等待耗时会比L2的访问时间少很多。</p>
<p>在工作集超过L2的大小之后，预取的效果更明显了。前面我们说过，主存的访问需要耗时200个周期以上。但在预取的帮助下，实际耗时保持在9个周期左右。200 vs 9，效果非常不错。 </p>
<p>我们可以观察到预取的行为，至少可以间接地观察到。图3.11中有4条线，它们表示处理不同大小结构时的耗时情况。随着结构的变大，元素间的距离变大了。图中4条线对应的元素距离分别是0、56、120和248字节。 </p>
<p>图中最下面的这一条线来自前一个图，但在这里更像是一条直线。其它三条线的耗时情况比较差。图中这些线也有比较明显的三个阶段，同时，在小工作集的情况下也有比较大的错误(请再次忽略这些错误)。在只使用L1d的阶段，这些线条基本重合。因为这时候还不需要预取，只需要访问L1d就行。 </p>
<p>在L2阶段，三条新加的线基本重合，而且耗时比老的那条线高很多，大约在28个周期左右，差不多就是L2的访问时间。这表明，从L2到L1d的预取并没有生效。这是因为，对于最下面的线(NPAD=0)，由于结构小，8次循环后才需要访问一条新缓存线，而上面三条线对应的结构比较大，拿相对最小的NPAD=7来说，光是一次循环就需要访问一条新线，更不用说更大的NPAD=15和31了。而预取逻辑是无法在每个周期装载新线的，因此每次循环都需要从L2读取，我们看到的就是从L2读取的时延。 </p>
<p>更有趣的是工作集超过L2容量后的阶段。快看，4条线远远地拉开了。元素的大小变成了主角，左右了性能。处理器应能识别每一步(stride)的大小，不去为NPAD=15和31获取那些实际并不需要的缓存线(参见6.3.1)。元素大小对预取的约束是根源于硬件预取的限制——它无法跨越页边界。如果允许预取器跨越页边界，而下一页不存在或无效，那么OS还得去寻找它。这意味着，程序需要遭遇一次并非由它自己产生的页错误，这是完全不能接受的。在NPAD=7或者更大的时候，由于每个元素都至少需要一条缓存线，预取器已经帮不上忙了，它没有足够的时间去从内存装载数据。另一个导致慢下来的原因是TLB缓存的未命中。TLB是存储虚实地址映射的缓存，参见第4节。为了保持快速，TLB只有很小的容量。如果有大量页被反复访问，超出了TLB缓存容量，就会导致反复地进行地址翻译，这会耗费大量时间。TLB查找的代价分摊到所有元素上，如果元素越大，那么元素的数量越少，每个元素承担的那一份就越多。</p>
<p>为了观察TLB的性能，我们可以进行另两项测试。第一项：我们还是顺序存储列表中的元素，使NPAD=7，让每个元素占满整个cache  line，第二项：我们将列表的每个元素存储在一个单独的页上，忽略每个页没有使用的部分以用来计算工作集的大小。（这样做可能不太一致，因为在前面的测试中，我计算了结构体中每个元素没有使用的部分，从而用来定义NPAD的大小，因此每个元素占满了整个页，这样以来工作集的大小将会有所不同。但是这不是这项测试的重点，预取的低效率多少使其有点不同）。结果表明，第一项测试中，每次列表的迭代都需要一个新的cache line，而且每64个元素就需要一个新的页。第二项测试中，每次迭代都会在一个新的页中加载一个新的cache line。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113716_ifZC.png" alt="img" /></p>
<p><strong>图 3.12: TLB 对顺序读的影响</strong></p>
</blockquote>
<p>结果见图3.12。该测试与图3.11是在同一台机器上进行的。基于可用RAM空间的有限性，测试设置容量空间大小为2的24次方字节，这就需要1GB的容量将对象放置在分页上。图3.12中下方的红色曲线正好对应了图3.11中NPAD等于7的曲线。我们看到不同的步长显示了高速缓存L1d和L2的大小。第二条曲线看上去完全不同，其最重要的特点是当工作容量到达2的13次方字节时开始大幅度增长。这就是TLB缓存溢出的时候。我们能计算出一个64字节大小的元素的TLB缓存有64个输入。成本不会受页面错误影响，因为程序锁定了存储器以防止内存被换出。</p>
<p>可以看出，计算物理地址并把它存储在TLB中所花费的周期数量级是非常高的。图3.12的表格显示了一个极端的例子，但从中可以清楚的得到：TLB缓存效率降低的一个重要因素是大型NPAD值的减缓。由于物理地址必须在缓存行能被L2或主存读取之前计算出来，地址转换这个不利因素就增加了内存访问时间。这一点部分解释了为什么NPAD等于31时每个列表元素的总花费比理论上的RAM访问时间要高。 </p>
<p><img src="cpu-memory/assets/27113717_mCPy.png" alt="img" />
图3.13 NPAD等于1时的顺序读和写 </p>
<p>通过查看链表元素被修改时测试数据的运行情况，我们可以窥见一些更详细的预取实现细节。图3.13显示了三条曲线。所有情况下元素宽度都为16个字节。第一条曲线“Follow”是熟悉的链表走线在这里作为基线。第二条曲线，标记为“Inc”，仅仅在当前元素进入下一个前给其增加thepad[0]成员。第三条曲线，标记为&quot;Addnext0&quot;， 取出下一个元素的thepad[0]链表元素并把它添加为当前链表元素的thepad[0]成员。 </p>
<p>在没运行时，大家可能会以为&quot;Addnext0&quot;更慢，因为它要做的事情更多——在没进到下个元素之前就需要装载它的值。但实际的运行结果令人惊讶——在某些小工作集下，&quot;Addnext0&quot;比&quot;Inc&quot;更快。这是为什么呢？原因在于，系统一般会对下一个元素进行强制性预取。当程序前进到下个元素时，这个元素其实早已被预取在L1d里。因此，只要工作集比L2小，&quot;Addnext0&quot;的性能基本就能与&quot;Follow&quot;测试媲美。 </p>
<p>但是，&quot;Addnext0&quot;比&quot;Inc&quot;更快离开L2，这是因为它需要从主存装载更多的数据。而在工作集达到2 21字节时，&quot;Addnext0&quot;的耗时达到了28个周期，是同期&quot;Follow&quot;14周期的两倍。这个两倍也很好解释。&quot;Addnext0&quot;和&quot;Inc&quot;涉及对内存的修改，因此L2的逐出操作不能简单地把数据一扔了事，而必须将它们写入内存。因此FSB的可用带宽变成了一半，传输等量数据的耗时也就变成了原来的两倍。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113717_SILO.png" alt="img" /><br />
<strong>图3.14: 更大L2/L3缓存的优势</strong> </p>
</blockquote>
<p>决定顺序式缓存处理性能的另一个重要因素是缓存容量。虽然这一点比较明显，但还是值得一说。图3.14展示了128字节长元素的测试结果(64位机，NPAD=15)。这次我们比较三台不同计算机的曲线，两台P4，一台Core 2。两台P4的区别是缓存容量不同，一台是32k的L1d和1M的L2，一台是16K的L1d、512k的L2和2M的L3。Core  2那台则是32k的L1d和4M的L2。 </p>
<p>图中最有趣的地方，并不是Core 2如何大胜两台P4，而是工作集开始增长到连末级缓存也放不下、需要主存热情参与之后的部分。 </p>
<div class="table-wrapper"><table><thead><tr><th>Set  Size</th><th>Sequential</th><th>Random</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>L2 Hit</td><td>L2 Miss</td><td>#Iter</td><td>Ratio Miss/Hit</td><td>L2 Accesses Per Iter</td><td>L2 Hit</td><td>L2 Miss</td><td>#Iter</td><td>Ratio Miss/Hit</td><td>L2 Accesses Per Iter</td><td></td></tr>
<tr><td>220</td><td>88,636</td><td>843</td><td>16,384</td><td>0.94%</td><td>5.5</td><td>30,462</td><td>4721</td><td>1,024</td><td>13.42%</td><td>34.4</td></tr>
<tr><td>221</td><td>88,105</td><td>1,584</td><td>8,192</td><td>1.77%</td><td>10.9</td><td>21,817</td><td>15,151</td><td>512</td><td>40.98%</td><td>72.2</td></tr>
<tr><td>222</td><td>88,106</td><td>1,600</td><td>4,096</td><td>1.78%</td><td>21.9</td><td>22,258</td><td>22,285</td><td>256</td><td>50.03%</td><td>174.0</td></tr>
<tr><td>223</td><td>88,104</td><td>1,614</td><td>2,048</td><td>1.80%</td><td>43.8</td><td>27,521</td><td>26,274</td><td>128</td><td>48.84%</td><td>420.3</td></tr>
<tr><td>224</td><td>88,114</td><td>1,655</td><td>1,024</td><td>1.84%</td><td>87.7</td><td>33,166</td><td>29,115</td><td>64</td><td>46.75%</td><td>973.1</td></tr>
<tr><td>225</td><td>88,112</td><td>1,730</td><td>512</td><td>1.93%</td><td>175.5</td><td>39,858</td><td>32,360</td><td>32</td><td>44.81%</td><td>2,256.8</td></tr>
<tr><td>226</td><td>88,112</td><td>1,906</td><td>256</td><td>2.12%</td><td>351.6</td><td>48,539</td><td>38,151</td><td>16</td><td>44.01%</td><td>5,418.1</td></tr>
<tr><td>227</td><td>88,114</td><td>2,244</td><td>128</td><td>2.48%</td><td>705.9</td><td>62,423</td><td>52,049</td><td>8</td><td>45.47%</td><td>14,309.0</td></tr>
<tr><td>228</td><td>88,120</td><td>2,939</td><td>64</td><td>3.23%</td><td>1,422.8</td><td>81,906</td><td>87,167</td><td>4</td><td>51.56%</td><td>42,268.3</td></tr>
<tr><td>229</td><td>88,137</td><td>4,318</td><td>32</td><td>4.67%</td><td>2,889.2</td><td>119,079</td><td>163,398</td><td>2</td><td>57.84%</td><td>141,238.5</td></tr>
</tbody></table>
</div>
<p><strong>表3.2: 顺序访问与随机访问时L2命中与未命中的情况，NPAD=0</strong> </p>
<p>与我们预计的相似，最末级缓存越大，曲线停留在L2访问耗时区的时间越长。在220字节的工作集时，第二台P4(更老一些)比第一台P4要快上一倍，这要完全归功于更大的末级缓存。而Core 2拜它巨大的4M L2所赐，表现更为卓越。 </p>
<p>对于随机的工作负荷而言，可能没有这么惊人的效果，但是，如果我们能将工作负荷进行一些裁剪，让它匹配末级缓存的容量，就完全可以得到非常大的性能提升。也是由于这个原因，有时候我们需要多花一些钱，买一个拥有更大缓存的处理器。 </p>
<p><strong>单线程随机访问模式的测量</strong> </p>
<p>前面我们已经看到，处理器能够利用L1d到L2之间的预取消除访问主存、甚至是访问L2的时延。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113717_AuZx.png" alt="img" /><br />
<strong>图3.15: 顺序读取vs随机读取，NPAD=0</strong> </p>
</blockquote>
<p>但是，如果换成随机访问或者不可预测的访问，情况就大不相同了。图3.15比较了顺序读取与随机读取的耗时情况。 </p>
<p>换成随机之后，处理器无法再有效地预取数据，只有少数情况下靠运气刚好碰到先后访问的两个元素挨在一起的情形。 </p>
<p>图3.15中有两个需要关注的地方。首先，在大的工作集下需要非常多的周期。这台机器访问主存的时间大约为200-300个周期，但图中的耗时甚至超过了450个周期。我们前面已经观察到过类似现象(对比图3.11)。这说明，处理器的自动预取在这里起到了反效果。 </p>
<p>其次，代表随机访问的曲线在各个阶段不像顺序访问那样保持平坦，而是不断攀升。为了解释这个问题，我们测量了程序在不同工作集下对L2的访问情况。结果如图3.16和表3.2。 </p>
<p>从图中可以看出，当工作集大小超过L2时，未命中率(L2未命中次数/L2访问次数)开始上升。整条曲线的走向与图3.15有些相似:  先急速爬升，随后缓缓下滑，最后再度爬升。它与耗时图有紧密的关联。L2未命中率会一直爬升到100%为止。只要工作集足够大(并且内存也足够大)，就可以将缓存线位于L2内或处于装载过程中的可能性降到非常低。 </p>
<p>缓存未命中率的攀升已经可以解释一部分的开销。除此以外，还有一个因素。观察表3.2的L2/#Iter列，可以看到每个循环对L2的使用次数在增长。由于工作集每次为上一次的两倍，如果没有缓存的话，内存的访问次数也将是上一次的两倍。在按顺序访问时，由于缓存的帮助及完美的预见性，对L2使用的增长比较平缓，完全取决于工作集的增长速度。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113718_DCMb.png" alt="img" /><br />
<strong>图3.16: L2d未命中率</strong> </p>
</blockquote>
<blockquote>
<p><img src="cpu-memory/assets/27113718_LnzD.png" alt="img" /><br />
<strong>图3.17: 页意义上(Page-Wise)的随机化，NPAD=7</strong> </p>
</blockquote>
<p>而换成随机访问后，单位耗时的增长超过了工作集的增长，根源是TLB未命中率的上升。图3.17描绘的是NPAD=7时随机访问的耗时情况。这一次，我们修改了随机访问的方式。正常情况下是把整个列表作为一个块进行随机(以∞表示)，而其它11条线则是在小一些的块里进行随机。例如，标签为'60'的线表示以60页(245760字节)为单位进行随机。先遍历完这个块里的所有元素，再访问另一个块。这样一来，可以保证任意时刻使用的TLB条目数都是有限的。NPAD=7对应于64字节，正好等于缓存线的长度。由于元素顺序随机，硬件预取不可能有任何效果，特别是在元素较多的情况下。这意味着，分块随机时的L2未命中率与整个列表随机时的未命中率没有本质的差别。随着块的增大，曲线逐渐逼近整个列表随机对应的曲线。这说明，在这个测试里，性能受到TLB命中率的影响很大，如果我们能提高TLB命中率，就能大幅度地提升性能(在后面的一个例子里，性能提升了38%之多)。</p>
<h4 id="333-写入时的行为"><a class="header" href="#333-写入时的行为">3.3.3 写入时的行为</a></h4>
<p>在我们开始研究多个线程或进程同时使用相同内存之前，先来看一下缓存实现的一些细节。我们要求缓存是一致的，而且这种一致性必须对用户级代码完全透明。而内核代码则有所不同，它有时候需要对缓存进行转储(flush)。 </p>
<p>这意味着，如果对缓存线进行了修改，那么在这个时间点之后，系统的结果应该是与没有缓存的情况下是相同的，即主存的对应位置也已经被修改的状态。这种要求可以通过两种方式或策略实现： </p>
<ul>
<li>写通(write-through) </li>
<li>写回(write-back) </li>
</ul>
<p>写通比较简单。当修改缓存线时，处理器立即将它写入主存。这样可以保证主存与缓存的内容永远保持一致。当缓存线被替代时，只需要简单地将它丢弃即可。这种策略很简单，但是速度比较慢。如果某个程序反复修改一个本地变量，可能导致FSB上产生大量数据流，而不管这个变量是不是有人在用，或者是不是短期变量。 </p>
<p>写回比较复杂。当修改缓存线时，处理器不再马上将它写入主存，而是打上已弄脏(dirty)的标记。当以后某个时间点缓存线被丢弃时，这个已弄脏标记会通知处理器把数据写回到主存中，而不是简单地扔掉。 </p>
<p>写回有时候会有非常不错的性能，因此较好的系统大多采用这种方式。采用写回时，处理器们甚至可以利用FSB的空闲容量来存储缓存线。这样一来，当需要缓存空间时，处理器只需清除脏标记，丢弃缓存线即可。 </p>
<p>但写回也有一个很大的问题。当有多个处理器(或核心、超线程)访问同一块内存时，必须确保它们在任何时候看到的都是相同的内容。如果缓存线在其中一个处理器上弄脏了(修改了，但还没写回主存)，而第二个处理器刚好要读取同一个内存地址，那么这个读操作不能去读主存，而需要读第一个处理器的缓存线。在下一节中，我们将研究如何实现这种需求。 </p>
<p>在此之前，还有其它两种缓存策略需要提一下: </p>
<ul>
<li>写入合并 </li>
<li>不可缓存 </li>
</ul>
<p>这两种策略用于真实内存不支持的特殊地址区，内核为地址区设置这些策略(x86处理器利用内存类型范围寄存器MTRR)，余下的部分自动进行。MTRR还可用于写通和写回策略的选择。 </p>
<p>写入合并是一种有限的缓存优化策略，更多地用于显卡等设备之上的内存。由于设备的传输开销比本地内存要高的多，因此避免进行过多的传输显得尤为重要。如果仅仅因为修改了缓存线上的一个字，就传输整条线，而下个操作刚好是修改线上的下一个字，那么这次传输就过于浪费了。而这恰恰对于显卡来说是比较常见的情形——屏幕上水平邻接的像素往往在内存中也是靠在一起的。顾名思义，写入合并是在写出缓存线前，先将多个写入访问合并起来。在理想的情况下，缓存线被逐字逐字地修改，只有当写入最后一个字时，才将整条线写入内存，从而极大地加速内存的访问。 </p>
<p>最后来讲一下不可缓存的内存。一般指的是不被RAM支持的内存位置，它可以是硬编码的特殊地址，承担CPU以外的某些功能。对于商用硬件来说，比较常见的是映射到外部卡或设备的地址。在嵌入式主板上，有时也有类似的地址，用来开关LED。对这些地址进行缓存显然没有什么意义。比如上述的LED，一般是用来调试或报告状态，显然应该尽快点亮或关闭。而对于那些PCI卡上的内存，由于不需要CPU的干涉即可更改，也不该缓存。</p>
<p><strong>3.3.4 多处理器支持</strong> </p>
<p>在上节中我们已经指出当多处理器开始发挥作用的时候所遇到的问题。甚至对于那些不共享的高速级别的缓存（至少在L1d级别）的多核处理器也有问题。 </p>
<p>直接提供从一个处理器到另一处理器的高速访问，这是完全不切实际的。从一开始，连接速度根本就不够快。实际的选择是，在其需要的情况下，转移到其他处理器。需要注意的是，这同样应用在相同处理器上无需共享的高速缓存。 </p>
<p>现在的问题是，当该高速缓存线转移的时候会发生什么？这个问题回答起来相当容易：当一个处理器需要在另一个处理器的高速缓存中读或者写的脏的高速缓存线的时候。但怎样处理器怎样确定在另一个处理器的缓存中的高速缓存线是脏的？假设它仅仅是因为一个高速缓存线被另一个处理器加载将是次优的（最好的）。通常情况下，大多数的内存访问是只读的访问和产生高速缓存线，并不脏。在高速缓存线上处理器频繁的操作（当然，否则为什么我们有这样的文件呢？），也就意味着每一次写访问后，都要广播关于高速缓存线的改变将变得不切实际。</p>
<p>多年来，人们开发除了MESI缓存一致性协议(MESI=Modified, Exclusive, Shared, Invalid，变更的、独占的、共享的、无效的)。协议的名称来自协议中缓存线可以进入的四种状态: </p>
<ul>
<li><strong>变更的</strong>: 本地处理器修改了缓存线。同时暗示，它是所有缓存中唯一的拷贝。 </li>
<li><strong>独占的</strong>: 缓存线没有被修改，而且没有被装入其它处理器缓存。 </li>
<li><strong>共享的</strong>: 缓存线没有被修改，但可能已被装入其它处理器缓存。 </li>
<li><strong>无效的</strong>: 缓存线无效，即，未被使用。 </li>
</ul>
<p>MESI协议开发了很多年，最初的版本比较简单，但是效率也比较差。现在的版本通过以上4个状态可以有效地实现写回式缓存，同时支持不同处理器对只读数据的并发访问。 </p>
<p><img src="cpu-memory/assets/27113718_cSIF.png" alt="img" /> 
<strong>图3.18: MESI协议的状态跃迁图</strong> </p>
<p>在协议中，通过处理器监听其它处理器的活动，不需太多努力即可实现状态变更。处理器将操作发布在外部引脚上，使外部可以了解到处理过程。目标的缓存线地址则可以在地址总线上看到。在下文讲述状态时，我们将介绍总线参与的时机。 </p>
<p>一开始，所有缓存线都是空的，缓存为无效(Invalid)状态。当有数据装进缓存供写入时，缓存变为变更(Modified)状态。如果有数据装进缓存供读取，那么新状态取决于其它处理器是否已经状态了同一条缓存线。如果是，那么新状态变成共享(Shared)状态，否则变成独占(Exclusive)状态。 </p>
<p>如果本地处理器对某条Modified缓存线进行读写，那么直接使用缓存内容，状态保持不变。如果另一个处理器希望读它，那么第一个处理器将内容发给第一个处理器，然后可以将缓存状态置为Shared。而发给第二个处理器的数据由内存控制器接收，并放入内存中。如果这一步没有发生，就不能将这条线置为Shared。如果第二个处理器希望的是写，那么第一个处理器将内容发给它后，将缓存置为Invalid。这就是臭名昭著的&quot;请求所有权(Request For  Ownership,RFO)&quot;操作。在末级缓存执行RFO操作的代价比较高。如果是写通式缓存，还要加上将内容写入上一层缓存或主存的时间，进一步提升了代价。对于Shared缓存线，本地处理器的读取操作并不需要修改状态，而且可以直接从缓存满足。而本地处理器的写入操作则需要将状态置为Modified，而且需要将缓存线在其它处理器的所有拷贝置为Invalid。因此，这个写入操作需要通过RFO消息发通知其它处理器。如果第二个处理器请求读取，无事发生。因为主存已经包含了当前数据，而且状态已经为Shared。如果第二个处理器需要写入，则将缓存线置为Invalid。不需要总线操作。</p>
<p>Exclusive状态与Shared状态很像，只有一个不同之处:  在Exclusive状态时，本地写入操作不需要在总线上声明，因为本地的缓存是系统中唯一的拷贝。这是一个巨大的优势，所以处理器会尽量将缓存线保留在Exclusive状态，而不是Shared状态。只有在信息不可用时，才退而求其次选择shared。放弃Exclusive不会引起任何功能缺失，但会导致性能下降，因为E→M要远远快于S→M。 </p>
<p>从以上的说明中应该已经可以看出，在多处理器环境下，哪一步的代价比较大了。填充缓存的代价当然还是很高，但我们还需要留意RFO消息。一旦涉及RFO，操作就快不起来了。 </p>
<p>RFO在两种情况下是必需的: </p>
<ul>
<li>线程从一个处理器迁移到另一个处理器，需要将所有缓存线移到新处理器。 </li>
<li>某条缓存线确实需要被两个处理器使用。{<em>对于同一处理器的两个核心，也有同样的情况，只是代价稍低。RFO消息可能会被发送多次。</em>} </li>
</ul>
<p>多线程或多进程的程序总是需要同步，而这种同步依赖内存来实现。因此，有些RFO消息是合理的，但仍然需要尽量降低发送频率。除此以外，还有其它来源的RFO。在第6节中，我们将解释这些场景。缓存一致性协议的消息必须发给系统中所有处理器。只有当协议确定已经给过所有处理器响应机会之后，才能进行状态跃迁。也就是说，协议的速度取决于最长响应时间。{<em>这也是现在能看到三插槽AMD Opteron系统的原因。这类系统只有三个超级链路(hyperlink)，其中一个连接南桥，每个处理器之间都只有一跳的距离。</em>}总线上可能会发生冲突，NUMA系统的延时很大，突发的流量会拖慢通信。这些都是让我们避免无谓流量的充足理由。 </p>
<p>此外，关于多处理器还有一个问题。虽然它的影响与具体机器密切相关，但根源是唯一的——FSB是共享的。在大多数情况下，所有处理器通过唯一的总线连接到内存控制器(参见图2.1)。如果一个处理器就能占满总线(十分常见)，那么共享总线的两个或四个处理器显然只会得到更有限的带宽。 </p>
<p>即使每个处理器有自己连接内存控制器的总线，如图2.2，但还需要通往内存模块的总线。一般情况下，这种总线只有一条。退一步说，即使像图2.2那样不止一条，对同一个内存模块的并发访问也会限制它的带宽。 </p>
<p>对于每个处理器拥有本地内存的AMD模型来说，也是同样的问题。的确，所有处理器可以非常快速地同时访问它们自己的内存。但是，多线程呢？多进程呢？它们仍然需要通过访问同一块内存来进行同步。 </p>
<p>对同步来说，有限的带宽严重地制约着并发度。程序需要更加谨慎的设计，将不同处理器访问同一块内存的机会降到最低。以下的测试展示了这一点，还展示了与多线程代码相关的其它效果。 </p>
<p><strong>多线程测量</strong> </p>
<p>为了帮助大家理解问题的严重性，我们来看一些曲线图，主角也是前文的那个程序。只不过这一次，我们运行多个线程，并测量这些线程中最快那个的运行时间。也就是说，等它们全部运行完是需要更长时间的。我们用的机器有4个处理器，而测试是做多跑4个线程。所有处理器共享同一条通往内存控制器的总线，另外，通往内存模块的总线也只有一条。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113718_1VA6.png" alt="img" /><br />
<strong>图3.19: 顺序读操作，多线程</strong> </p>
</blockquote>
<p>图3.19展示了顺序读访问时的性能，元素为128字节长(64位计算机，NPAD=15)。对于单线程的曲线，我们预计是与图3.11相似，只不过是换了一台机器，所以实际的数字会有些小差别。 </p>
<p>更重要的部分当然是多线程的环节。由于是只读，不会去修改内存，不会尝试同步。但即使不需要RFO，而且所有缓存线都可共享，性能仍然分别下降了18%(双线程)和34%(四线程)。由于不需要在处理器之间传输缓存，因此这里的性能下降完全由以下两个瓶颈之一或同时引起:  一是从处理器到内存控制器的共享总线，二是从内存控制器到内存模块的共享总线。当工作集超过L3后，三种情况下都要预取新元素，而即使是双线程，可用的带宽也无法满足线性扩展(无惩罚)。 </p>
<p>当加入修改之后，场面更加难看了。图3.20展示了顺序递增测试的结果。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113718_X4NY.png" alt="img" /><br />
<strong>图3.20: 顺序递增，多线程</strong> </p>
</blockquote>
<p>图中Y轴采用的是对数刻度，不要被看起来很小的差值欺骗了。现在，双线程的性能惩罚仍然是18%，但四线程的惩罚飙升到了93%！原因在于，采用四线程时，预取的流量与写回的流量加在一起，占满了整个总线。 </p>
<p>我们用对数刻度来展示L1d范围的结果。可以发现，当超过一个线程后，L1d就无力了。单线程时，仅当工作集超过L1d时访问时间才会超过20个周期，而多线程时，即使在很小的工作集情况下，访问时间也达到了那个水平。 </p>
<p>这里并没有揭示问题的另一方面，主要是用这个程序很难进行测量。问题是这样的，我们的测试程序修改了内存，所以本应看到RFO的影响，但在结果中，我们并没有在L2阶段看到更大的开销。原因在于，要看到RFO的影响，程序必须使用大量内存，而且所有线程必须同时访问同一块内存。如果没有大量的同步，这是很难实现的，而如果加入同步，则会占满执行时间。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113719_dsub.png" alt="img" /><br />
<strong>图3.21: 随机的Addnextlast，多线程</strong> </p>
</blockquote>
<p>最后，在图3.21中，我们展示了随机访问的Addnextlast测试的结果。这里主要是为了让大家感受一下这些巨大到爆的数字。极端情况下，甚至用了1500个周期才处理完一个元素。如果加入更多线程，真是不可想象哪。我们把多线程的效能总结了一下: </p>
<blockquote>
<div class="table-wrapper"><table><thead><tr><th>#Threads</th><th>Seq Read</th><th>Seq Inc</th><th>Rand Add</th></tr></thead><tbody>
<tr><td>2</td><td>1.69</td><td>1.69</td><td>1.54</td></tr>
<tr><td>4</td><td>2.98</td><td>2.07</td><td>1.65</td></tr>
</tbody></table>
</div>
<p>表3.3: 多线程的效能</p>
</blockquote>
<p>这个表展示了图3.21中多线程运行大工作集时的效能。表中的数字表示测试程序在使用多线程处理大工作集时可能达到的最大加速因子。双线程和四线程的理论最大加速因子分别是2和4。从表中数据来看，双线程的结果还能接受，但四线程的结果表明，扩展到双线程以上是没有什么意义的，带来的收益可以忽略不计。只要我们把图3.21换个方式呈现，就可以很容易看清这一点。 </p>
<p><img src="cpu-memory/assets/27113719_WbIL.png" alt="img" /> 
<strong>图3.22: 通过并行化实现的加速因子</strong> </p>
<p>图3.22中的曲线展示了加速因子，即多线程相对于单线程所能获取的性能加成值。测量值的精确度有限，因此我们需要忽略比较小的那些数字。可以看到，在L2与L3范围内，多线程基本可以做到线性加速，双线程和四线程分别达到了2和4的加速因子。但是，一旦工作集的大小超出L3，曲线就崩塌了，双线程和四线程降到了基本相同的数值(参见表3.3中第4列)。也是部分由于这个原因，我们很少看到4CPU以上的主板共享同一个内存控制器。如果需要配置更多处理器，我们只能选择其它的实现方式(参见第5节)。 </p>
<p>可惜，上图中的数据并不是普遍情况。在某些情况下，即使工作集能够放入末级缓存，也无法实现线性加速。实际上，这反而是正常的，因为普通的线程都有一定的耦合关系，不会像我们的测试程序这样完全独立。而反过来说，即使是很大的工作集，即使是两个以上的线程，也是可以通过并行化受益的，但是需要程序员的聪明才智。我们会在第6节进行一些介绍。 </p>
<p><strong>特例: 超线程</strong> </p>
<p>由CPU实现的超线程(有时又叫对称多线程，SMT)是一种比较特殊的情况，每个线程并不能真正并发地运行。它们共享着除寄存器外的绝大多数处理资源。每个核心和CPU仍然是并行工作的，但核心上的线程则受到这个限制。理论上，每个核心可以有大量线程，不过到目前为止，Intel的CPU最多只有两个线程。CPU负责对各线程进行时分复用，但这种复用本身并没有多少厉害。它真正的优势在于，CPU可以在当前运行的超线程发生延迟时，调度另一个线程。这种延迟一般由内存访问引起。 </p>
<p>如果两个线程运行在一个超线程核心上，那么只有当两个线程<em>合起来**的</em>运行时间少于单线程运行时间时，效率才会比较高。我们可以将通常先后发生的内存访问叠合在一起，以实现这个目标。有一个简单的计算公式，可以帮助我们计算如果需要某个加速因子，最少需要多少的缓存命中率。 </p>
<p>程序的执行时间可以通过一个只有一级缓存的简单模型来进行估算(参见[htimpact]): </p>
<blockquote>
<p>T  exe   = N[(1-F  mem  )T  proc   + F  mem  (G  hit  T  cache   + (1-G  hit  )T  miss  )] </p>
</blockquote>
<p>各变量的含义如下: </p>
<blockquote>
<div class="table-wrapper"><table><thead><tr><th>N</th><th>=</th><th>指令数</th></tr></thead><tbody>
<tr><td>Fmem</td><td>=</td><td>N中访问内存的比例</td></tr>
<tr><td>Ghit</td><td>=</td><td>命中缓存的比例</td></tr>
<tr><td>Tproc</td><td>=</td><td>每条指令所用的周期数</td></tr>
<tr><td>Tcache</td><td>=</td><td>缓存命中所用的周期数</td></tr>
<tr><td>Tmiss</td><td>=</td><td>缓冲未命中所用的周期数</td></tr>
<tr><td>Texe</td><td>=</td><td>程序的执行时间</td></tr>
</tbody></table>
</div></blockquote>
<p>为了让任何判读使用双线程，两个线程之中任一线程的执行时间最多为单线程指令的一半。两者都有一个唯一的变量缓存命中数。 如果我们要解决最小缓存命中率相等的问题需要使我们获得的线程的执行率不少于50%或更多，如图 3.23. </p>
<blockquote>
<p><img src="cpu-memory/assets/27113721_Do9s.png" alt="img" /></p>
<p><strong>图 3.23: 最小缓存命中率-加速</strong> </p>
</blockquote>
<p>X轴表示单线程指令的缓存命中率Ghit，Y轴表示多线程指令所需的缓存命中率。这个值永远不能高于单线程命中率，否则，单线程指令也会使用改良的指令。为了使单线程的命中率在低于55%的所有情况下优于使用多线程，cup要或多或少的足够空闲因为缓存丢失会运行另外一个超线程。 </p>
<p>绿色区域是我们的目标。如果线程的速度没有慢过50%，而每个线程的工作量只有原来的一半，那么它们合起来的耗时应该会少于单线程的耗时。对我们用的示例系统来说(使用超线程的P4机器)，如果单线程代码的命中率为60%，那么多线程代码至少要达到10%才能获得收益。这个要求一般来说还是可以做到的。但是，如果单线程代码的命中率达到了95%，那么多线程代码要做到80%才行。这就很难了。而且，这里还涉及到超线程，在两个超线程的情况下，每个超线程只能分到一半的有效缓存。因为所有超线程是使用同一个缓存来装载数据的，如果两个超线程的工作集没有重叠，那么原始的95%也会被打对折——47%，远低于80%。</p>
<p>因此，超线程只在某些情况下才比较有用。单线程代码的缓存命中率必须低到一定程度，从而使缓存容量变小时新的命中率仍能满足要求。只有在这种情况下，超线程才是有意义的。在实践中，采用超线程能否获得更快的结果，取决于处理器能否有效地将每个进程的等待时间与其它进程的执行时间重叠在一起。并行化也需要一定的开销，需要加到总的运行时间里，这个开销往往是不能忽略的。 </p>
<p>在6.3.4节中，我们会介绍一种技术，它将多个线程通过公用缓存紧密地耦合起来。这种技术适用于许多场合，前提是程序员们乐意花费时间和精力扩展自己的代码。 </p>
<p>如果两个超线程执行完全不同的代码(两个线程就像被当成两个处理器，分别执行不同进程)，那么缓存容量就真的会降为一半，导致缓冲未命中率大为攀升，这一点应该是很清楚的。这样的调度机制是很有问题的，除非你的缓存足够大。所以，除非程序的工作集设计得比较合理，能够确实从超线程获益，否则还是建议在BIOS中把超线程功能关掉。{我们可能会因为另一个原因  <em>开启**超线程，那就是调试，因为SMT在查找并行代码的问题方面真的非常好用。</em>}</p>
<h4 id="335-其它细节"><a class="header" href="#335-其它细节">3.3.5 其它细节</a></h4>
<p>我们已经介绍了地址的组成，即标签、集合索引和偏移三个部分。那么，实际会用到什么样的地址呢？目前，处理器一般都向进程提供虚拟地址空间，意味着我们有两种不同的地址: 虚拟地址和物理地址。 </p>
<p>虚拟地址有个问题——并不唯一。随着时间的变化，虚拟地址可以变化，指向不同的物理地址。同一个地址在不同的进程里也可以表示不同的物理地址。那么，是不是用物理地址会比较好呢？ </p>
<p>问题是，处理器指令用的虚拟地址，而且需要在内存管理单元(MMU)的协助下将它们翻译成物理地址。这并不是一个很小的操作。在执行指令的管线(pipeline)中，物理地址只能在很后面的阶段才能得到。这意味着，缓存逻辑需要在很短的时间里判断地址是否已被缓存过。而如果可以使用虚拟地址，缓存查找操作就可以更早地发生，一旦命中，就可以马上使用内存的内容。结果就是，使用虚拟内存后，可以让管线把更多内存访问的开销隐藏起来。</p>
<p>处理器的设计人员们现在使用虚拟地址来标记第一级缓存。这些缓存很小，很容易被清空。在进程页表树发生变更的情况下，至少是需要清空部分缓存的。如果处理器拥有指定变更地址范围的指令，那么可以避免缓存的完全刷新。由于一级缓存L1i及L1d的时延都很小(~3周期)，基本上必须使用虚拟地址。 </p>
<p>对于更大的缓存，包括L2和L3等，则需要以物理地址作为标签。因为这些缓存的时延比较大，虚拟到物理地址的映射可以在允许的时间里完成，而且由于主存时延的存在，重新填充这些缓存会消耗比较长的时间，刷新的代价比较昂贵。 </p>
<p>一般来说，我们并不需要了解这些缓存处理地址的细节。我们不能更改它们，而那些可能影响性能的因素，要么是应该避免的，要么是有很高代价的。填满缓存是不好的行为，缓存线都落入同一个集合，也会让缓存早早地出问题。对于后一个问题，可以通过缓存虚拟地址来避免，但作为一个用户级程序，是不可能避免缓存物理地址的。我们唯一可以做的，是尽最大努力不要在同一个进程里用多个虚拟地址映射同一个物理地址。</p>
<p>另一个细节对程序员们来说比较乏味，那就是缓存的替换策略。大多数缓存会优先逐出最近最少使用(Least  Recently  Used,LRU)的元素。这往往是一个效果比较好的策略。在关联性很大的情况下(随着以后核心数的增加，关联性势必会变得越来越大)，维护LRU列表变得越来越昂贵，于是我们开始看到其它的一些策略。 </p>
<p>在缓存的替换策略方面，程序员可以做的事情不多。如果缓存使用物理地址作为标签，我们是无法找出虚拟地址与缓存集之间关联的。有可能会出现这样的情形:  所有逻辑页中的缓存线都映射到同一个缓存集，而其它大部分缓存却空闲着。即使有这种情况，也只能依靠OS进行合理安排，避免频繁出现。 </p>
<p>虚拟化的出现使得这一切变得更加复杂。现在不仅操作系统可以控制物理内存的分配。虚拟机监视器（VMM，也称为 hypervisor）也负责分配内存。 </p>
<p>对程序员来说，最好 a) 完全使用逻辑内存页面 b) 在有意义的情况下，使用尽可能大的页面大小来分散物理地址。更大的页面大小也有其他好处，不过这是另一个话题（见第4节）。 </p>
<h3 id="34-指令缓存"><a class="header" href="#34-指令缓存">3.4 指令缓存</a></h3>
<p>其实，不光处理器使用的数据被缓存，它们执行的指令也是被缓存的。只不过，指令缓存的问题相对来说要少得多，因为: </p>
<ul>
<li>执行的代码量取决于代码大小。而代码大小通常取决于问题复杂度。问题复杂度则是固定的。 </li>
<li>程序的数据处理逻辑是程序员设计的，而程序的指令却是编译器生成的。编译器的作者知道如何生成优良的代码。 </li>
<li>程序的流向比数据访问模式更容易预测。现如今的CPU很擅长模式检测，对预取很有利。 </li>
<li>代码永远都有良好的时间局部性和空间局部性。 </li>
</ul>
<p>有一些准则是需要程序员们遵守的，但大都是关于如何使用工具的，我们会在第6节介绍它们。而在这里我们只介绍一下指令缓存的技术细节。 </p>
<p>随着CPU的核心频率大幅上升，缓存与核心的速度差越拉越大，CPU的处理开始管线化。也就是说，指令的执行分成若干阶段。首先，对指令进行解码，随后，准备参数，最后，执行它。这样的管线可以很长(例如，Intel的Netburst架构超过了20个阶段)。在管线很长的情况下，一旦发生延误(即指令流中断)，需要很长时间才能恢复速度。管线延误发生在这样的情况下: 下一条指令未能正确预测，或者装载下一条指令耗时过长(例如，需要从内存读取时)。</p>
<p>为了解决这个问题，CPU的设计人员们在分支预测上投入大量时间和芯片资产(chip real estate)，以降低管线延误的出现频率。 </p>
<p>在CISC处理器上，指令的解码阶段也需要一些时间。x86及x86-64处理器尤为严重。近年来，这些处理器不再将指令的原始字节序列存入L1i，而是缓存解码后的版本。这样的L1i被叫做“追踪缓存(trace cache)”。追踪缓存可以在命中的情况下让处理器跳过管线最初的几个阶段，在管线发生延误时尤其有用。 </p>
<p>前面说过，L2以上的缓存是统一缓存，既保存代码，也保存数据。显然，这里保存的代码是原始字节序列，而不是解码后的形式。 </p>
<p>在提高性能方面，与指令缓存相关的只有很少的几条准则: </p>
<ol>
<li>生成尽量少的代码。也有一些例外，如出于管线化的目的需要更多的代码，或使用小代码会带来过高的额外开销。 </li>
<li>尽量帮助处理器作出良好的预取决策，可以通过代码布局或显式预取来实现。 </li>
</ol>
<p>这些准则一般会由编译器的代码生成阶段强制执行。至于程序员可以参与的部分，我们会在第6节介绍。 </p>
<h4 id="341-自修改的代码"><a class="header" href="#341-自修改的代码">3.4.1 自修改的代码</a></h4>
<p>在计算机的早期岁月里，内存十分昂贵。人们想尽千方百计，只为了尽量压缩程序容量，给数据多留一些空间。其中，有一种方法是修改程序自身，称为自修改代码(SMC)。现在，有时候我们还能看到它，一般是出于提高性能的目的，也有的是为了攻击安全漏洞。 </p>
<p>一般情况下，应该避免SMC。虽然一般情况下没有问题，但有时会由于执行错误而出现性能问题。显然，发生改变的代码是无法放入追踪缓存(追踪缓存放的是解码后的指令)的。即使没有使用追踪缓存(代码还没被执行或有段时间没执行)，处理器也可能会遇到问题。如果某个进入管线的指令发生了变化，处理器只能扔掉目前的成果，重新开始。在某些情况下，甚至需要丢弃处理器的大部分状态。 </p>
<p>最后，由于处理器认为代码页是不可修改的(这是出于简单化的考虑，而且在99.9999999%情况下确实是正确的)，L1i用到并不是MESI协议，而是一种简化后的SI协议。这样一来，如果万一检测到修改的情况，就需要作出大量悲观的假设。 </p>
<p>因此，对于SMC，强烈建议能不用就不用。现在内存已经不再是一种那么稀缺的资源了。最好是写多个函数，而不要根据需要把一个函数改来改去。也许有一天可以把SMC变成可选项，我们就能通过这种方式检测入侵代码。如果一定要用SMC，应该让写操作越过缓存，以免由于L1i需要L1d里的数据而产生问题。更多细节，请参见6.1节。 </p>
<p>在Linux上，判断程序是否包含SMC是很容易的。利用正常工具链(toolchain)构建的程序代码都是写保护(write-protected)的。程序员需要在链接时施展某些关键的魔术才能生成可写的代码页。现代的Intel x86和x86-64处理器都有统计SMC使用情况的专用计数器。通过这些计数器，我们可以很容易判断程序是否包含SMC，即使它被准许运行。 </p>
<h3 id="35-缓存未命中的因素"><a class="header" href="#35-缓存未命中的因素">3.5 缓存未命中的因素</a></h3>
<p>我们已经看过内存访问没有命中缓存时，那陡然猛涨的高昂代价。但是有时候，这种情况又是无法避免的，因此我们需要对真正的代价有所认识，并学习如何缓解这种局面。 </p>
<h4 id="351-缓存与内存带宽"><a class="header" href="#351-缓存与内存带宽">3.5.1 缓存与内存带宽</a></h4>
<p>为了更好地理解处理器的能力，我们测量了各种理想环境下能够达到的带宽值。由于不同处理器的版本差别很大，所以这个测试比较有趣，也因为如此，这一节都快被测试数据灌满了。我们使用了x86和x86-64处理器的SSE指令来装载和存储数据，每次16字节。工作集则与其它测试一样，从1kB增加到512MB，测量的具体对象是每个周期所处理的字节数。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113719_aUHT.png" alt="img" /><br />
<strong>图3.24: P4的带宽</strong> </p>
</blockquote>
<p>图3.24展示了一颗64位Intel  Netburst处理器的性能图表。当工作集能够完全放入L1d时，处理器的每个周期可以读取完整的16字节数据，即每个周期执行一条装载指令(moveaps指令，每次移动16字节的数据)。测试程序并不对数据进行任何处理，只是测试读取指令本身。当工作集增大，无法再完全放入L1d时，性能开始急剧下降，跌至每周期6字节。在218工作集处出现的台阶是由于DTLB缓存耗尽，因此需要对每个新页施加额外处理。由于这里的读取是按顺序的，预取机制可以完美地工作，而FSB能以5.3字节/周期的速度传输内容。但预取的数据并不进入L1d。当然，真实世界的程序永远无法达到以上的数字，但我们可以将它们看作一系列实际上的极限值。 </p>
<p>更令人惊讶的是写操作和复制操作的性能。即使是在很小的工作集下，写操作也始终无法达到4字节/周期的速度。这意味着，Intel为Netburst处理器的L1d选择了写通(write-through)模式，所以写入性能受到L2速度的限制。同时，这也意味着，复制测试的性能不会比写入测试差太多(复制测试是将某块内存的数据拷贝到另一块不重叠的内存区)，因为读操作很快，可以与写操作实现部分重叠。最值得关注的地方是，两个操作在工作集无法完全放入L2后出现了严重的性能滑坡，降到了0.5字节/周期！比读操作慢了10倍！显然，如果要提高程序性能，优化这两个操作更为重要。</p>
<p>再来看图3.25，它来自同一颗处理器，只是运行双线程，每个线程分别运行在处理器的一个超线程上。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113719_kjeK.png" alt="img" /><br />
<strong>图3.25: P4开启两个超线程时的带宽表现</strong> </p>
</blockquote>
<p>图3.25采用了与图3.24相同的刻度，以方便比较两者的差异。图3.25中的曲线抖动更多，是由于采用双线程的缘故。结果正如我们预期，由于超线程共享着几乎所有资源(仅除寄存器外)，所以每个超线程只能得到一半的缓存和带宽。所以，即使每个线程都要花上许多时间等待内存，从而把执行时间让给另一个线程，也是无济于事——因为另一个线程也同样需要等待。这里恰恰展示了使用超线程时可能出现的最坏情况。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113720_Rz7T.png" alt="img" /><br />
<strong>图3.26: Core 2的带宽表现</strong> </p>
</blockquote>
<p>再来看Core 2处理器的情况。看看图3.26和图3.27，再对比下P4的图3.24和3.25，可以看出不小的差异。Core 2是一颗双核处理器，有着共享的L2，容量是P4 L2的4倍。但更大的L2只能解释写操作的性能下降出现较晚的现象。 </p>
<p>当然还有更大的不同。可以看到，读操作的性能在整个工作集范围内一直稳定在16字节/周期左右，在220处的下降同样是由于DTLB的耗尽引起。能够达到这么高的数字，不但表明处理器能够预取数据，并且按时完成传输，而且还意味着，预取的数据是被装入L1d的。 </p>
<p>写/复制操作的性能与P4相比，也有很大差异。处理器没有采用写通策略，写入的数据留在L1d中，只在必要时才逐出。这使得写操作的速度可以逼近16字节/周期。一旦工作集超过L1d，性能即飞速下降。由于Core 2读操作的性能非常好，所以两者的差值显得特别大。当工作集超过L2时，两者的差值甚至超过20倍！但这并不表示Core  2的性能不好，相反，Core 2永远都比Netburst强。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113720_XDgl.png" alt="img" /><br />
<strong>图3.27: Core 2运行双线程时的带宽表现</strong> </p>
</blockquote>
<p>在图3.27中，启动双线程，各自运行在Core 2的一个核心上。它们访问相同的内存，但不需要完美同步。从结果上看，读操作的性能与单线程并无区别，只是多了一些多线程情况下常见的抖动。 </p>
<p>有趣的地方来了——当工作集小于L1d时，写操作与复制操作的性能很差，就好像数据需要从内存读取一样。两个线程彼此竞争着同一个内存位置，于是不得不频频发送RFO消息。问题的根源在于，虽然两个核心共享着L2，但无法以L2的速度处理RFO请求。而当工作集超过L1d后，性能出现了迅猛提升。这是因为，由于L1d容量不足，于是将被修改的条目刷新到共享的L2。由于L1d的未命中可以由L2满足，只有那些尚未刷新的数据才需要RFO，所以出现了这样的现象。这也是这些工作集情况下速度下降一半的原因。这种渐进式的行为也与我们期待的一致: 由于每个核心共享着同一条FSB，每个核心只能得到一半的FSB带宽，因此对于较大的工作集来说，每个线程的性能大致相当于单线程时的一半。</p>
<p>由于同一个厂商的不同处理器之间都存在着巨大差异，我们没有理由不去研究一下其它厂商处理器的性能。图3.28展示了AMD家族10h Opteron处理器的性能。这颗处理器有64kB的L1d、512kB的L2和2MB的L3，其中L3缓存由所有核心所共享。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113720_O0pJ.png" alt="img" /><br />
<strong>图3.28: AMD家族10h Opteron的带宽表现</strong> </p>
</blockquote>
<p>大家首先应该会注意到，在L1d缓存足够的情况下，这个处理器每个周期能处理两条指令。读操作的性能超过了32字节/周期，写操作也达到了18.7字节/周期。但是，不久，读操作的曲线就急速下降，跌到2.3字节/周期，非常差。处理器在这个测试中并没有预取数据，或者说，没有有效地预取数据。 </p>
<p>另一方面，写操作的曲线随几级缓存的容量而流转。在L1d阶段达到最高性能，随后在L2阶段下降到6字节/周期，在L3阶段进一步下降到2.8字节/周期，最后，在工作集超过L3后，降到0.5字节/周期。它在L1d阶段超过了Core 2，在L2阶段基本相当(Core 2的L2更大一些)，在L3及主存阶段比Core 2慢。 </p>
<p>复制的性能既无法超越读操作的性能，也无法超越写操作的性能。因此，它的曲线先是被读性能压制，随后又被写性能压制。 </p>
<p>图3.29显示的是Opteron处理器在多线程时的性能表现。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113720_XTVx.png" alt="img" /><br />
<strong>图3.29: AMD Fam 10h在双线程时的带宽表现</strong> </p>
</blockquote>
<p>读操作的性能没有受到很大的影响。每个线程的L1d和L2表现与单线程下相仿，L3的预取也依然表现不佳。两个线程并没有过渡争抢L3。问题比较大的是写操作的性能。两个线程共享的所有数据都需要经过L3，而这种共享看起来却效率很差。即使是在L3足够容纳整个工作集的情况下，所需要的开销仍然远高于L3的访问时间。再来看图3.27，可以发现，在一定的工作集范围内，Core  2处理器能以共享的L2缓存的速度进行处理。而Opteron处理器只能在很小的一个范围内实现相似的性能，而且，它仅仅只能达到L3的速度，无法与Core 2的L2相比。 </p>
<p><strong>3.5.2 关键字加载</strong> </p>
<p>内存以比缓存线还小的块从主存储器向缓存传送。如今64位可一次性传送，缓存线的大小为64或128比特。这意味着每个缓存线需要8或16次传送。 </p>
<p>DRAM芯片可以以触发模式传送这些64位的块。这使得不需要内存控制器的进一步指令和可能伴随的延迟，就可以将缓存线充满。如果处理器预取了缓存，这有可能是最好的操作方式。 </p>
<p>如果程序在访问数据或指令缓存时没有命中(这可能是强制性未命中或容量性未命中，前者是由于数据第一次被使用，后者是由于容量限制而将缓存线逐出)，情况就不一样了。程序需要的并不总是缓存线中的第一个字，而数据块的到达是有先后顺序的，即使是在突发模式和双倍传输率下，也会有明显的时间差，一半在4个CPU周期以上。举例来说，如果程序需要缓存线中的第8个字，那么在首字抵达后它还需要额外等待30个周期以上。 </p>
<p>当然，这样的等待并不是必需的。事实上，内存控制器可以按不同顺序去请求缓存线中的字。当处理器告诉它，程序需要缓存中具体某个字，即「关键字(critical  word)」时，内存控制器就会先请求这个字。一旦请求的字抵达，虽然缓存线的剩余部分还在传输中，缓存的状态还没有达成一致，但程序已经可以继续运行。这种技术叫做关键字优先及较早重启(Critical Word First &amp; Early Restart)。 </p>
<p>现在的处理器都已经实现了这一技术，但有时无法运用。比如，预取操作的时候，并不知道哪个是关键字。如果在预取的中途请求某条缓存线，处理器只能等待，并不能更改请求的顺序。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113720_2RwQ.png" alt="img" /><br />
<strong>图3.30: 关键字位于缓存线尾时的表现</strong> </p>
</blockquote>
<p>在关键字优先技术生效的情况下，关键字的位置也会影响结果。图3.30展示了下一个测试的结果，图中表示的是关键字分别在线首和线尾时的性能对比情况。元素大小为64字节，等于缓存线的长度。图中的噪声比较多，但仍然可以看出，当工作集超过L2后，关键字处于线尾情况下的性能要比线首情况下低0.7%左右。而顺序访问时受到的影响更大一些。这与我们前面提到的预取下条线时可能遇到的问题是相符的。 </p>
<p><strong>3.5.3 缓存设定</strong> </p>
<p>缓存放置的位置与超线程，内核和处理器之间的关系，不在程序员的控制范围之内。但是程序员可以决定线程执行的位置，接着高速缓存与使用的CPU的关系将变得非常重要。 </p>
<p>这里我们将不会深入（探讨）什么时候选择什么样的内核以运行线程的细节。我们仅仅描述了在设置关联线程的时候，程序员需要考虑的系统结构的细节。 </p>
<p>超线程，通过定义，共享除去寄存器集以外的所有数据。包括 L1 缓存。这里没有什么可以多说的。多核处理器的独立核心带来了一些乐趣。每个核心都至少拥有自己的 L1 缓存。除此之外，下面列出了一些不同的特性： </p>
<ul>
<li>早期多核心处理器有独立的 L2 缓存且没有更高层级的缓存。 </li>
<li>之后英特尔的双核心处理器模型拥有共享的L2 缓存。对四核处理器，则分对拥有独立的L2 缓存，且没有更高层级的缓存。 </li>
<li>AMD 家族的 10h 处理器有独立的 L2 缓存以及一个统一的L3 缓存。 </li>
</ul>
<p>关于各种处理器模型的优点，已经在它们各自的宣传手册里写得够多了。在每个核心的工作集互不重叠的情况下，独立的L2拥有一定的优势，单线程的程序可以表现优良。考虑到目前实际环境中仍然存在大量类似的情况，这种方法的表现并不会太差。不过，不管怎样，我们总会遇到工作集重叠的情况。如果每个缓存都保存着某些通用运行库的常用部分，那么很显然是一种浪费。 </p>
<p>如果像Intel的双核处理器那样，共享除L1外的所有缓存，则会有一个很大的优点。如果两个核心的工作集重叠的部分较多，那么综合起来的可用缓存容量会变大，从而允许容纳更大的工作集而不导致性能的下降。如果两者的工作集并不重叠，那么则是由Intel的高级智能缓存管理(Advanced Smart Cache management)发挥功用，防止其中一个核心垄断整个缓存。 </p>
<p>即使每个核心只使用一半的缓存，也会有一些摩擦。缓存需要不断衡量每个核心的用量，在进行逐出操作时可能会作出一些比较差的决定。我们来看另一个测试程序的结果。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113720_DTZJ.png" alt="img" /><br />
<strong>图3.31: 两个进程的带宽表现</strong> </p>
</blockquote>
<p>这次，测试程序两个进程，第一个进程不断用SSE指令读/写2MB的内存数据块，选择2MB，是因为它正好是Core  2处理器L2缓存的一半，第二个进程则是读/写大小变化的内存区域，我们把这两个进程分别固定在处理器的两个核心上。图中显示的是每个周期读/写的字节数，共有4条曲线，分别表示不同的读写搭配情况。例如，标记为读/写(read/write)的曲线代表的是后台进程进行写操作(固定2MB工作集)，而被测量进程进行读操作(工作集从小到大)。 </p>
<p>图中最有趣的是220到223之间的部分。如果两个核心的L2是完全独立的，那么所有4种情况下的性能下降均应发生在221到222之间，也就是L2缓存耗尽的时候。但从图上来看，实际情况并不是这样，特别是背景进程进行写操作时尤为明显。当工作集达到1MB(220)时，性能即出现恶化，两个进程并没有共享内存，因此并不会产生RFO消息。所以，完全是缓存逐出操作引起的问题。目前这种智能的缓存处理机制有一个问题，每个核心能实际用到的缓存更接近1MB，而不是理论上的2MB。如果未来的处理器仍然保留这种多核共享缓存模式的话，我们唯有希望厂商会把这个问题解决掉。</p>
<p>推出拥有双L2缓存的4核处理器仅仅只是一种临时措施，是开发更高级缓存之前的替代方案。与独立插槽及双核处理器相比，这种设计并没有带来多少性能提升。两个核心是通过同一条总线(被外界看作FSB)进行通信，并没有什么特别快的数据交换通道。 </p>
<p>未来，针对多核处理器的缓存将会包含更多层次。AMD的10h家族是一个开始，至于会不会有更低级共享缓存的出现，还需要我们拭目以待。我们有必要引入更多级别的缓存，因为频繁使用的高速缓存不可能被许多核心共用，否则会对性能造成很大的影响。我们也需要更大的高关联性缓存，它们的数量、容量和关联性都应该随着共享核心数的增长而增长。巨大的L3和适度的L2应该是一种比较合理的选择。L3虽然速度较慢，但也较少使用。 </p>
<p>对于程序员来说，不同的缓存设计就意味着调度决策时的复杂性。为了达到最高的性能，我们必须掌握工作负载的情况，必须了解机器架构的细节。好在我们在判断机器架构时还是有一些支援力量的，我们会在后面的章节介绍这些接口。 </p>
<h4 id="354-fsb的影响"><a class="header" href="#354-fsb的影响">3.5.4 FSB的影响</a></h4>
<p>FSB在性能中扮演了核心角色。缓存数据的存取速度受制于内存通道的速度。我们做一个测试，在两台机器上分别跑同一个程序，这两台机器除了内存模块的速度有所差异，其它完全相同。图3.32展示了Addnext0测试(将下一个元素的pad[0]加到当前元素的pad[0]上)在这两台机器上的结果(NPAD=7，64位机器)。两台机器都采用Core 2处理器，一台使用667MHz的DDR2内存，另一台使用800MHz的DDR2内存(比前一台增长20%)。 </p>
<blockquote>
<p><img src="cpu-memory/assets/27113721_Do9s.png" alt="img" /><br />
<strong>图3.32: FSB速度的影响</strong> </p>
</blockquote>
<p>图上的数字表明，当工作集大到对FSB造成压力的程度时，高速FSB确实会带来巨大的优势。在我们的测试中，性能的提升达到了18.5%，接近理论上的极限。而当工作集比较小，可以完全纳入缓存时，FSB的作用并不大。当然，这里我们只测试了一个程序的情况，在实际环境中，系统往往运行多个进程，工作集是很容易超过缓存容量的。 </p>
<p>如今，一些英特尔的处理器，支持前端总线(FSB)的速度高达1,333  MHz，这意味着速度有另外60％的提升。将来还会出现更高的速度。速度是很重要的，工作集会更大，快速的RAM和高FSB速度的内存肯定是值得投资的。我们必须小心使用它，因为即使处理器可以支持更高的前端总线速度，但是主板的北桥芯片可能不会。使用时，检查它的规范是至关重要的。</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="4-虚拟内存"><a class="header" href="#4-虚拟内存"><strong>4 虚拟内存</strong></a></h2>
<p>处理器的虚拟内存子系统为每个进程实现了虚拟地址空间。这让每个进程认为它在系统中是独立的。虚拟内存的优点列表别的地方描述的非常详细，所以这里就不重复了。本节集中在虚拟内存的实际的实现细节，和相关的成本。 </p>
<p>虚拟地址空间是由CPU的内存管理单元(MMU)实现的。OS必须填充页表数据结构，但大多数CPU自己做了剩下的工作。这事实上是一个相当复杂的机制；最好的理解它的方法是引入数据结构来描述虚拟地址空间。 </p>
<p>由MMU进行地址翻译的输入地址是虚拟地址。通常对它的值很少有限制 — 假设还有一点的话。  虚拟地址在32位系统中是32位的数值，在64位系统中是64位的数值。在一些系统，例如x86和x86-64，使用的地址实际上包含了另一个层次的间接寻址：这些结构使用分段，这些分段只是简单的给每个逻辑地址加上位移。我们可以忽略这一部分的地址产生，它不重要，不是程序员非常关心的内存处理性能方面的东西。{<em>x86的分段限制是与性能相关的，但那是另一回事了</em>} </p>
<h3 id="41-最简单的地址转换"><a class="header" href="#41-最简单的地址转换">4.1 最简单的地址转换</a></h3>
<p>有趣的地方在于由虚拟地址到物理地址的转换。MMU可以在逐页的基础上重新映射地址。就像地址缓存排列的时候，虚拟地址被分割为不同的部分。这些部分被用来做多个表的索引，而这些表是被用来创建最终物理地址用的。最简单的模型是只有一级表。 </p>
<blockquote>
<p><img src="cpu-memory/assets/18143754_rBvU.png" alt="img" /></p>
<p><strong>Figure 4.1: 1-Level Address Translation</strong> </p>
</blockquote>
<p>图 4.1  显示了虚拟地址的不同部分是如何使用的。高字节部分是用来选择一个页目录的条目；那个目录中的每个地址可以被OS分别设置。页目录条目决定了物理内存页的地址；页面中可以有不止一个条目指向同样的物理地址。完整的内存物理地址是由页目录获得的页地址和虚拟地址低字节部分合并起来决定的。页目录条目还包含一些附加的页面信息，如访问权限。 </p>
<p>页目录的数据结构存储在内存中。OS必须分配连续的物理内存，并将这个地址范围的基地址存入一个特殊的寄存器。然后虚拟地址的适当的位被用来作为页目录的索引，这个页目录事实上是目录条目的列表。 </p>
<p>作为一个具体的例子，这是  x86机器4MB分页设计。虚拟地址的位移部分是22位大小，足以定位一个4M页内的每一个字节。虚拟地址中剩下的10位指定页目录中1024个条目的一个。每个条目包括一个10位的4M页内的基地址，它与位移结合起来形成了一个完整的32位地址。 </p>
<h3 id="42-多级页表"><a class="header" href="#42-多级页表">4.2 多级页表</a></h3>
<p>4MB的页不是规范，它们会浪费很多内存，因为OS需要执行的许多操作需要内存页的队列。对于4kB的页（32位机器的规范，甚至通常是64位机器的规范），虚拟地址的位移部分只有12位大小。这留下了20位作为页目录的指针。具有220个条目的表是不实际的。即使每个条目只要4比特，这个表也要4MB大小。由于每个进程可能具有其唯一的页目录，因为这些页目录许多系统中物理内存被绑定起来。 </p>
<p>解决办法是用多级页表。然后这些就能表示一个稀疏的大的页目录，目录中一些实际不用的区域不需要分配内存。因此这种表示更紧凑，使它可能为内存中的很多进程使用页表而并不太影响性能。. </p>
<p>今天最复杂的页表结构由四级构成。图4.2显示了这样一个实现的原理图。 </p>
<blockquote>
<p><img src="cpu-memory/assets/18143754_BaNt.png" alt="img" /></p>
<p><strong>Figure 4.2: 4-Level Address Translation</strong> </p>
</blockquote>
<p>在这个例子中，虚拟地址被至少分为五个部分。其中四个部分是不同的目录的索引。被引用的第4级目录使用CPU中一个特殊目的的寄存器。第4级到第2级目录的内容是对次低一级目录的引用。如果一个目录条目标识为空，显然就是不需要指向任何低一级的目录。这样页表树就能稀疏和紧凑。正如图4.1，第1级目录的条目是一部分物理地址，加上像访问权限的辅助数据。 </p>
<p>为了决定相对于虚拟地址的物理地址，处理器先决定最高级目录的地址。这个地址一般保存在一个寄存器。然后CPU取出虚拟地址中相对于这个目录的索引部分，并用那个索引选择合适的条目。这个条目是下一级目录的地址，它由虚拟地址的下一部分索引。处理器继续直到它到达第1级目录，那里那个目录条目的值就是物理地址的高字节部分。物理地址在加上虚拟地址中的页面位移之后就完整了。这个过程称为页面树遍历。一些处理器（像x86和x86-64）在硬件中执行这个操作，其他的需要OS的协助。</p>
<p>系统中运行的每个进程可能需要自己的页表树。有部分共享树的可能，但是这相当例外。因此如果页表树需要的内存尽可能小的话将对性能与可扩展性有利。理想的情况是将使用的内存紧靠着放在虚拟地址空间；但实际使用的物理地址不影响。一个小程序可能只需要第2，3，4级的一个目录和少许第1级目录就能应付过去。在一个采用4kB页面和每个目录512条目的x86-64机器上，这允许用4级目录对2MB定位（每一级一个）。1GB连续的内存可以被第2到第4级的一个目录和第1级的512个目录定位。 </p>
<p>但是，假设所有内存可以被连续分配是太简单了。由于复杂的原因，大多数情况下，一个进程的栈与堆的区域是被分配在地址空间中非常相反的两端。这样使得任一个区域可以根据需要尽可能的增长。这意味着最有可能需要两个第2级目录和相应的更多的低一级的目录。 </p>
<p>但即使这也不常常匹配现在的实际。由于安全的原因，一个可运行的（代码，数据，堆，栈，动态共享对象，aka共享库）不同的部分被映射到随机的地址[未选中的]。随机化延伸到不同部分的相对位置；那意味着一个进程使用的不同的内存范围，遍布于虚拟地址空间。通过对随机的地址位数采用一些限定，范围可以被限制，但在大多数情况下，这当然不会让一个进程只用一到两个第2和第3级目录运行。 </p>
<p>如果性能真的远比安全重要，随机化可以被关闭。OS然后通常是在虚拟内存中至少连续的装载所有的动态共享对象(DSO)。 </p>
<h3 id="43-优化页表访问"><a class="header" href="#43-优化页表访问">4.3 优化页表访问</a></h3>
<p>页表的所有数据结构都保存在主存中；在那里OS建造和更新这些表。当一个进程创建或者一个页表变化，CPU将被通知。页表被用来解决每个虚拟地址到物理地址的转换，用上面描述的页表遍历方式。更多有关于此：至少每一级有一个目录被用于处理虚拟地址的过程。这需要至多四次内存访问（对一个运行中的进程的单次访问来说），这很慢。有可能像普通数据一样处理这些目录表条目，并将他们缓存在L1d,L2等等，但这仍然非常慢。 </p>
<p>从虚拟内存的早期阶段开始，CPU的设计者采用了一种不同的优化。简单的计算显示，只有将目录表条目保存在L1d和更高级的缓存，才会导致可怕的性能问题。每个绝对地址的计算，都需要相对于页表深度的大量的L1d访问。这些访问不能并行，因为它们依赖于前面查询的结果。在一个四级页表的机器上，这种单线性将 至少至少需要12次循环。再加上L1d的非命中的可能性，结果是指令流水线没有什么能隐藏的。额外的L1d访问也消耗了珍贵的缓存带宽。</p>
<p>所以，替代于只是缓存目录表条目，物理页地址的完整的计算结果被缓存了。因为同样的原因，代码和数据缓存也工作起来，这样的地址计算结果的缓存是高效的。由于虚拟地址的页面位移部分在物理页地址的计算中不起任何作用，只有虚拟地址的剩余部分被用作缓存的标签。根据页面大小这意味着成百上千的指令或数据对象共享同一个标签，因此也共享同一个物理地址前缀。 </p>
<p>保存计算数值的缓存叫做旁路转换缓存(TLB)。因为它必须非常的快，通常这是一个小的缓存。现代CPU像其它缓存一样，提供了多级TLB缓存；越高级的缓存越大越慢。小号的L1级TLB通常被用来做全相联映像缓存，采用LRU回收策略。最近这种缓存大小变大了，而且在处理器中变得集相联。其结果之一就是，当一个新的条目必须被添加的时候，可能不是最久的条目被回收于替换了。 </p>
<p>正如上面提到的，用来访问TLB的标签是虚拟地址的一个部分。如果标签在缓存中有匹配，最终的物理地址将被计算出来，通过将来自虚拟地址的页面位移地址加到缓存值的方式。这是一个非常快的过程；也必须这样，因为每条使用绝对地址的指令都需要物理地址，还有在一些情况下，因为使用物理地址作为关键字的L2查找。如果TLB查询未命中，处理器就必须执行一次页表遍历；这可能代价非常大。 </p>
<p>通过软件或硬件预取代码或数据，会在地址位于另一页面时，暗中预取TLB的条目。硬件预取不可能允许这样，因为硬件会初始化非法的页面表遍历。因此程序员不能依赖硬件预取机制来预取TLB条目。它必须使用预取指令明确的完成。就像数据和指令缓存，TLB可以表现为多个等级。正如数据缓存，TLB通常表现为两种形式：指令TLB(ITLB)和数据TLB(DTLB)。高级的TLB像L2TLB通常是统一的，就像其他的缓存情形一样。 </p>
<p><strong>4.3.1 使用TLB的注意事项</strong> </p>
<p>TLB是以处理器为核心的全局资源。所有运行于处理器的线程与进程使用同一个TLB。由于虚拟到物理地址的转换依赖于安装的是哪一种页表树，如果页表变化了，CPU不能盲目的重复使用缓存的条目。每个进程有一个不同的页表树（不算在同一个进程中的线程），内核与内存管理器VMM(管理程序)也一样，如果存在的话。也有可能一个进程的地址空间布局发生变化。有两种解决这个问题的办法： </p>
<ul>
<li>当页表树变化时TLB刷新。 </li>
<li>TLB条目的标签附加扩展并唯一标识其涉及的页表树</li>
</ul>
<p>第一种情况，只要执行一个上下文切换TLB就被刷新。因为大多数OS中，从一个线程/进程到另一个的切换需要执行一些核心代码，TLB刷新被限制进入或离开核心地址空间。在虚拟化的系统中，当内核必须调用内存管理器VMM和返回的时候，这也会发生。如果内核和/或内存管理器没有使用虚拟地址，或者当进程或内核调用系统/内存管理器时，能重复使用同一个虚拟地址，TLB必须被刷新。当离开内核或内存管理器时，处理器继续执行一个不同的进程或内核。 </p>
<p>刷新TLB高效但昂贵。例如，当执行一个系统调用，触及的内核代码可能仅限于几千条指令，或许少许新页面（或一个大的页面，像某些结构的Linux的就是这样）。这个工作将替换触及页面的所有TLB条目。对Intel带128ITLB和256DTLB条目的Core2架构，完全的刷新意味着多于100和200条目（分别的）将被不必要的刷新。当系统调用返回同一个进程，所有那些被刷新的TLB条目可能被再次用到，但它们没有了。内核或内存管理器常用的代码也一样。每条进入内核的条目上，TLB必须擦去再装，即使内核与内存管理器的页表通常不会改变。因此理论上说，TLB条目可以被保持一个很长时间。这也解释了为什么现在处理器中的TLB缓存都不大：程序很有可能不会执行时间长到装满所有这些条目。</p>
<p>当然事实逃脱不了CPU的结构。对缓存刷新优化的一个可能的方法是单独的使TLB条目失效。例如，如果内核代码与数据落于一个特定的地址范围，只有落入这个地址范围的页面必须被清除出TLB。这只需要比较标签，因此不是很昂贵。在部分地址空间改变的场合，例如对去除内存页的一次调用，这个方法也是有用的， </p>
<p>更好的解决方法是为TLB访问扩展标签。如果除了虚拟地址的一部分之外，一个唯一的对应每个页表树的标识（如一个进程的地址空间）被添加，TLB将根本不需要完全刷新。内核，内存管理程序，和独立的进程都可以有唯一的标识。这种场景唯一的问题在于，TLB标签可以获得的位数异常有限，但是地址空间的位数却不是。这意味着一些标识的再利用是有必要的。这种情况发生时TLB必须部分刷新（如果可能的话）。所有带有再利用标识的条目必须被刷新，但是希望这是一个非常小的集合。 </p>
<p>当多个进程运行在系统中时，这种扩展的TLB标签具有一般优势。如果每个可运行进程对内存的使用（因此TLB条目的使用）做限制，进程最近使用的TLB条目,当其再次列入计划时，有很大机会仍然在TLB。但还有两个额外的优势： </p>
<ol>
<li>特殊的地址空间，像内核和内存管理器使用的那些，经常仅仅进入一小段时间；之后控制经常返回初始化此次调用的地址空间。没有标签，就有两次TLB刷新操作。有标签，调用地址空间缓存的转换地址将被保存，而且由于内核与内存管理器地址空间根本不会经常改变TLB条目，系统调用之前的地址转换等等可以仍然使用。 </li>
<li>当同一个进程的两个线程之间切换时，TLB刷新根本就不需要。虽然没有扩展TLB标签时，进入内核的条目会破坏第一个线程的TLB的条目。 </li>
</ol>
<p>有些处理器在一些时候实现了这些扩展标签。AMD给帕西菲卡（Pacifica）虚拟化扩展引入了一个1位的扩展标签。在虚拟化的上下文中，这个1位的地址空间ID（ASID）被用来从客户域区别出内存管理程序的地址空间。这使得OS能够避免在每次进入内存管理程序的时候（例如为了处理一个页面错误）刷新客户的TLB条目，或者当控制回到客户时刷新内存管理程序的TLB条目。这个架构未来会允许使用更多的位。其它主流处理器很可能会随之适应并支持这个功能。</p>
<p><strong>4.3.2 影响TLB性能</strong> </p>
<p>有一些因素会影响TLB性能。第一个是页面的大小。显然页面越大，装进去的指令或数据对象就越多。所以较大的页面大小减少了所需的地址转换总次数，即需要更少的TLB缓存条目。大多数架构允许使用多个不同的页面尺寸；一些尺寸可以并存使用。例如，x86/x86-64处理器有一个普通的4kB的页面尺寸，但它们也可以分别用4MB和2MB页面。IA-64 和 PowerPC允许如64kB的尺寸作为基本的页面尺寸。 </p>
<p>然而，大页面尺寸的使用也随之带来了一些问题。用作大页面的内存范围必须是在物理内存中连续的。如果物理内存管理的单元大小升至虚拟内存页面的大小，浪费的内存数量将会增长。各种内存操作（如加载可执行文件）需要页面边界对齐。这意味着平均每次映射浪费了物理内存中页面大小的一半。这种浪费很容易累加；因此它给物理内存分配的合理单元大小划定了一个上限。 </p>
<p>在x86-64结构中增加单元大小到2MB来适应大页面当然是不实际的。这是一个太大的尺寸。但这转而意味着每个大页面必须由许多小一些的页面组成。这些小页面必须在物理内存中连续。以4kB单元页面大小分配2MB连续的物理内存具有挑战性。它需要找到有512个连续页面的空闲区域。在系统运行一段时间并且物理内存开始碎片化以后，这可能极为困难（或者不可能） </p>
<p>因此在Linux中有必要在系统启动的时候，用特别的Huge TLBfs文件系统，预分配这些大页面。一个固定数目的物理页面被保留，以单独用作大的虚拟页面。这使可能不会经常用到的资源捆绑留下来。它也是一个有限的池；增大它一般意味着要重启系统。尽管如此，大页面是进入某些局面的方法，在这些局面中性能具有保险性，资源丰富，而且麻烦的安装不会成为大的妨碍。数据库服务器就是一个例子。 </p>
<p>增大最小的虚拟页面大小（正如选择大页面的相反面）也有它的问题。内存映射操作（例如加载应用）必须确认这些页面大小。不可能有更小的映射。对大多数架构来说，一个可执行程序的各个部分位置有一个固定的关系。如果页面大小增加到超过了可执行程序或DSO(Dynamic Shared  Object)创建时考虑的大小，加载操作将无法执行。脑海里记得这个限制很重要。图4.3显示了一个ELF二进制的对齐需求是如何决定的。它编码在ELF程序头部。 </p>
<pre><code>$ eu-readelf -l /bin/ls
Program Headers:
  Type   Offset   VirtAddr           PhysAddr           FileSiz  MemSiz   Flg Align
...
  LOAD   0x000000 0x0000000000400000 0x0000000000400000 0x0132ac 0x0132ac R E 0x200000
  LOAD   0x0132b0 0x00000000006132b0 0x00000000006132b0 0x001a71 0x001a71 RW  0x200000
...
</code></pre>
<p><strong>Figure 4.3: ELF 程序头表明了对齐需求</strong> </p>
<p>在这个例子中，一个x86-64二进制，它的值为0x200000 = 2,097,152 = 2MB，符合处理器支持的最大页面尺寸。 </p>
<p>使用较大内存尺寸有第二个影响：页表树的级数减少了。由于虚拟地址相对于页面位移的部分增加了，需要用来在页目录中使用的位，就没有剩下许多了。这意味着当一个TLB未命中时，需要做的工作数量减少了。 </p>
<p>超出使用大页面大小，它有可能减少移动数据时需要同时使用的TLB条目数目，减少到数页。这与一些上面我们谈论的缓存使用的优化机制类似。只有现在对齐需求是巨大的。考虑到TLB条目数目如此小，这可能是一个重要的优化。 </p>
<h3 id="44-虚拟化的影响"><a class="header" href="#44-虚拟化的影响">4.4 虚拟化的影响</a></h3>
<p>OS映像的虚拟化将变得越来越流行；这意味着另一个层次的内存处理被加入了想象。进程（基本的隔间）或者OS容器的虚拟化，因为只涉及一个OS而没有落入此分类。类似Xen或KVM的技术使OS映像能够独立运行 — 有或者没有处理器的协助。这些情形下，有一个单独的软件直接控制物理内存的访问。 </p>
<blockquote>
<p><img src="cpu-memory/assets/18143755_MZnV.png" alt="img" /></p>
<p><strong>图 4.4: Xen 虚拟化模型</strong> </p>
</blockquote>
<p>对Xen来说（见图4.4），Xen  VMM(Xen内存管理程序)就是那个软件。但是，VMM没有自己实现许多硬件的控制，不像其他早先的系统（包括Xen  VMM的第一个版本）的VMM，内存以外的硬件和处理器由享有特权的Dom0域控制。现在，这基本上与没有特权的DomU内核一样，就内存处理方面而言，它们没有什么不同。这里重要的是，VMM自己分发物理内存给Dom0和DomU内核，然后就像他们是直接运行在一个处理器上一样，实现通常的内存处理 </p>
<p>为了实现完成虚拟化所需的各个域之间的分隔，Dom0和DomU内核中的内存处理不具有无限制的物理内存访问权限。VMM不是通过分发独立的物理页并让客户OS处理地址的方式来分发内存；这不能提供对错误或欺诈客户域的防范。替代的，VMM为每一个客户域创建它自己的页表树，并且用这些数据结构分发内存。好处是对页表树管理信息的访问能得到控制。如果代码没有合适的特权，它不能做任何事。                                                                                      在虚拟化的Xen支持中，这种访问控制已被开发，不管使用的是参数的或硬件的（又名全）虚拟化。客户域以意图上与参数的和硬件的虚拟化极为相似的方法，给每个进程创建它们的页表树。每当客户OS修改了VMM调用的页表，VMM就会用客户域中更新的信息去更新自己的影子页表。这些是实际由硬件使用的页表。显然这个过程非常昂贵：每次对页表树的修改都需要VMM的一次调用。而没有虚拟化时内存映射的改变也不便宜，它们现在变得甚至更昂贵。                                                                                      考虑到从客户OS的变化到VMM以及返回，其本身已经相当昂贵，额外的代价可能真的很大。这就是为什么处理器开始具有避免创建影子页表的额外功能。这样很好不仅是因为速度的问题，而且它减少了VMM消耗的内存。Intel有扩展页表(EPTs)，AMD称之为嵌套页表(NPTs)。基本上两种技术都具有客户OS的页表，来产生虚拟的物理地址。然后通过每个域一个EPT/NPT树的方式，这些地址会被进一步转换为真实的物理地址。这使得可以用几乎非虚拟化情境的速度进行内存处理，因为大多数用来内存处理的VMM条目被移走了。它也减少了VMM使用的内存，因为现在一个域（相对于进程）只有一个页表树需要维护。                                                                                      额外的地址转换步骤的结果也存储于TLB。那意味着TLB不存储虚拟物理地址，而替代以完整的查询结果。已经解释过AMD的帕西菲卡扩展为了避免TLB刷新而给每个条目引入ASID。ASID的位数在最初版本的处理器扩展中是一位；这正好足够区分VMM和客户OS。Intel有服务同一个目的的虚拟处理器ID(VPIDs)，它们只有更多位。但对每个客户域VPID是固定的，因此它不能标记单独的进程，也不能避免TLB在那个级别刷新。</p>
<p>对虚拟OS，每个地址空间的修改需要的工作量是一个问题。但是还有另一个内在的基于VMM虚拟化的问题：没有什么办法处理两层的内存。但内存处理很难（特别是考虑到像NUMA一样的复杂性，见第5部分）。Xen方法使用一个单独的VMM，这使最佳的（或最好的）处理变得困难，因为所有内存管理实现的复杂性，包括像发现内存范围之类“琐碎的”事情，必须被复制于VMM。OS有完全成熟的与最佳的实现；人们确实想避免复制它们。 </p>
<blockquote>
<p><img src="cpu-memory/assets/18143755_Lonh.png" alt="img" /></p>
<p><strong>图 4.5: KVM 虚拟化模型</strong> </p>
</blockquote>
<p>这就是为什么对VMM/Dom0模型的分析是这么有吸引力的一个选择。图4.5显示了KVM的Linux内核扩展如何尝试解决这个问题的。并没有直接运行在硬件之上且管理所有客户的单独的VMM，替代的，一个普通的Linux内核接管了这个功能。这意味着Linux内核中完整且复杂的内存管理功能，被用来管理系统的内存。客户域运行于普通的用户级进程，创建者称其为“客户模式”。虚拟化的功能，参数的或全虚拟化的，被另一个用户级进程KVM VMM控制。这也就是另一个进程用特别的内核实现的KVM设备，去恰巧控制一个客户域。</p>
<p>这个模型相较Xen独立的VMM模型好处在于，即使客户OS使用时，仍然有两个内存处理程序在工作，只需要在Linux内核里有一个实现。不需要像Xen  VMM那样从另一段代码复制同样的功能。这带来更少的工作，更少的bug，或许还有更少的两个内存处理程序接触产生的摩擦，因为一个Linux客户的内存处理程序与运行于裸硬件之上的Linux内核的外部内存处理程序，做出了相同的假设。 </p>
<p>总的来说，程序员必须清醒认识到，采用虚拟化时，内存操作的代价比没有虚拟化要高很多。任何减少这个工作的优化，将在虚拟化环境付出更多。随着时间的过去，处理器的设计者将通过像EPT和NPT技术越来越减少这个差距，但它永远都不会完全消失。 </p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="5-numa-支持"><a class="header" href="#5-numa-支持">5 NUMA 支持</a></h2>
<p>在第二部分中，我们看到在某些机器上，访问物理内存特定区域的成本取决于访问的来源。这种类型的硬件需要操作系统和应用程序特别注意。我们将从 NUMA 硬件的一些细节开始，然后再介绍 Linux 内核为 NUMA 提供的一些支持。</p>
<h3 id="51-numa-hardware"><a class="header" href="#51-numa-hardware">5.1 NUMA Hardware</a></h3>
<p>非均匀内存体系结构越来越普遍。在最简单的 NUMA 形式中，处理器可以具有本地内存（参见图2.3），其访问成本比其他处理器本地内存要便宜。这种类型的 NUMA 系统的成本差异并不高，即 NUMA 因子很低。</p>
<p>NUMA 还特别用于大型机器。我们已经描述了许多处理器访问同一内存所带来的问题。对于商用硬件，所有处理器都将共享同一个北桥（暂不考虑 AMD Opteron NUMA 节点，它们有自己的问题）。这使得北桥成为一个严重的瓶颈，因为所有内存流量都通过它进行路由。当然，大型机器可以使用自定义硬件代替北桥，但是，除非使用的内存芯片具有多个端口，即它们可以从多个总线中使用，否则仍然存在瓶颈。多端口 RAM 很复杂，成本很高，因此几乎不会使用。</p>
<p>复杂度的下一个增加是 AMD 使用的模型，其中一种互连机制（在 AMD 的情况下是 Hypertransport，这是他们从 Digital 获得许可的技术）为未直接连接到 RAM 的处理器提供访问。这种方式可以形成的结构的大小受到限制，除非想任意增加直径（即任意两个节点之间的最大距离）。</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.20.png" alt="img" /></p>
<p><strong>Figure 5.1: Hypercubes</strong></p>
</blockquote>
<p>高效的节点拓扑结构是超立方体，它将节点数限制为2C，其中C是每个节点的互连接口数。对于所有带有2n个CPU的系统，超立方体具有最小的直径。图5.1显示了前三个超立方体。每个超立方体的直径为C，这是绝对最小的。AMD的第一代Opteron处理器每个处理器有三个HyperTransport连接。至少有一个处理器必须连接一个Southbridge，这意味着目前可以直接且高效地实现C=2的超立方体。下一代处理器宣布将有四个连接，到那时C=3的超立方体将成为可能。</p>
<p>但这并不意味着不支持更大的处理器积累。一些公司已经开发了交叉开关，允许使用更大的处理器集合（例如Newisys的Horus）。但是这些交叉开关会增加NUMA因素，并且在一定数量的处理器上不再有效。</p>
<p>下一步是连接CPU组并为它们实现共享内存。所有这样的系统都需要专用硬件，绝不是商品系统。这样的设计存在多个复杂度级别。一个仍然非常接近商品机的系统是IBM x445和类似的机器。它们可以作为普通的4U，8路机器购买，带有x86和x86-64处理器。然后可以将两个（在某些时候最多四个）这样的机器连接起来，以共享内存的方式作为一个单一的机器。所使用的互连引入了一个显著的NUMA因素，这是操作系统以及应用程序必须考虑的。</p>
<p>在另一端的系统中，像SGI的Altix就是专门设计用于互连。SGI的NUMAlink互连结构非常快且延迟低，这些都是高性能计算（HPC）的要求，特别是当使用消息传递接口（MPI）时。缺点当然是这种复杂性和专业化非常昂贵。它们可以实现相对较低的NUMA因素，但由于这些机器可以拥有数千个CPU并且互连的容量有限，NUMA因素实际上是动态的，取决于工作负载而可能达到不可接受的水平。</p>
<p>更常见的解决方案是使用高速网络连接商品机集群。但这些不是NUMA机器；它们不实现共享地址空间，因此不属于本文讨论的任何类别。</p>
<h3 id="52-os-support-for-numa"><a class="header" href="#52-os-support-for-numa">5.2 OS Support for NUMA</a></h3>
<p>To support NUMA machines, the OS has to take the distributed nature of the memory into account.  For instance, if a process is run on a given processor, the physical RAM assigned to the process's address space should come from local memory.  Otherwise each instruction has to access remote memory for code and data.  There are special cases to be taken into account which are only present in NUMA machines.  The text segment of DSOs is normally present exactly once in a machine's physical RAM.  But if the DSO is used by processes and threads on all CPUs (for instance, the basic runtime libraries like <code>libc</code>) this means that all but a few processors have to have remote accesses.  The OS ideally would “mirror” such DSOs into each processor's physical RAM and use local copies.  This is an optimization, not a requirement, and generally hard to implement.  It might not be supported or only in a limited fashion.</p>
<p>To avoid making the situation worse, the OS should not migrate a process or thread from one node to another.  The OS should already try to avoid migrating processes on normal multi-processor machines because migrating from one processor to another means the cache content is lost.  If load distribution requires migrating a process or thread off of a processor, the OS can usually pick an arbitrary new processor which has sufficient capacity left.  In NUMA environments the selection of the new processor is a bit more limited.  The newly selected processor should not have higher access costs to the memory the process is using than the old processor; this restricts the list of targets.  If there is no free processor matching that criteria available, the OS has no choice but to migrate to a processor where memory access is more expensive.</p>
<p>In this situation there are two possible ways forward.  First, one can hope the situation is temporary and the process can be migrated back to a better-suited processor.  Alternatively, the OS can also migrate the process's memory to physical pages which are closer to the newly-used processor. This is quite an expensive operation.  Possibly huge amounts of memory have to be copied, albeit not necessarily in one step.  While this is happening the process, at least briefly, has to be stopped so that modifications to the old pages are correctly migrated.  There are a whole list of other requirements for page migration to be efficient and fast.  In short, the OS should avoid it unless it is really necessary.</p>
<p>Generally, it cannot be assumed that all processes on a NUMA machine use the same amount of memory such that, with the distribution of processes across the processors, memory usage is also equally distributed.  In fact, unless the applications running on the machines are very specific (common in the HPC world, but not outside) the memory use will be very unequal.  Some applications will use vast amounts of memory, others hardly any.  This will, sooner or later, lead to problems if memory is always allocated local to the processor where the request is originated.  The system will eventually run out of memory local to nodes running large processes.</p>
<p>In response to these severe problems, memory is, by default, not allocated exclusively on the local node.  To utilize all the system's memory the default strategy is to stripe the memory.  This guarantees equal use of all the memory of the system.  As a side effect, it becomes possible to freely migrate processes between processors since, on average, the access cost to all the memory used does not change.  For small NUMA factors, striping is acceptable but still not optimal (see data in Section 5.4).</p>
<p>This is a pessimization which helps the system avoid severe problems and  makes it more predictable under normal operation.  But it does decrease overall system performance, in some situations significantly. This is why Linux allows the memory allocation rules to be selected by each process.  A process can select a different strategy for itself and its children.  We will introduce the interfaces which can be used for this in Section 6.</p>
<h3 id="53-published-information"><a class="header" href="#53-published-information">5.3 Published Information</a></h3>
<p>The kernel publishes, through the <code>sys</code> pseudo file system (sysfs), information about the processor caches below</p>
<pre><code>/sys/devices/system/cpu/cpu*/cache
</code></pre>
<p>In Section 6.2.1 we will see interfaces which can be used to query the size of the various caches.  What is important here is the topology of the caches.  The directories above contain subdirectories (named <code>index*</code>) which list information about the various caches the CPU possesses.  The files <code>type</code>, <code>level</code>, and <code>shared_cpu_map</code> are the important files in these directories as far as the topology is concerned.  For an Intel Core 2 QX6700 the information looks as in Table 5.1.</p>
<blockquote>
<div class="table-wrapper"><table><thead><tr><th></th><th>type</th><th>level</th><th>shared_cpu_map</th><th></th></tr></thead><tbody>
<tr><td><code>cpu0</code></td><td>index0</td><td>Data</td><td>1</td><td>00000001</td></tr>
<tr><td></td><td>index1</td><td>Instruction</td><td>1</td><td>00000001</td></tr>
<tr><td></td><td>index2</td><td>Unified</td><td>2</td><td>00000003</td></tr>
<tr><td><code>cpu1</code></td><td>index0</td><td>Data</td><td>1</td><td>00000002</td></tr>
<tr><td></td><td>index1</td><td>Instruction</td><td>1</td><td>00000003</td></tr>
<tr><td></td><td>index2</td><td>Unified</td><td>2</td><td>00000003</td></tr>
<tr><td><code>cpu2</code></td><td>index0</td><td>Data</td><td>1</td><td>00000004</td></tr>
<tr><td></td><td>index1</td><td>Instruction</td><td>1</td><td>00000004</td></tr>
<tr><td></td><td>index2</td><td>Unified</td><td>2</td><td>0000000c</td></tr>
<tr><td><code>cpu3</code></td><td>index0</td><td>Data</td><td>1</td><td>00000008</td></tr>
<tr><td></td><td>index1</td><td>Instruction</td><td>1</td><td>00000008</td></tr>
<tr><td></td><td>index2</td><td>Unified</td><td>2</td><td>0000000c</td></tr>
</tbody></table>
</div>
<p><strong>Table 5.1: <code>sysfs</code> Information for Core 2 CPU Caches</strong></p>
</blockquote>
<p>What this data means is as follows:</p>
<ul>
<li>
<p>Each core {The knowledge that <code>cpu0</code> to <code>cpu3</code>  are cores comes from another place that will be explained shortly.}  has three caches: L1i, L1d, L2.</p>
</li>
<li>
<p>The L1d and L1i caches are not shared with anybody—each core  has its own set of caches.  This is indicated by the bitmap  in shared_cpu_maphaving only one set bit.</p>
</li>
<li>
<p>The L2 cache on <code>cpu0</code> and <code>cpu1</code> is shared, as is the  L2 on <code>cpu2</code> and <code>cpu3</code>.</p>
</li>
</ul>
<p>If the CPU had more cache levels, there would be more <code>index*</code> directories.</p>
<p>For a four-socket, dual-core Opteron machine the cache information looks like Table 5.2:</p>
<blockquote>
<div class="table-wrapper"><table><thead><tr><th></th><th>type</th><th>level</th><th>shared_cpu_map</th><th></th></tr></thead><tbody>
<tr><td><code>cpu0</code></td><td>index0</td><td>Data</td><td>1</td><td>00000001</td></tr>
<tr><td></td><td>index1</td><td>Instruction</td><td>1</td><td>00000001</td></tr>
<tr><td></td><td>index2</td><td>Unified</td><td>2</td><td>00000001</td></tr>
<tr><td><code>cpu1</code></td><td>index0</td><td>Data</td><td>1</td><td>00000002</td></tr>
<tr><td></td><td>index1</td><td>Instruction</td><td>1</td><td>00000002</td></tr>
<tr><td></td><td>index2</td><td>Unified</td><td>2</td><td>00000002</td></tr>
<tr><td><code>cpu2</code></td><td>index0</td><td>Data</td><td>1</td><td>00000004</td></tr>
<tr><td></td><td>index1</td><td>Instruction</td><td>1</td><td>00000004</td></tr>
<tr><td></td><td>index2</td><td>Unified</td><td>2</td><td>00000004</td></tr>
<tr><td><code>cpu3</code></td><td>index0</td><td>Data</td><td>1</td><td>00000008</td></tr>
<tr><td></td><td>index1</td><td>Instruction</td><td>1</td><td>00000008</td></tr>
<tr><td></td><td>index2</td><td>Unified</td><td>2</td><td>00000008</td></tr>
<tr><td><code>cpu4</code></td><td>index0</td><td>Data</td><td>1</td><td>00000010</td></tr>
<tr><td></td><td>index1</td><td>Instruction</td><td>1</td><td>00000010</td></tr>
<tr><td></td><td>index2</td><td>Unified</td><td>2</td><td>00000010</td></tr>
<tr><td><code>cpu5</code></td><td>index0</td><td>Data</td><td>1</td><td>00000020</td></tr>
<tr><td></td><td>index1</td><td>Instruction</td><td>1</td><td>00000020</td></tr>
<tr><td></td><td>index2</td><td>Unified</td><td>2</td><td>00000020</td></tr>
<tr><td><code>cpu6</code></td><td>index0</td><td>Data</td><td>1</td><td>00000040</td></tr>
<tr><td></td><td>index1</td><td>Instruction</td><td>1</td><td>00000040</td></tr>
<tr><td></td><td>index2</td><td>Unified</td><td>2</td><td>00000040</td></tr>
<tr><td><code>cpu7</code></td><td>index0</td><td>Data</td><td>1</td><td>00000080</td></tr>
<tr><td></td><td>index1</td><td>Instruction</td><td>1</td><td>00000080</td></tr>
<tr><td></td><td>index2</td><td>Unified</td><td>2</td><td>00000080</td></tr>
</tbody></table>
</div>
<p><strong>Table 5.2: <code>sysfs</code> Information for Opteron CPU Caches</strong></p>
</blockquote>
<p>As can be seen these processors also have three caches: L1i, L1d, L2.  None of the cores shares any level of cache.  The interesting part for this system is the processor topology.  Without this additional information one cannot make sense of the cache data.  The <code>sys</code> file system exposes this information in the files below</p>
<pre><code>    /sys/devices/system/cpu/cpu*/topology
</code></pre>
<p>Table 5.3 shows the interesting files in this hierarchy for the SMP Opteron machine.</p>
<blockquote>
<div class="table-wrapper"><table><thead><tr><th></th><th><code>physical_package_id</code></th><th><code>core_id</code></th><th><code>core_siblings</code></th><th><code>thread_siblings</code></th></tr></thead><tbody>
<tr><td><code>cpu0</code></td><td>0</td><td>0</td><td>00000003</td><td>00000001</td></tr>
<tr><td><code>cpu1</code></td><td>1</td><td>00000003</td><td>00000002</td><td></td></tr>
<tr><td><code>cpu2</code></td><td>1</td><td>0</td><td>0000000c</td><td>00000004</td></tr>
<tr><td><code>cpu3</code></td><td>1</td><td>0000000c</td><td>00000008</td><td></td></tr>
<tr><td><code>cpu4</code></td><td>2</td><td>0</td><td>00000030</td><td>00000010</td></tr>
<tr><td><code>cpu5</code></td><td>1</td><td>00000030</td><td>00000020</td><td></td></tr>
<tr><td><code>cpu6</code></td><td>3</td><td>0</td><td>000000c0</td><td>00000040</td></tr>
<tr><td><code>cpu7</code></td><td>1</td><td>000000c0</td><td>00000080</td><td></td></tr>
</tbody></table>
</div>
<p><strong>Table 5.3: <code>sysfs</code> Information for Opteron CPU Topology</strong></p>
</blockquote>
<p>Taking Table 5.2 and Table 5.3 together we can see that no CPU has hyper-threads (the <code>thread_siblings</code> bitmaps have one bit set), that the system in fact has four processors (<code>physical_package_id</code> 0 to 3), that each processor has two cores, and that none of the cores share any cache.  This is exactly what corresponds  to earlier Opterons.</p>
<p>What is completely missing in the data provided so far is information about the nature of NUMA on this machine.  Any SMP Opteron machine is a NUMA machine.  For this data we have to look at yet another part of the <code>sys</code> file system which exists on NUMA machines, namely in the hierarchy below</p>
<pre><code>    /sys/devices/system/node
</code></pre>
<p>This directory contains a subdirectory for every NUMA node on the system.  In the node-specific directories there are a number of files.  The important files and their content for the Opteron machine described in the previous two tables are shown in Table 5.4.</p>
<blockquote>
<div class="table-wrapper"><table><thead><tr><th></th><th><code>cpumap</code></th><th><code>distance</code></th></tr></thead><tbody>
<tr><td><code>node0</code></td><td>00000003</td><td>10 20 20 20</td></tr>
<tr><td><code>node1</code></td><td>0000000c</td><td>20 10 20 20</td></tr>
<tr><td><code>node2</code></td><td>00000030</td><td>20 20 10 20</td></tr>
<tr><td><code>node3</code></td><td>000000c0</td><td>20 20 20 10</td></tr>
</tbody></table>
</div>
<p><strong>Table 5.4: <code>sysfs</code> Information for Opteron Nodes</strong></p>
</blockquote>
<p>This information ties all the rest together; now we have a complete picture of the architecture of the machine.  We already know that the machine has four processors.  Each processor constitutes its own node as can be seen by the bits set in the value in <code>cpumap</code> file in the <code>node*</code> directories.  The <code>distance</code> files in those directories contains a set of values, one for each node, which represent a cost of memory accesses at the respective nodes.  In this example all local memory accesses have the cost 10, all remote access to any other node has the cost 20.  {<em>This is, by the way, incorrect.  The ACPI information is apparently wrong since, although the processors used have three coherent HyperTransport links, at least one processor must be connected to a Southbridge.  At least one pair of nodes must therefore have a larger distance.</em>}  This means that, even though the processors are organized as a two-dimensional hypercube (see Figure 5.1), accesses between processors which are not directly connected is not more expensive.  The relative values of the costs should be usable as an estimate of the actual difference of the access times.  The accuracy of all this information is another question.</p>
<h3 id="54-remote-access-costs"><a class="header" href="#54-remote-access-costs">5.4 Remote Access Costs</a></h3>
<p>The distance is relevant, though.  In [amdccnuma] AMD documents the NUMA cost of a four socket machine.  For write operations the numbers are shown in Figure 5.3.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.49.png" alt="img" /></p>
<p><strong>Figure 5.3: Read/Write Performance with Multiple Nodes</strong></p>
</blockquote>
<p>Writes are slower than reads, this is no surprise.  The interesting parts are the costs of the 1- and 2-hop cases.  The two 1-hop cases actually have slightly different costs.  See [amdccnuma] for the details.  The fact we need to remember from this chart is that 2-hop reads and writes are 30% and 49% (respectively) slower than 0-hop reads.  2-hop writes are 32% slower than 0-hop writes, and 17% slower than 1-hop writes. The relative position of processor and memory nodes can make a big difference.  The next generation of processors from AMD will feature four coherent HyperTransport links per processor.  In that case a four socket machine would have diameter of one.  With eight sockets the same problem returns, with a vengeance, since the diameter of a hypercube with eight nodes is three.</p>
<p>All this information is available but it is cumbersome to use.  In Section 6.5 we will see an interface which helps accessing and using this information easier.</p>
<p>The last piece of information the system provides is in the status of a process itself.  It is possible to determine how the memory-mapped files, Copy-On-Write (COW) pages and anonymous memory are distributed over the nodes in the  system. {<em>Copy-On-Write is a method often used in OS implementations when a memory page has one user at first and then has to be copied to allow independent users.  In many situations the copying is unnecessary, at all or at first, in which case it makes sense to only copy when either user modifies the memory.  The operating system intercepts the write operation, duplicates the memory page, and then allows the write instruction to proceed.</em>}  Each process has a file <code>/proc/**PID**/numa_maps</code>, where <code>**PID**</code> is the ID of the process, as shown in Figure 5.2.</p>
<blockquote>
<pre><code>00400000 default file=/bin/cat mapped=3 N3=3
00504000 default file=/bin/cat anon=1 dirty=1 mapped=2 N3=2
00506000 default heap anon=3 dirty=3 active=0 N3=3
38a9000000 default file=/lib64/ld-2.4.so mapped=22 mapmax=47 N1=22
38a9119000 default file=/lib64/ld-2.4.so anon=1 dirty=1 N3=1
38a911a000 default file=/lib64/ld-2.4.so anon=1 dirty=1 N3=1
38a9200000 default file=/lib64/libc-2.4.so mapped=53 mapmax=52 N1=51 N2=2
38a933f000 default file=/lib64/libc-2.4.so
38a943f000 default file=/lib64/libc-2.4.so anon=1 dirty=1 mapped=3 mapmax=32 N1=2 N3=1
38a9443000 default file=/lib64/libc-2.4.so anon=1 dirty=1 N3=1
38a9444000 default anon=4 dirty=4 active=0 N3=4
2b2bbcdce000 default anon=1 dirty=1 N3=1
2b2bbcde4000 default anon=2 dirty=2 N3=2
2b2bbcde6000 default file=/usr/lib/locale/locale-archive mapped=11 mapmax=8 N0=11
7fffedcc7000 default stack anon=2 dirty=2 N3=2
</code></pre>
<p><strong>Figure 5.2: Content of <code>/proc/\*PID\*/numa_maps</code></strong></p>
</blockquote>
<p>The important information in the file is the values for <code>N0</code> to <code>N3</code>, which indicate the number of pages allocated for the memory area on nodes 0 to 3.  It is a good guess that the program was executed on a core on node 3.  The program itself and the dirtied pages are allocated on that node.  Read-only mappings, such as the first mapping for <code>ld-2.4.so</code> and <code>libc-2.4.so</code> as well as the shared file <code>locale-archive</code> are allocated on other nodes.</p>
<p>As we have seen in Figure 5.3 the read performance across nodes falls by 9% and 30% respectively for 1- and 2-hop reads. For execution, such reads are needed and, if the L2 cache is missed, each cache line incurs these additional costs.  All the costs measured for large workloads beyond the size of the cache would have to be increased by 9%/30% if the memory is remote to the processor.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.66.png" alt="img" /></p>
<p><strong>Figure 5.4: Operating on Remote Memory</strong></p>
</blockquote>
<p>To see the effects in the real world we can measure the bandwidth as in Section 3.5.1 but this time with the memory being on a remote node, one hop away.  The result of this test when compared with the data for using local memory can be seen in Figure 5.4.  The numbers have a few big spikes in both directions which are the result of a problem of measuring multi-threaded code and can be ignored.  The important information in this graph is that read operations are always 20% slower.  This is significantly slower than the 9% in Figure 5.3, which is, most likely, not a number for uninterrupted read/write operations and might refer to older processor revisions.  Only AMD knows.</p>
<p>For working set sizes which fit into the caches, the performance of write and copy operations is also 20% slower.  For working sets exceeding the size of the caches, the write performance is not measurably slower than the operation on the local node.  The speed of the interconnect is fast enough to keep up with the memory.  The dominating factor is the time spent waiting on the main memory.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="6-what-programmers-can-do"><a class="header" href="#6-what-programmers-can-do">6 What Programmers Can Do</a></h2>
<p>After the descriptions in the previous sections it is clear that there are many, many opportunities for programmers to influence a program's performance, positively or negatively.  And this is for memory-related operations only.  We will proceed in covering the opportunities from the ground up, starting with the lowest levels of physical RAM access and L1 caches, up to and including OS functionality which influences memory handling.</p>
<h3 id="61-bypassing-the-cache"><a class="header" href="#61-bypassing-the-cache">6.1 Bypassing the Cache</a></h3>
<p>When data is produced and not (immediately) consumed again, the fact that memory store operations read a full cache line first and then modify the cached data is detrimental to performance. This operation pushes data out of the caches which might be needed again in favor of data which will not be used soon.  This is especially true for large data structures, like matrices, which are filled and then used later.  Before the last element of the matrix is filled the sheer size evicts the first elements, making caching of the writes ineffective.</p>
<p>For this and similar situations, processors provide support for <em>non-temporal</em> write operations.  Non-temporal in this context means the data will not be reused soon, so there is no reason to cache it. These non-temporal write operations do not read a cache line and then modify it; instead, the new content is directly written to memory.</p>
<p>This might sound expensive but it does not have to be.  The processor will try to use write-combining (see Section 3.3.3) to fill entire cache lines.  If this succeeds no memory read operation is needed at all. For the x86 and x86-64 architectures a number of intrinsics are provided by gcc:</p>
<blockquote>
<pre><code>#include &lt;emmintrin.h&gt;
void _mm_stream_si32(int *p, int a);
void _mm_stream_si128(int *p, __m128i a);
void _mm_stream_pd(double *p, __m128d a);

#include &lt;xmmintrin.h&gt;
void _mm_stream_pi(__m64 *p, __m64 a);
void _mm_stream_ps(float *p, __m128 a);

#include &lt;ammintrin.h&gt;
void _mm_stream_sd(double *p, __m128d a);
void _mm_stream_ss(float *p, __m128 a);
</code></pre>
</blockquote>
<p>These instructions are used most efficiently if they process large amounts of data in one go.  Data is loaded from memory, processed in one or more steps, and then written back to memory.  The data “streams” through the processor, hence the names of the intrinsics.</p>
<p>The memory address must be aligned to 8 or 16 bytes respectively. In code using the multimedia extensions it is possible to replace the normal <code>_mm_store_*</code> intrinsics with these non-temporal versions. In the matrix multiplication code in Section 9.1 we do not do this since the written values are reused in a short order of time.  This is an example where using the stream instructions is not useful.  More on this code in Section 6.2.1.</p>
<p>The processor's write-combining buffer can hold requests for partial writing to a cache line for only so long.  It is generally necessary to issue all the instructions which modify a single cache line one after another so that the write-combining can actually take place.  An example for how to do this is as follows:</p>
<blockquote>
<pre><code>#include &lt;emmintrin.h&gt;
void setbytes(char *p, int c)
{
  __m128i i = _mm_set_epi8(c, c, c, c,
                           c, c, c, c,
                           c, c, c, c,
                           c, c, c, c);
  _mm_stream_si128((__m128i *)&amp;p[0], i);
  _mm_stream_si128((__m128i *)&amp;p[16], i);
  _mm_stream_si128((__m128i *)&amp;p[32], i);
  _mm_stream_si128((__m128i *)&amp;p[48], i);
}
</code></pre>
</blockquote>
<p>Assuming the pointer <code>p</code> is appropriately aligned, a call to this function will set all bytes of the addressed cache line to <code>c</code>. The write-combining logic will see the four generated <code>movntdq</code> instructions and only issue the write command for the memory once the last instruction has been executed.  To summarize, this code sequence not only avoids reading the cache line before it is written, it also avoids polluting the cache with data which might not be needed  soon.  This can have huge benefits in certain situations.  An example of everyday code using this technique is the <code>memset</code> function in the C runtime, which should use a code sequence like the above for large blocks.</p>
<p>Some architectures provide specialized solutions.  The PowerPC architecture defines the <code>dcbz</code> instruction which can be used to clear an entire cache line.  The instruction does not really bypass the cache since a cache line is allocated for the result, but no data is read from memory.  It is more limited than the non-temporal store instructions since a cache line can only be set to all-zeros and it pollutes the cache (in case the data is non-temporal), but no write-combining logic is needed to achieve the results.</p>
<p>To see the non-temporal instructions in action we will look at a new test which is used to measure writing to a matrix, organized as a two-dimensional array.  The compiler lays out the matrix in memory so that the leftmost (first) index addresses the row which has all elements laid out sequentially in memory.  The right (second) index addresses the elements in a row.  The test program iterates over the matrix in two ways: first by increasing the column number in the inner loop and then by increasing the row index in the inner loop. This means we get the behavior shown in Figure 6.1.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.36.png" alt="img" /></p>
<p><strong>Figure 6.1: Matrix Access Pattern</strong></p>
</blockquote>
<p>We measure the time it takes to initialize a 3000×3000 matrix.  To see how memory behaves, we use store instructions which do not use the cache.  On IA-32 processors the “non-temporal hint” is used for this.  For comparison we also measure ordinary store operations.  The results can be seen in Table 6.1.</p>
<blockquote>
<div class="table-wrapper"><table><thead><tr><th></th><th>Inner Loop Increment</th><th></th></tr></thead><tbody>
<tr><td></td><td>Row</td><td>Column</td></tr>
<tr><td>Normal</td><td>0.048s</td><td>0.127s</td></tr>
<tr><td>Non-Temporal</td><td>0.048s</td><td>0.160s</td></tr>
</tbody></table>
</div>
<p><strong>Table 6.1: Timing Matrix Initialization</strong></p>
</blockquote>
<p>For the normal writes which do use the cache we see the expected result: if memory is used sequentially we get a much better result, 0.048s for the whole operation translating to about 750MB/s, compared to the more-or-less random access which takes 0.127s (about 280MB/s). The matrix is large enough that the caches are essentially ineffective.</p>
<p>The part we are mainly interested in here are the writes bypassing the cache.  It might be surprising that the sequential access is just as fast here as in the case where the cache is used.  The reason for this behavior is that the processor is performing write-combining as explained above. Additionally, the memory ordering rules for non-temporal writes are relaxed: the program needs to explicitly insert memory barriers (<code>sfence</code> instructions for the x86 and x86-64 processors).  This means the processor has more freedom to write back the data and thereby using the available bandwidth as well as possible.</p>
<p>In the case of column-wise access in the inner loop the situation is different.  The results are significantly slower than in the case of cached accesses (0.16s, about 225MB/s).  Here we can see that no write combining is possible and each memory cell must be addressed individually.  This requires constantly selecting new rows in the RAM chips with all the associated delays.  The result is a 25% worse result than the cached run.</p>
<p>On the read side, processors, until recently, lacked support aside from weak hints using non-temporal access (NTA) prefetch instructions.  There is no equivalent to write-combining for reads, which is especially bad for uncacheable memory such as memory-mapped I/O.  Intel, with the SSE4.1 extensions, introduced NTA loads.  They are implemented using a small number of streaming load buffers; each buffer contains a cache line. The first <code>movntdqa</code> instruction for a given cache line will load a cache line into a buffer, possibly replacing another cache line. Subsequent 16-byte aligned accesses to the same cache line will be serviced from the load buffer at little cost.  Unless there are other reasons to do so, the cache line will not be loaded into a cache, thus enabling the loading of large amounts of memory without polluting the caches.  The compiler provides an intrinsic for this instruction:</p>
<blockquote>
<pre><code>#include &lt;smmintrin.h&gt;
__m128i _mm_stream_load_si128 (__m128i *p);
</code></pre>
</blockquote>
<p>This intrinsic should be used multiple times, with addresses of 16-byte blocks passed as the parameter, until each cache line is read.  Only then should the next cache line be started.  Since there are a few streaming read buffers it might be possible to read from two memory locations at once.</p>
<p>What we should take away from this experiment is that modern CPUs very nicely optimize uncached write and (more recently) read accesses as long as they are sequential.  This knowledge can come in very handy when handling large data structures which are used only once. Second, caches can help to cover up some—but not all—of the costs of random memory access.  Random access in this example is 70% slower due to the implementation of RAM access.  Until the implementation changes, random accesses should be avoided whenever possible.</p>
<p>In the section about prefetching we will again take a look at the non-temporal flag.</p>
<h3 id="62-cache-access"><a class="header" href="#62-cache-access">6.2 Cache Access</a></h3>
<p>The most important improvements a programmer can make with respect to caches are those which affect the level 1 cache.  We will discuss it first before including the other levels.  Obviously, all the optimizations for the level 1 cache also affect the other caches.  The theme for all memory access is the same: improve locality (spatial and temporal) and align the code and data.</p>
<p><strong>6.2.1 Optimizing Level 1 Data Cache Access</strong></p>
<p>In section Section 3.3 we have already seen how much the effective use of the L1d cache can improve performance.  In this section we will show what kinds of code changes can help to improve that performance. Continuing from the previous section, we first concentrate on optimizations to access memory sequentially.  As seen in the numbers of Section 3.3, the processor automatically prefetches data when memory is accessed sequentially.</p>
<p>The example code used is a matrix multiplication.  We use two square matrices of 1000×1000 <code>double</code> elements.  For those who have forgotten the math, given two matrices A and B with elements aij and bij with 0 ≤ i,j &lt; N the product is</p>
<blockquote>
<p><img src="cpu-memory/assets/matrixmult.png" alt="[formula]" /></p>
</blockquote>
<p>A straight-forward C implementation of this can look like this:</p>
<blockquote>
<pre><code>  for (i = 0; i &lt; N; ++i)
    for (j = 0; j &lt; N; ++j)
      for (k = 0; k &lt; N; ++k)
        res[i][j] += mul1[i][k] * mul2[k][j];
</code></pre>
</blockquote>
<p>The two input matrices are <code>mul1</code> and <code>mul2</code>.  The result matrix <code>res</code> is assumed to be initialized to all zeroes.  It is a nice and simple implementation.  But it should be obvious that we have exactly the problem explained in Figure 6.1.  While <code>mul1</code> is accessed sequentially, the inner loop advances the row number of <code>mul2</code>.  That means that <code>mul1</code> is handled like the left matrix in Figure 6.1 while <code>mul2</code> is handled like the right matrix.  This cannot be good.</p>
<p>There is one possible remedy one can easily try.  Since each element in the matrices is accessed multiple times it might be worthwhile to rearrange (“transpose,” in mathematical terms) the second matrix <code>mul2</code> before using it.</p>
<blockquote>
<p><img src="cpu-memory/assets/matrixmultT.png" alt="[formula]" /></p>
</blockquote>
<p>After the transposition (traditionally indicated by a superscript ‘T’) we now iterate over both matrices sequentially.  As far as the C code is concerned, it now looks like this:</p>
<blockquote>
<pre><code>  double tmp[N][N];
  for (i = 0; i &lt; N; ++i)
    for (j = 0; j &lt; N; ++j)
      tmp[i][j] = mul2[j][i];
  for (i = 0; i &lt; N; ++i)
    for (j = 0; j &lt; N; ++j)
      for (k = 0; k &lt; N; ++k)
        res[i][j] += mul1[i][k] * tmp[j][k];
</code></pre>
</blockquote>
<p>We create a temporary variable to contain the transposed matrix.  This requires touching more memory, but this cost is, hopefully, recovered since the 1000 non-sequential accesses per column are more expensive (at least on modern hardware).  Time for some performance tests.  The results on a Intel Core 2 with 2666MHz clock speed are (in clock cycles):</p>
<blockquote>
<div class="table-wrapper"><table><thead><tr><th></th><th>Original</th><th>Transposed</th></tr></thead><tbody>
<tr><td>Cycles</td><td>16,765,297,870</td><td>3,922,373,010</td></tr>
<tr><td>Relative</td><td>100%</td><td>23.4%</td></tr>
</tbody></table>
</div></blockquote>
<p>Through the simple transformation of the matrix we can achieve a 76.6% speed-up!  The copy operation is more than made up.  The 1000 non-sequential accesses really hurt.</p>
<p>The next question is whether this is the best we can do.  We certainly need an alternative method anyway which does not require the additional copy.  We will not always have the luxury to be able to perform the copy: the matrix can be too large or the available memory too small.</p>
<p>The search for an alternative implementation should start with a close examination of the math involved and the operations performed by the original implementation.  Trivial math knowledge allows us to see that the order in which the additions for each element of the result matrix are performed is irrelevant as long as each addend appears exactly once. {<em>We ignore arithmetic effects here which might change  the occurrence of overflows, underflows, or rounding.</em>}  This understanding allows us to look for solutions which reorder the additions performed in the inner loop of the original code.</p>
<p>Now let us examine the actual problem in the execution of the original code.  The order in which the elements of <code>mul2</code> are accessed is: (0,0), (1,0), …, (N-1,0), (0,1), (1,1), ….  The elements (0,0) and (0,1) are in the same cache line but, by the time the inner loop completes one round, this cache line has long been evicted.  For this example, each round of the inner loop requires, for each of the three matrices, 1000 cache lines (with 64 bytes for the Core 2 processor).  This adds up to much more than the 32k of L1d available.</p>
<p>But what if we handle two iterations of the middle loop together while executing the inner loop?  In this case we use two <code>double</code> values from the cache line which is guaranteed to be in L1d.  We cut the L1d miss rate in half.  That is certainly an improvement, but, depending on the cache line size, it still might not be as good as we can get it. The Core 2 processor has a L1d cache line size of 64 bytes.  The actual value can be queried using</p>
<blockquote>
<pre><code>sysconf (_SC_LEVEL1_DCACHE_LINESIZE)
</code></pre>
</blockquote>
<p>at runtime or using the <code>getconf</code> utility from the command line so that the program can be compiled for a specific cache line size.  With <code>sizeof(double)</code> being 8 this means that, to fully utilize the cache line, we should unroll the middle loop 8 times.  Continuing this thought, to effectively use the <code>res</code> matrix as well, i.e., to write 8 results at the same time, we should unroll the outer loop 8 times as well.  We assume here cache lines of size 64 but the code works also well on systems with 32-byte cache lines since both cache lines are also 100% utilized.  In general it is best to hardcode cache line sizes at compile time by using the <code>getconf</code> utility as in:</p>
<blockquote>
<pre><code>  gcc -DCLS=$(getconf LEVEL1_DCACHE_LINESIZE) ...
</code></pre>
</blockquote>
<p>If the binaries are supposed to be generic, the largest cache line size should be used.  With very small L1ds this might mean that not all the data fits into the cache but such processors are not suitable for high-performance programs anyway.  The code we arrive at looks something like this:</p>
<blockquote>
<pre><code>  #define SM (CLS / sizeof (double))

  for (i = 0; i &lt; N; i += SM)
      for (j = 0; j &lt; N; j += SM)
          for (k = 0; k &lt; N; k += SM)
              for (i2 = 0, rres = &amp;res[i][j],
                   rmul1 = &amp;mul1[i][k]; i2 &lt; SM;
                   ++i2, rres += N, rmul1 += N)
                  for (k2 = 0, rmul2 = &amp;mul2[k][j];
                       k2 &lt; SM; ++k2, rmul2 += N)
                      for (j2 = 0; j2 &lt; SM; ++j2)
                          rres[j2] += rmul1[k2] * rmul2[j2];
</code></pre>
</blockquote>
<p>This looks quite scary.  To some extent it is but only because it incorporates some tricks.  The most visible change is that we now have six nested loops.  The outer loops iterate with intervals of <code>SM</code> (the cache line size divided by <code>sizeof(double)</code>).  This divides the multiplication in several smaller problems which can be handled with more cache locality.  The inner loops iterate over the missing indexes of the outer loops.  There are, once again, three loops.  The only tricky part here is that the <code>k2</code> and <code>j2</code> loops are in a different order.  This is done since, in the actual computation, only one expression depends on <code>k2</code> but two depend on <code>j2</code>.</p>
<p>The rest of the complication here results from the fact that gcc is not very smart when it comes to optimizing array indexing.  The introduction of the additional variables <code>rres</code>, <code>rmul1</code>, and <code>rmul2</code> optimizes the code by pulling common expressions out of the inner loops, as far down as possible.  The default aliasing rules of the C and C++ languages do not help the compiler making these decisions (unless <code>restrict</code> is used, all pointer accesses are potential sources of aliasing).  This is why Fortran is still a preferred language for numeric programming: it makes writing fast code easier. {<em>In theory the <code>restrict</code> keyword introduced into the C language in the 1999 revision should solve the problem. Compilers have not caught up yet, though.  The reason is mainly that too much incorrect code exists which would mislead the compiler and cause it to generate incorrect object code.</em>}</p>
<p>How all this work pays off can be seen in Table 6.2.</p>
<blockquote>
<div class="table-wrapper"><table><thead><tr><th></th><th>Original</th><th>Transposed</th><th>Sub-Matrix</th><th>Vectorized</th></tr></thead><tbody>
<tr><td>Cycles</td><td>16,765,297,870</td><td>3,922,373,010</td><td>2,895,041,480</td><td>1,588,711,750</td></tr>
<tr><td>Relative</td><td>100%</td><td>23.4%</td><td>17.3%</td><td>9.47%</td></tr>
</tbody></table>
</div>
<p><strong>Table 6.2: Matrix Multiplication Timing</strong></p>
</blockquote>
<p>By avoiding the copying we gain another 6.1% of performance. Plus, we do not need any additional memory.  The input matrices can be arbitrarily large as long as the result matrix fits into memory as well.  This is a requirement for a general solution which we have now achieved.</p>
<p>There is one more column in Table 6.2 which has not been explained.  Most modern processors nowadays include special support for vectorization.  Often branded as multi-media extensions, these special instructions allow processing of 2, 4, 8, or more values at the same time.  These are often SIMD (Single Instruction, Multiple Data) operations, augmented by others to get the data in the right form.  The SSE2 instructions provided by Intel processors can handle two <code>double</code> values in one operation.  The instruction reference manual lists the intrinsic functions which provide access to these SSE2 instructions.  If these intrinsics are used the program runs another 7.3% (relative to the original) faster.  The result is a program which runs in 10% of the time of the original code.  Translated into numbers which people recognize, we went from 318 MFLOPS to 3.35 GFLOPS.  Since we are here only interested in memory effects here, the program code is pushed out into Section 9.1.</p>
<p>It should be noted that, in the last version of the code, we still have some cache problems with <code>mul2</code>; prefetching still will not work.  But this cannot be solved without transposing the matrix. Maybe the cache prefetching units will get smarter to recognize the patterns, then no additional change would be needed.  3.19 GFLOPS on a 2.66 GHz processor with single-threaded code is not bad, though.</p>
<p>What we optimized in the example of the matrix multiplication is the use of the loaded cache lines.  All bytes of a cache line are always used.  We just made sure they are used before the cache line is evacuated.  This is certainly a special case.</p>
<p>It is much more common to have data structures which fill one or more cache lines where the program uses only a few members at any one time. In Figure 3.11 we have already seen the effects of large structure sizes if only few members are used.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.37.png" alt="img" /></p>
<p><strong>Figure 6.2: Spreading Over Multiple Cache Lines</strong></p>
</blockquote>
<p>Figure 6.2 shows the results of yet another set of benchmarks performed using the by now well-known program.  This time two values of the same list element are added.  In one case, both elements are in the same cache line; in the other case, one element is in the first cache line of the list element and the second is in the last cache line. The graph shows the slowdown we are experiencing.</p>
<p>Unsurprisingly, in all cases there are no negative effects if the working set fits into L1d.  Once L1d is no longer sufficient, penalties are paid by using two cache lines in the process instead of one.  The red line shows the data when the list is laid out sequentially in memory.  We see the usual two step patterns: about 17% penalty when the L2 cache is sufficient and about 27% penalty when the main memory has to be used.</p>
<p>In the case of random memory accesses the relative data looks a bit different.  The slowdown for working sets which fit into L2 is between 25% and 35%.  Beyond that it goes down to about 10%.  This is not because the penalties get smaller but, instead, because the actual memory accesses get disproportionally more costly.  The data also shows that, in some cases, the distance between the elements does matter.  The Random 4 CLs curve shows higher penalties because the first and fourth cache lines are used.</p>
<p>An easy way to see the layout of a data structure compared to cache lines is to use the pahole program (see [dwarves]).  This program examines the data structures defined in a binary.  Take a program containing this definition:</p>
<blockquote>
<pre><code>struct foo {
  int a;
  long fill[7];
  int b;
};
</code></pre>
</blockquote>
<p>Compiled on a 64-bit machine, the output of pahole contains (among other things) the information shown in Figure 6.3.</p>
<blockquote>
<pre><code>struct foo {
        int                        a;                    /*     0     4 */

        /* XXX 4 bytes hole, try to pack */

        long int                   fill[7];              /*     8    56 */
        /* --- cacheline 1 boundary (64 bytes) --- */
        int                        b;                    /*    64     4 */
}; /* size: 72, cachelines: 2 */
   /* sum members: 64, holes: 1, sum holes: 4 */
   /* padding: 4 */
   /* last cacheline: 8 bytes */
</code></pre>
<p><strong>Figure 6.3: Output of pahole Run</strong></p>
</blockquote>
<p>This output tells us a lot.  First, it shows that the data structure uses up more than one cache line.  The tool assumes the currently-used processor's cache line size, but this value can be overridden using a command line parameter.  Especially in cases where the size of the structure is barely over the limit of a cache line, and many objects of this type are allocated, it makes sense to seek a way to compress that structure.  Maybe a few elements can have a smaller type, or maybe some fields are actually flags which can be represented using individual bits.</p>
<p>In the case of the example the compression is easy and it is hinted at by the program.  The output shows that there is a hole of four bytes after the first element.  This hole is caused by the alignment requirement of the structure and the <code>fill</code> element.  It is easy to see that the element <code>b</code>, which has a size of four bytes (indicated by the 4 at the end of the line), fits perfectly into the gap.  The result in this case is that the gap no longer exists and that the data structure fits onto one cache line.  The pahole tool can perform this optimization itself.  If the <code>—reorganize</code> parameter is used and the structure name is added at the end of the command line the output of the tool is the optimized structure and the cache line use. Besides moving elements to fill gaps, the tool can also optimize bit fields and combine padding and holes.  For more details see [dwarves].</p>
<p>Having a hole which is just large enough for the trailing element is, of course, the ideal situation.  For this optimization to be useful it is required that the object itself is aligned to a cache line.  We get to that in a bit.</p>
<p>The pahole output also makes it easier to determine whether elements have to be reordered so that those elements which are used together are also stored together.  Using the pahole tool, it is easily possible to determine which elements are in the same cache line and when, instead, the elements have to be reshuffled to achieve that.  This is not an automatic process but the tool can help quite a bit.</p>
<p>The position of the individual structure elements and the way they are used is important, too.  As we have seen in Section 3.5.2 the performance of code with the critical word late in the cache line is worse.  This means a programmer should always follow the following two rules:</p>
<ol start="2">
<li>
<p>Always move the structure element which is most likely to be the critical word to the beginning of the structure.</p>
</li>
<li>
<p>When accessing the data structures, and the order of access is not dictated by the situation, access the elements in the order in which they are defined in the structure.</p>
</li>
</ol>
<p>For small structures, this means that the programmer should arrange the elements in the order in which they are likely accessed.  This must be handled in a flexible way to allow the other optimizations, such as filling holes, to be applied as well.  For bigger data structures each cache line-sized block should be arranged to follow the rules.</p>
<p>Reordering elements is not worth the time it takes, though, if the object itself is not aligned.  The alignment of an object is determined by the alignment requirement of the data type.  Each fundamental type has its own alignment requirement.  For structured types the largest alignment requirement of any of its elements determines the alignment of the structure.  This is almost always smaller than the cache line size.  This means even if the members of a structure are lined up to fit into the same cache line an allocated object might not have an alignment matching the cache line size. There are two ways to ensure that the object has the alignment which was used when designing the layout of the structure:</p>
<ul>
<li>the object can be allocated with an explicit alignment  requirement.  For dynamic allocation a call to </li>
</ul>
<p>malloc</p>
<p>would  only allocate the object with an alignment matching that of the most  demanding standard type (usually </p>
<p>long double</p>
<p>).  It is possible  to use </p>
<p>posix_memalign</p>
<p>, though, to request higher alignments.</p>
<blockquote>
<pre><code>#include &lt;stdlib.h&gt;
int posix_memalign(void **memptr,
                   size_t align,
                   size_t size);
</code></pre>
</blockquote>
<pre><code>The function stores the pointer to the newly-allocated memory in the  pointer variable pointed to by `memptr`.  The memory block is  `size` bytes in size and is aligned on a `align`-byte  boundary.

For objects allocated by the compiler (in `.data`, `.bss`,  etc, and on the stack) a variable attribute can be used:
</code></pre>
<blockquote>
<pre><code>  struct strtype variable
     __attribute((aligned(64)));
</code></pre>
</blockquote>
<pre><code>In this case the `variable` is aligned at a 64 byte boundary  regardless of the alignment requirement of the `strtype`  structure.  This works for global variables as well as automatic  variables.

This method does not work for arrays, though.  Only the first  element of the array would be aligned unless the size of each array  element is a multiple of the alignment value.  It also means that  every single variable must be annotated appropriately.  The use of  `posix_memalign` is also not entirely free since the alignment requirements  usually lead to fragmentation and/or higher memory consumption.
</code></pre>
<ul>
<li>the alignment requirement of a type can be changed using a type  attribute:</li>
</ul>
<blockquote>
<pre><code>  struct strtype {
    ...members...
  } __attribute((aligned(64)));
</code></pre>
</blockquote>
<pre><code>This will cause the compiler to allocate all objects with the  appropriate alignment, including arrays.  The programmer has to take  care of requesting the appropriate alignment for dynamically  allocated objects, though.  Here once again `posix_memalign`  must be used.  It is easy enough to use the `alignof` operator  gcc provides  and pass the value as the second parameter to `posix_memalign`.
</code></pre>
<p>The multimedia extensions previously mentioned in this section almost always require that the memory accesses are aligned.  I.e., for 16 byte memory accesses the address is supposed to be 16 byte aligned. The x86 and x86-64 processors have special variants of the memory operations which can handle unaligned accesses but these are slower. This hard alignment requirement is nothing new for most RISC architectures which require full alignment for all memory accesses. Even if an architecture supports unaligned accesses this is sometimes slower than using appropriate alignment, especially if the misalignment causes a load or store to use two cache lines instead of one.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.48.png" alt="img" /></p>
<p><strong>Figure 6.4: Overhead of Unaligned Accesses</strong></p>
</blockquote>
<p>Figure 6.4 shows the effects of unaligned memory accesses. The now well-known tests which increment a data element while visiting memory (sequentially or randomly) are measured, once with aligned list elements and once with deliberately misaligned elements.  The graph shows the slowdown the program incurs because of the unaligned accesses.  The effects are more dramatic for the sequential access case than for the random case because, in the latter case, the costs of unaligned accesses are partially hidden by the generally higher costs of the memory access.  In the sequential case, for working set sizes which do fit into the L2 cache, the slowdown is about 300%. This can be explained by the reduced effectiveness of the L1 cache. Some increment operations now touch two cache lines, and beginning work on a list element now often requires reading of two cache lines. The connection between L1 and L2 is simply too congested.</p>
<p>For very large working set sizes, the effects of the unaligned access are still 20% to 30%—which is a lot given that the aligned access time for those sizes is long.  This graph should show that alignment must be taken seriously.  Even if the architecture supports unaligned accesses, this must not be taken as “they are as good as aligned accesses”.</p>
<p>There is some fallout from these alignment requirements, though.  If an automatic variable has an alignment requirement, the compiler has to ensure that it is met in all situations.   This is not trivial since the compiler has no control over the call sites and the way they handle the stack.  This problem can be handled in two ways:</p>
<ol start="2">
<li>
<p>The generated code actively aligns the stack, inserting gaps if  necessary.  This requires code to check for alignment, create  alignment, and later undo the alignment.</p>
</li>
<li>
<p>Require that all callers have the stack aligned.</p>
</li>
</ol>
<p>All of the commonly used application binary interfaces (ABIs) follow the second route.  Programs will likely fail if a caller violates the rule and alignment is needed in the callee.  Keeping alignment intact does not come for free, though.</p>
<p>The size of a stack frame used in a function is not necessarily a multiple of the alignment.  This means padding is needed if other functions are called from this stack frame.  The big difference is that the stack frame size is, in most cases, known to the compiler and, therefore, it knows how to adjust the stack pointer to ensure alignment for any function which is called from that stack frame.  In fact, most compilers will simply round the stack frame size up and be done with it.</p>
<p>This simple way to handle alignment is not possible if variable length arrays (VLAs) or <code>alloca</code> are used.  In that case, the total size of the stack frame is only known at runtime.  Active alignment control might be needed in this case, making the generated code (slightly) slower.</p>
<p>On some architectures, only the multimedia extensions require strict alignment; stacks on those architectures are always minimally aligned for the normal data types, usually 4 or 8 byte alignment for 32- and 64-bit architectures respectively.  On these systems, enforcing the alignment incurs unnecessary costs.  That means that, in this case, we might want to get rid of the strict alignment requirement if we know that it is never depended upon.  Tail functions (those which call no other functions) which do no multimedia operations do not need alignment. Neither do functions which only call functions which need no alignment.  If a large enough set of functions can be identified, a program might want to relax the alignment requirement.  For x86 binaries gcc has support for relaxed stack alignment requirements:</p>
<blockquote>
<pre><code>  -mpreferred-stack-boundary=2
</code></pre>
</blockquote>
<p>If this option is given a value of N, the stack alignment requirement will be set to 2N bytes.  So, if a value of 2 is used, the stack alignment requirement is reduced from the default (which is 16 bytes) to just 4 bytes.  In most cases this means no additional alignment operation is needed since normal stack push and pop operations work on four-byte boundaries anyway.  This machine-specific option can help to reduce code size and also improve execution speed.  But it cannot be applied for many other architectures. Even for x86-64 it is generally not applicable since the x86-64 ABI requires that floating-point parameters are passed in an SSE register and the SSE instructions require full 16 byte alignment. Nevertheless, whenever the option is usable it can make a noticeable difference.</p>
<p>Efficient placement of structure elements and alignment are not the only aspects of data structures which influence cache efficiency.  If an array of structures is used, the entire structure definition affects performance.  Remember the results in Figure 3.11: in this case we had increasing amounts of unused data in the elements of the array.  The result was that prefetching was increasingly less effective and the program, for large data sets, became less efficient.</p>
<p>For large working sets it is important to use the available cache as well as possible.  To achieve this, it might be necessary to rearrange data structures.  While it is easier for the programmer to put all the data which conceptually belongs together in the same data structure, this might not be the best approach for maximum performance.  Assume we have a data structure as follows:</p>
<blockquote>
<pre><code>  struct order {
    double price;
    bool paid;
    const char *buyer[5];
    long buyer_id;
  };
</code></pre>
</blockquote>
<p>Further assume that these records are stored in a big array and that a frequently-run job adds up the expected payments of all the outstanding bills.  In this scenario, the memory used for the <code>buyer</code> and <code>buyer_id</code> fields is unnecessarily loaded into the caches.  Judging from the data in Figure 3.11 the program will perform up to 5 times worse than it could.</p>
<p>It is much better to split the <code>order</code> data structure in two, storing the first two fields in one structure and the other fields elsewhere.  This change certainly increases the complexity of the program, but the performance gains might justify this cost.</p>
<p>Finally, let's consider another cache use optimization which, while also applying to the other caches, is primarily felt in the L1d access.  As seen in Figure 3.8 an increased associativity of the cache benefits normal operation.  The larger the cache, the higher the associativity usually is.  The L1d cache is too large to be fully associative but not large enough to have the same associativity as L2 caches.  This can be a problem if many of the objects in the working set fall into the same cache set.  If this leads to evictions due to overuse of a set, the program can experience delays even though much of the cache is unused.  These cache misses are sometimes called <em>conflict misses</em>.  Since the L1d addressing uses virtual addresses, this is actually something the programmer can have control over.  If variables which are used together are also stored together the likelihood of them falling into the same set is minimized.  Figure 6.5 shows how quickly the problem can hit.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.69.png" alt="img" /></p>
<p><strong>Figure 6.5: Cache Associativity Effects</strong></p>
</blockquote>
<p>In the figure, the now familiar Follow {<em>The test was performed on a 32-bit machine, hence <code>NPAD</code>=15 means one 64-byte cache line per list element.</em>} with <code>NPAD</code>=15 test is measured with a special setup. The X–axis is the distance between two list elements, measured in empty list elements.  In other words, a distance of 2 means that the next element's address is 128 bytes after the previous one.  All elements are laid out in the virtual address space with the same distance.  The Y–axis shows the total length of the list.  Only one to 16 elements are used, meaning that the total working set size is 64 to 1024 bytes. The z–axis shows the average number of cycles needed to traverse each list element.</p>
<p>The result shown in the figure should not be surprising.  If few elements are used, all the data fits into L1d and the access time is only 3 cycles per list element.  The same is true for almost all arrangements of the list elements: the virtual addresses are nicely mapped to L1d slots with almost no conflicts.  There are two (in this graph) special distance values for which the situation is different.  If the distance is a multiple of 4096 bytes (i.e., distance of 64 elements) and the length of the list is greater than eight, the average number of cycles per list element increases dramatically.  In these situations all entries are in the same set and, once the list length is greater than the associativity, entries are flushed from L1d and have to be re-read from L2 the next round.  This results in the cost of about 10 cycles per list element.</p>
<p>With this graph we can determine that the processor used has an L1d cache with associativity 8 and a total size of 32kB.  That means that the test could, if necessary, be used to determine these values.  The same effects can be measured for the L2 cache but, here, it is more complex since the L2 cache is indexed using physical addresses and it is much larger.</p>
<p>For programmers this means that associativity is something worth paying attention to.  Laying out data at boundaries that are powers of two happens often enough in the real world, but this is exactly the situation which can easily lead to the above effects and degraded performance.  Unaligned accesses can increase the probability of conflict misses since each access might require an additional cache line.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.58.png" alt="img" /></p>
<p><strong>Figure 6.6: Bank Address of L1d on AMD</strong></p>
</blockquote>
<p>If this optimization is performed, another related optimization is possible, too.  AMD's processors, at least, implement the L1d as several individual banks.  The L1d can receive two data words per cycle but only if both words are stored in different banks or in a bank with the same index.  The bank address is encoded in the low bits of the virtual address as shown in Figure 6.6.  If variables which are used together are also stored together the likelihood that they are in different banks or the same bank with the same index is high.</p>
<p><strong>6.2.2 Optimizing Level 1 Instruction Cache Access</strong></p>
<p>Preparing code for good L1i use needs similar techniques as good L1d use.  The problem is, though, that the programmer usually does not directly influence the way L1i is used unless s/he writes code in assembler.  If compilers are used, programmers can indirectly determine the L1i use by guiding the compiler to create a better code layout.</p>
<p>Code has the advantage that it is linear between jumps.  In these periods the processor can prefetch memory efficiently.  Jumps disturb this nice picture because</p>
<ul>
<li>
<p>the jump target might not be statically determined;</p>
</li>
<li>
<p>and even if it is static the memory fetch might take a long time  if it misses all caches.</p>
</li>
</ul>
<p>These problems create stalls in execution with a possibly severe impact on performance.  This is why today's processors invest heavily in branch prediction (BP).  Highly specialized BP units try to determine the target of a jump as far ahead of the jump as possible so that the processor can initiate loading the instructions at the new location into the cache.  They use static and dynamic rules and are increasingly good at determining patterns in execution.</p>
<p>Getting data into the cache as soon as possible is even more important for the instruction cache.  As mentioned in Section 3.1, instructions have to be decoded before they can be executed and, to speed this up (important on x86 and x86-64), instructions are actually cached in the decoded form, not in the byte/word form read from memory.</p>
<p>To achieve the best L1i use programmers should look out for at least the following aspects of code generation:</p>
<ol>
<li>
<p>reduce the code footprint as much as possible.  This has to be  balanced with optimizations like loop unrolling and inlining.</p>
</li>
<li>
<p>code execution should be linear without  bubbles. {Bubbles describe graphically the holes in the    execution in the pipeline of a processor which appear when the  execution has to wait for resources.  For more details the reader    is referred to literature on processor design.}</p>
</li>
<li>
<p>aligning code when it makes sense.</p>
</li>
</ol>
<p>We will now look at some compiler techniques available to help with optimizing programs according to these aspects.</p>
<p>Compilers have options to enable levels of optimization; specific optimizations can also be individually enabled.  Many of the optimizations enabled at high optimization levels (-O2 and -O3 for gcc) deal with loop optimizations and function inlining.  In general, these are good optimizations.  If the code which is optimized in these ways accounts for a significant part of the total execution time of the program, overall performance can be improved.  Inlining of functions, in particular, allows the compiler to optimize larger chunks of code at a time which, in turn, enables the generation of machine code which better exploits the processor's pipeline architecture.  The handling of both code and data (through dead code elimination or value range propagation, and others) works better when larger parts of the program can be considered as a single unit.</p>
<p>A larger code size means higher pressure on the L1i (and also L2 and higher level) caches.  This <em>can</em> lead to less performance. Smaller code can be faster.  Fortunately gcc has an optimization option to specify this.  If -Os is used the compiler will optimize for code size.  Optimizations which are known to increase the code size are disabled.  Using this option often produces surprising results. Especially if the compiler cannot really take advantage of loop unrolling and inlining, this option is a big win.</p>
<p>Inlining can be controlled individually as well.  The compiler has heuristics and limits which guide inlining; these limits can be controlled by the programmer.  The -finline-limit option specifies how large a function must be to be considered too large for inlining.  If a function is called in multiple places, inlining it in all of them would cause an explosion in the code size.  But there is more.  Assume a function <code>inlcand</code> is called in two functions <code>f1</code> and <code>f2</code>.  The functions <code>f1</code> and <code>f2</code> are themselves called in sequence.</p>
<div class="table-wrapper"><table><thead><tr><th>With inlining</th><th></th><th>Without inlining</th></tr></thead><tbody>
<tr><td><code>start f1  code f1  inlined inlcand  more code f1 end f1 start f2  code f2  inlined inlcand  more code f2 end f2 </code></td><td></td><td><code>start inlcand  code inlcand end inlcand start f1  code f1 end f1 start f2  code f2 end f2 </code></td></tr>
</tbody></table>
</div>
<p><strong>Table 6.3: Inlining Vs Not</strong>Table 6.3 shows how the generated code could look like in the cases of no inline and inlining in both functions.  If the function <code>inlcand</code> is inlined in both <code>f1</code> and <code>f2</code> the total size of the generated code is:</p>
<blockquote>
<p>size <code>f1</code> + size <code>f2</code> + 2 × size <code>inlcand</code></p>
</blockquote>
<p>If no inlining happens, the total size is smaller by <code>size inlcand</code>.  This is how much more L1i and L2 cache is needed if <code>f1</code> and <code>f2</code> are called shortly after one another.  Plus: if <code>inlcand</code> is not inlined, the code might still be in L1i and it will not have to be decoded again.  Plus: the branch prediction unit might do a better job of predicting jumps since it has already seen the code.  If the compiler default for the upper limit on the size of inlined functions is not the best for the program, it should be lowered.</p>
<p>There are cases, though, when inlining always makes sense.  If a function is only called once it might as well be inlined.  This gives the compiler the opportunity to perform more optimizations (like value range propagation, which might significantly improve the code).  That inlining might be thwarted by the selection limits.  gcc has, for cases like this, an option to specify that a function is always inlined. Adding the <code>always_inline</code> function attribute instructs the compiler to do exactly what the name suggests.</p>
<p>In the same context, if a function should never be inlined despite being small enough, the <code>noinline</code> function attribute can be used. Using this attribute makes sense even for small functions if they are called often from different places.  If the L1i content can be reused and the overall footprint is reduced this often makes up for the additional cost of the extra function call.  Branch prediction units are pretty good these days.  If inlining can lead to more aggressive optimizations things look different.  This is something which must be decided on a case-by-case basis.</p>
<p>The <code>always_inline</code> attribute works well if the inline code is always  used.  But what if this is not the case?  What if the inlined function is called only occasionally:</p>
<blockquote>
<pre><code>  void fct(void) {
    ... code block A ...
   if (condition)
     inlfct()
   ... code block C ...
  }
</code></pre>
</blockquote>
<p>The code generated for such a code sequence in general matches the structure of the sources.  That means first comes the code block A, then a conditional jump which, if the condition evaluates to false, jumps forward.  The code generated for the inlined <code>inlfct</code> comes next, and finally the code block C.  This looks all reasonable but it has a problem.</p>
<p>If the <code>condition</code> is frequently false, the execution is not linear.  There is a big chunk of unused code in the middle which not only pollutes the L1i due to prefetching, it also can cause problems with branch prediction.  If the branch prediction is wrong the conditional expression can be very inefficient.</p>
<p>This is a general problem and not specific to inlining functions. Whenever conditional execution is used and it is lopsided (i.e., the expression far more often leads to one result than the other) there is the potential for false static branch prediction and thus bubbles in the pipeline.  This can be prevented by telling the compiler to move the less often executed code out of the main code path.  In that case the conditional branch generated for an <code>if</code> statement would jump to a place out of the order as can be seen in the following figure.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.38.png" alt="img" /></p>
</blockquote>
<p>The upper parts represents the simple code layout.  If the area B, e.g. generated from the inlined function <code>inlfct</code> above, is often not executed because the conditional I jumps over it, the prefetching of the processor will pull in cache lines containing block B which are rarely used.  Using block reordering this can be changed, with a result that can be seen in the lower part of the figure.  The often-executed code is linear in memory while the rarely-executed code is moved somewhere where it does not hurt prefetching and L1i efficiency.</p>
<p>gcc provides two methods to achieve this.  First, the compiler can take profiling output into account while recompiling code and lay out the code blocks according to the profile.  We will see how this works in Section 7.  The second method is through explicit branch prediction.  gcc recognizes <code>__builtin_expect</code>:</p>
<blockquote>
<pre><code>  long __builtin_expect(long EXP, long C);
</code></pre>
</blockquote>
<p>This construct tells the compiler that the expression <code>EXP</code> most likely will have the value <code>C</code>.  The return value is <code>EXP</code>. <code>__builtin_expect</code> is meant to be used in an conditional expression.  In almost all cases will it be used in the context of boolean expressions in which case it is much more convenient to define two helper macros:</p>
<blockquote>
<pre><code>  #define unlikely(expr) __builtin_expect(!!(expr), 0)
  #define likely(expr) __builtin_expect(!!(expr), 1)
</code></pre>
</blockquote>
<p>These macros can then be used as in</p>
<blockquote>
<pre><code>  if (likely(a &gt; 1))
</code></pre>
</blockquote>
<p>If the programmer makes use of these macros and then uses the <code>-freorder-blocks</code> optimization option gcc will reorder blocks as in the figure above.  This option is enabled with <code>-O2</code> but disabled for <code>-Os</code>.  There is another option to reorder block (<code>-freorder-blocks-and-partition</code>) but it has limited usefulness because it does not work with exception handling.</p>
<p>There is another big advantage of small loops, at least on certain processors.  The Intel Core 2 front end has a special feature called Loop Stream Detector (LSD).  If a loop has no more than 18 instructions (none of which is a call to a subroutine), requires only up to 4 decoder fetches of 16 bytes, has at most 4 branch instructions, and is executed more than 64 times, than the loop is sometimes locked in the instruction queue and therefore more quickly available when the loop is used again.  This applies, for instance, to small inner loops which are entered many times through an outer loop. Even without such specialized hardware compact loops have advantages.</p>
<p>Inlining is not the only aspect of optimization with respect to L1i. Another aspect is alignment, just as for data.  There are obvious differences: code is a mostly linear blob which cannot be placed arbitrarily in the address space and it cannot be influenced directly by the programmer as the compiler generates the code.  There are some aspects which the programmer can control, though.</p>
<p>Aligning each single instruction does not make any sense.  The goal is to have the instruction stream be sequential.  So alignment only makes sense in strategic places.  To decide where to add alignments it is necessary to understand what the advantages can be.  Having an instruction at the beginning of a cache line {<em>For some processors cache lines are  not the atomic blocks for instructions.  The Intel Core 2 front end  issues 16 byte blocks to the decoder.  They are appropriately  aligned and so no issued block can span a cache line boundary.  Aligning at the beginning of a cache line still has advantages since  it optimizes the positive effects of prefetching.</em>} means that the prefetch of the cache line is maximized.  For instructions this also means the decoder is more effective.  It is easy to see that, if an instruction at the end of a cache line is executed, the processor has to get ready to read a new cache line and decode the instructions.  There are things which can go wrong (such as cache line misses), meaning that an instruction at the end of the cache line is, on average, not as effectively executed as one at the beginning.</p>
<p>Combine this with the follow-up deduction that the problem is most severe if control was just transferred to the instruction in question (and hence prefetching is not effective) and we arrive at our final conclusion where alignment of code is most useful:</p>
<ul>
<li>
<p>at the beginning of functions;</p>
</li>
<li>
<p>at the beginning of basic blocks which are reached only through  jumps;</p>
</li>
<li>
<p>to some extent, at the beginning of loops.</p>
</li>
</ul>
<p>In the first two cases the alignment comes at little cost.  Execution proceeds at a new location and, if we choose it to be at the beginning of a cache line, we optimize prefetching and decoding. {<em>For instruction decoding processors often use a smaller unit than cache lines, 16 bytes in case of x86 and x86-64.</em>}  The compiler accomplishes this alignment through the insertion of a series of no-op instructions to fill the gap created by aligning the code.  This “dead code” takes a little space but does not normally hurt performance.</p>
<p>The third case is slightly different:  aligning beginning of each loop might create performance problems.  The problem is that beginning of a loop often follows other code sequentially.  If the circumstances are not very lucky there will be a gap between the previous instruction and the aligned beginning of the loop.  Unlike in the previous two cases, this gap cannot be completely dead.  After execution of the previous instruction the first instruction in the loop must be executed.  This means that, following the previous instruction, there either must be a number of no-op instructions to fill the gap or there must be an unconditional jump to the beginning of the loop.  Neither possibility is free. Especially if the loop itself is not executed often, the no-ops or the jump might cost more than one saves by aligning the loop.</p>
<p>There are three ways the programmer can influence the alignment of code.  Obviously, if the code is written in assembler the function and all instructions in it can be explicitly aligned.  The assembler provides for all architectures the <code>.align</code> pseudo-op to do that. For high-level languages the compiler must be told about alignment requirements.  Unlike for data types and variables this is not possible in the source code.  Instead a compiler option is used:</p>
<blockquote>
<pre><code>  -falign-functions=N
</code></pre>
</blockquote>
<p>This option instructs the compiler to align all functions to the next power-of-two boundary greater than <code>N</code>.  That means a gap of up to <code>N</code> bytes is created.  For small functions using a large value for <code>N</code> is a waste.  Equally for code which is executed only rarely.  The latter can  happen a lot in libraries which can contain both popular and not-so-popular interfaces.  A wise choice of the option value can speed things up or save memory by avoiding alignment.  All alignment is turned off by using one as the value of <code>N</code> or by using the <code>-fno-align-functions</code> option.</p>
<p>The alignment for the second case above—beginning of basic blocks which are not reached sequentially—can be controlled with a different option:</p>
<blockquote>
<pre><code>  -falign-jumps=N
</code></pre>
</blockquote>
<p>All the other details are equivalent, the same warning about waste of memory applies.</p>
<p>The third case also has its own option:</p>
<blockquote>
<pre><code>  -falign-loops=N
</code></pre>
</blockquote>
<p>Yet again, the same details and warnings apply.  Except that here, as explained before, alignment comes at a runtime cost since either no-ops or a jump instruction has to be executed if the aligned address is reached sequentially.</p>
<p>gcc knows about one more option for controlling alignment which is mentioned here only for completeness.  <code>-falign-labels</code> aligns every single label in the code (basically the beginning of each basic block).  This, in all but a few exceptional cases, slows down the code and therefore should not be used.</p>
<p><strong>6.2.3 Optimizing Level 2 and Higher Cache Access</strong></p>
<p>Everything said about optimizations for using level 1 cache also applies to level 2 and higher cache accesses.  There are two additional aspects of last level caches:</p>
<ul>
<li>
<p>cache misses are always very expensive.  While L1 misses (hopefully) frequently hit L2 and higher cache, thus limiting the penalties, there is obviously no fallback for the last level cache.</p>
</li>
<li>
<p>L2 caches and higher are often shared by multiple cores and/or hyper-threads.  The effective cache size available to each execution unit is therefore usually less than the total cache size.</p>
</li>
</ul>
<p>To avoid the high costs of cache misses, the working set size should be matched to the cache size.  If data is only needed once this obviously is not necessary since the cache would be ineffective anyway.  We are talking about workloads where the data set is needed more than once.  In such a case the use of a working set which is too large to fit into the cache will create large amounts of cache misses which, even with prefetching being performed successfully, will slow down the program.</p>
<p>A program has to perform its job even if the data set is too large. It is the programmer's job to do the work in a way which minimizes cache misses.  For last-level caches this is possible—just as for L1 caches—by working on the job in smaller pieces.  This is very similar to the optimized matrix multiplication on Table 6.2.  One difference, though, is that, for last level caches, the data blocks which are be worked on can be bigger.  The code becomes yet more complicated if L1 optimizations are needed, too.  Imagine a matrix multiplication where the data sets—the two input matrices and the output matrix—do not fit into the last level cache together.  In this case it might be appropriate to optimize the L1 and last level cache accesses at the same time.</p>
<p>The L1 cache line size is usually constant over many processor generations;  even if it is not, the differences will be small.  It is no big problem to just assume the larger size.  On processors with smaller cache sizes two or more cache lines will then be used instead of one.  In any case, it is reasonable to hardcode the cache line size and optimize the code for it.</p>
<p>For higher level caches this is  not the case if the program is supposed to be generic.  The sizes of those caches can vary widely. Factors of eight or more are not uncommon.  It is not possible to assume the larger cache size as a default since this would mean the code performs poorly on all machines except those with the biggest cache.  The opposite choice is bad too: assuming the smallest cache means throwing away 87% of the cache or more.  This is bad; as we can see from Figure 3.14 using large caches can have a huge impact on the program's speed.</p>
<p>What this means is that the code must dynamically adjust itself to the cache line size.  This is an optimization specific to the program. All we can say here is that the programmer should compute the program's requirements correctly.  Not only are the data sets themselves needed, the higher level caches are also used for other purposes; for example, all the executed instructions are loaded from cache.  If library functions are used this cache usage might add up to a significant amount.  Those library functions might also need data of their own which further reduces the available memory.</p>
<p>Once we have a formula for the memory requirement we can compare it with the cache size.  As mentioned before, the cache might be shared with multiple other cores.  Currently {<em>There definitely will  sometime soon be a better way!</em>} the only way to get correct information without hardcoding knowledge is through the <code>/sys</code> filesystem.  In Table 5.2 we have seen the what the kernel publishes about the hardware.  A program has to find the directory:</p>
<blockquote>
<pre><code>/sys/devices/system/cpu/cpu*/cache
</code></pre>
</blockquote>
<p>for the last level cache.  This can be recognized by the highest numeric value in the <code>level</code> file in that directory.  When the directory is identified the program should read the content of the <code>size</code> file in that directory and divide the numeric value by the number of bits set in the bitmask in the file <code>shared_cpu_map</code>.</p>
<p>The value which is computed this way is a safe lower limit.  Sometimes a program knows a bit more about the behavior of other threads or processes.  If those threads are scheduled on a core or hyper-thread sharing the cache, and the cache usage is known to not exhaust its fraction of the total cache size, then the computed limit might be too low to be optimal.  Whether more than the fair share should be used really depends on the situation.  The programmer has to make a choice or has to allow the user to make a decision.</p>
<p><strong>6.2.4 Optimizing TLB Usage</strong></p>
<p>There are two kinds of optimization of TLB usage.  The first optimization is to reduce the number of pages a program has to use. This automatically results in fewer TLB misses.  The second optimization is to make the TLB lookup cheaper by reducing the number higher level directory tables which must be allocated.  Fewer tables means less memory usage which can result is higher cache hit rates for the directory lookup.</p>
<p>The first optimization is closely related to the minimization of page faults.  We will cover that topic in detail in Section 7.5.  While page faults usually are a one-time cost, TLB misses are a perpetual penalty given that the TLB cache is usually small and it is flushed frequently.  Page faults are orders of magnitude more expensive than TLB misses but, if a program is running long enough and certain parts of the program are executed frequently enough, TLB misses can outweigh even page fault costs.  It is therefore important to regard page optimization not only from the perspective of page faults but also from the TLB miss perspective.  The difference is that, while page fault optimizations only require page-wide grouping of the code and data, TLB optimization requires that, at any point in time, as few TLB entries are in use as possible.</p>
<p>The second TLB optimization is even harder to control.  The number of page directories which have to be used depends on the distribution of the address ranges used in the virtual address space of the process. Widely varying locations in the address space mean more directories.  A complication is that Address Space Layout Randomization (ASLR) leads to exactly these situations.  The load addresses of stack, DSOs, heap, and possibly executable are randomized at runtime to prevent attackers of the machine from guessing the addresses of functions or variables.</p>
<p>For maximum performance ASLR certainly should be turned off.  The costs of the extra directories is low enough, though, to make this step unnecessary in all but a few extreme cases.  One possible optimization the kernel could at any time perform is to ensure that a single mapping does not cross the address space boundary between two directories.  This would limit ASLR in a minimal fashion but not enough to substantially weaken it.</p>
<p>The only way a programmer is directly affected by this is when an address space region is explicitly requested.  This happens when using <code>mmap</code> with <code>MAP_FIXED</code>.  Allocating new a address space region this way is very dangerous and hardly ever done.  It is possible, though, and, if it is used, the programmer should know about the boundaries of the last level page directory and select the requested address appropriately.</p>
<h3 id="63-prefetching"><a class="header" href="#63-prefetching">6.3 Prefetching</a></h3>
<p>The purpose of prefetching is to hide the latency of a memory access. The command pipeline and out-of-order (OOO) execution capabilities of today's processors can hide some latency but, at best, only for accesses which hit the caches.  To cover the latency of main memory accesses, the command queue would have to be incredibly long.  Some processors without OOO try to compensate by increasing the number of cores, but this is a bad trade unless all the code in use is parallelized.</p>
<p>Prefetching can further help to hide latency.  The processor performs prefetching on its own, triggered by certain events (hardware prefetching) or explicitly requested by the program (software prefetching).</p>
<p><strong>6.3.1 Hardware Prefetching</strong></p>
<p>The trigger for hardware prefetching is usually a sequence of two or more cache misses in a certain pattern.  These cache misses can be to succeeding or preceding cache lines.  In old implementations only cache misses to adjacent cache lines are recognized.  With contemporary hardware, strides are recognized as well, meaning that skipping a fixed number of cache lines is recognized as a pattern and handled appropriately.</p>
<p>It would be bad for performance if every single cache miss triggered a hardware prefetch.  Random memory accesses, for instance to global variables, are quite common and the resulting prefetches would mostly waste FSB bandwidth.  This is why, to kickstart prefetching, at least two cache misses are needed.  Processors today all expect there to be more than one stream of memory accesses.  The processor tries to automatically assign each cache miss to such a stream and, if the threshold is reached, start hardware prefetching.  CPUs today can keep track of eight to sixteen separate streams for the higher level caches.</p>
<p>The units responsible for the pattern recognition are associated with the respective cache.  There can be a prefetch unit for the L1d and L1i caches.  There is most probably a prefetch unit for the L2 cache and higher.  The L2 and higher prefetch unit is shared with all the other cores and hyper-threads using the same cache.  The number of eight to sixteen separate streams therefore is quickly reduced.</p>
<p>Prefetching has one big weakness: it cannot cross page boundaries. The reason should be obvious when one realizes that the CPUs support demand paging.  If the prefetcher were allowed to cross page boundaries, the access might trigger an OS event to make the page available.  This by itself can be bad, especially for performance. What is worse is that the prefetcher does not know about the semantics of the program or the OS itself.  It might therefore prefetch pages which, in real life, never would be requested.  That means the prefetcher would run past the end of the memory region the processor accessed in a recognizable pattern before.  This is not only possible, it is very likely.  If the processor, as a side effect of a prefetch, triggered a request for such a page the OS might even be completely thrown off its tracks if such a request could never otherwise happen.</p>
<p>It is therefore important to realize that, regardless of how good the prefetcher is at predicting the pattern, the program will experience cache misses at page boundaries unless it explicitly prefetches or reads from the new page.  This is another reason to optimize the layout of data as described in Section 6.2 to minimize cache pollution by keeping unrelated data out.</p>
<p>Because of this page limitation the processors do not have terribly sophisticated logic to recognize prefetch patterns.  With the still predominant 4k page size there is only so much which makes sense.  The address range in which strides are recognized has been increased over the years, but it probably does not make much sense to go beyond the 512 byte window which is often used today.  Currently prefetch units do not recognize non-linear access patterns.  It is more likely than not that such patterns are truly random or, at least, sufficiently non-repeating that it makes no sense to try recognizing them.</p>
<p>If hardware prefetching is accidentally triggered there is only so much one can do.  One possibility is to try to detect this problem and change the data and/or code layout a bit. This is likely to prove hard.  There might be special localized solutions like using the <code>ud2</code> instruction {<em>Or non-instruction.  It is the recommended  undefined opcode.</em>} on x86 and x86-64 processors.  This instruction, which cannot be executed itself, is used after an indirect jump instruction; it is used as a signal to the instruction fetcher that the processor should not waste efforts decoding the following memory since the execution will continue at a different location.  This is a very special situation, though.  In most cases one has to live with this problem.</p>
<p>It is possible to completely or partially disable hardware prefetching for the entire processor.  On Intel processors an Model Specific Register (MSR) is used for this (IA32_MISC_ENABLE, bit 9 on many processors; bit 19 disables only the adjacent cache line prefetch). This, in most cases, has to happen in the kernel since it is a privileged operation.  If profiling shows that an important application running on a system suffers from bandwidth exhaustion and premature cache evictions due to hardware prefetches, using this MSR is a possibility.</p>
<p><strong>6.3.2 Software Prefetching</strong></p>
<p>The advantage of hardware prefetching is that programs do not have to be adjusted.  The drawbacks, as just described, are that the access patterns must be trivial and that prefetching cannot happen across page boundaries.  For these reasons we now have more possibilities, software prefetching the most important of them.  Software prefetching does require modification of the source code by inserting special instructions.  Some compilers support pragmas to more or less automatically insert prefetch instructions.  On x86 and x86-64 Intel's convention for compiler intrinsics to insert these special instructions is generally used:</p>
<blockquote>
<pre><code>#include &lt;xmmintrin.h&gt;
enum _mm_hint
{
  _MM_HINT_T0 = 3,
  _MM_HINT_T1 = 2,
  _MM_HINT_T2 = 1,
  _MM_HINT_NTA = 0
};
void _mm_prefetch(void *p,
                  enum _mm_hint h);
</code></pre>
</blockquote>
<p>Programs can use the <code>_mm_prefetch</code> intrinsic on any pointer in the program.  Most processors (certainly all x86 and x86-64 processors) ignore errors resulting from invalid pointers which make the life of the programmer significantly easier.  If the passed pointer references valid memory, though, the prefetch unit will be instructed to load the data into cache and, if necessary, evict other data.  Unnecessary prefetches should definitely be avoided since this might reduce the effectiveness of the caches and it consumes memory bandwidth (possibly for two cache lines in case the evicted cache line is dirty).</p>
<p>The different hints to be used with the <code>_mm_prefetch</code> intrinsic are implementation defined.  That means each processor version can implement them (slightly) differently.  What can generally be said is that <code>_MM_HINT_T0</code> fetches data to all levels of the cache for inclusive caches and to the lowest level cache for exclusive caches. If the data item is in a higher level cache it is loaded into L1d. The <code>_MM_HINT_T1</code> hint pulls the data into L2 and not into L1d.  If there is an L3 cache the <code>_MM_HINT_T2</code> hints can do something similar for it.  These are details, though, which are weakly specified and need to be verified for the actual processor in use.  In general, if the data is to be used right away using <code>_MM_HINT_T0</code> is the right thing to do.  Of course this requires that the L1d cache size is large enough to hold all the prefetched data.  If the size of the immediately used working set is too large, prefetching everything into L1d is a bad idea and the other two hints should be used.</p>
<p>The fourth hint, <code>_MM_HINT_NTA</code>, is special in that it allows  telling the processor to treat the prefetched cache line specially.  NTA stands for non-temporal aligned which we already explained in Section 6.1.  The program tells the processor that polluting caches with this data should be avoided as much as possible since the data is only used for a short time.  The processor can therefore, upon loading, avoid reading the data into the lower level caches for inclusive cache implementations.  When the data is evicted from L1d the data need not be pushed into L2 or higher but, instead, can be written directly to memory.  There might be other tricks the processor designers can deploy if this hint is given.  The programmer must be careful using this hint: if the immediate working set size is too large and forces eviction of a cache line loaded with the NTA hint, reloading from memory will occur.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.56.png" alt="img" /></p>
<p><strong>Figure 6.7: Average with Prefetch, NPAD=31</strong></p>
</blockquote>
<p>Figure 6.7 shows the results of a test using the now familiar pointer chasing framework.  The list is randomized.  The difference to previous test is that the program actually spends some time at each list node (about 160 cycles).  As we learned from the data in Figure 3.15, the program's performance suffers badly as soon as the working set size is larger than the last-level cache.</p>
<p>We can now try to improve the situation by issuing prefetch requests ahead of the computation.  I.e., in each round of the loop we prefetch a new element.  The distance between the prefetched node in the list and the node which is currently worked on must be carefully chosen. Given that each node is processed in 160 cycles and that we have to prefetch two cache lines (<code>NPAD</code>=31), a distance of five list elements is enough.</p>
<p>The results in Figure 6.7 show that the prefetch does indeed help.  As long as the working set size does not exceed the size of the last level cache (the machine has 512kB = 219B of L2) the numbers are identical.  The prefetch instructions do not add a measurable extra burden.  As soon as the L2 size is exceeded the prefetching saves between 50 to 60 cycles, up to 8%.  The use of prefetch cannot hide all the penalties but it does help a bit.</p>
<p>AMD implements, in their family 10h of the Opteron line, another instruction: <code>prefetchw</code>.  This instruction has so far no equivalent on the Intel side and is not available through intrinsics. The <code>prefetchw</code> instruction prefetches the cache line into L1 just like the other prefetch instructions.  The difference is that the cache line is immediately put into 'M' state.  This will be a disadvantage if no write to the cache line follows later.  If there are one or more writes, they will be accelerated since the writes do not have to change the cache state—that already happened when the cache line was prefetched.</p>
<p>Prefetching can have bigger advantages than the meager 8% we achieved here.  But it is notoriously hard to do right, especially if the same binary is supposed to perform well on a variety of machines.  The performance counters provided by the CPU can help the programmer analyze prefetches.  Events which can be counted and sampled include hardware prefetches, software prefetches, useful software prefetches, cache misses at the various levels, and more.  In Section 7.1 we will introduce a number of these events.  All these counters are machine specific.</p>
<p>When analyzing programs one should first look at the cache misses.  When a large source of cache misses is located one should try to add a prefetch instruction for the problematic memory accesses.  This should be done in one place at a time.  The result of each modification should be checked by observing the performance counters measuring useful prefetch instructions.  If those counters do not increase the prefetch might be wrong, it is not given enough time to load from memory, or the prefetch evicts memory from the cache which is still needed.</p>
<p>gcc today is able to emit prefetch  instructions automatically in one situation.  If a loop is iterating over an array the following option can be used:</p>
<blockquote>
<pre><code>-fprefetch-loop-arrays
</code></pre>
</blockquote>
<p>The compiler will figure out whether prefetching makes sense and, if so, how far ahead it should look.  For small arrays this can be a disadvantage and, if the size of the array is not known at compile time, the results might be worse.  The gcc manual warns that the benefits highly depend on the form of the code and that in some situation the code might actually run slower.  Programmers have to use this option carefully.</p>
<p><strong>6.3.3 Special Kind of Prefetch: Speculation</strong></p>
<p>The OOO execution capability of a processor allows moving instructions around if they do not conflict with each other.  For instance (using this time IA-64 for the example):</p>
<blockquote>
<pre><code>  st8        [r4] = 12
  add        r5 = r6, r7;;
  st8        [r18] = r5
</code></pre>
</blockquote>
<p>This code sequence stores 12 at the address specified by register <code>r4</code>, adds the content of registers <code>r6</code> and <code>r7</code> and stores it in register <code>r5</code>.  Finally it stores the sum at the address specified by register <code>r18</code>.  The point here is that the add instruction can be executed before—or at the same time as—the first <code>st8</code> instruction since there is no data dependency.  But what happens if one of the addends has to be loaded?</p>
<blockquote>
<pre><code>  st8        [r4] = 12
  ld8        r6 = [r8];;
  add        r5 = r6, r7;;
  st8        [r18] = r5
</code></pre>
</blockquote>
<p>The extra <code>ld8</code> instruction loads the value from the address specified by the register <code>r8</code>.  There is an obvious data dependency between this load instruction and the following <code>add</code> instruction (this is the reason for the <code>;;</code> after the instruction, thanks for asking).  What is critical here is that the new <code>ld8</code> instruction—unlike the <code>add</code> instruction—cannot be moved in front of the first <code>st8</code>.  The processor cannot determine quickly enough during the instruction decoding whether the store and load conflict, i.e., whether <code>r4</code> and <code>r8</code> might have same value.  If they do have the same value, the <code>st8</code> instruction would determine the value loaded into <code>r6</code>.  What is worse, the <code>ld8</code> might also bring with it a large latency in case the load misses the caches. The IA-64 architecture supports speculative loads for this case:</p>
<blockquote>
<pre><code>  ld8.a      r6 = [r8];;
  [... other instructions ...]
  st8        [r4] = 12
  ld8.c.clr  r6 = [r8];;
  add        r5 = r6, r7;;
  st8        [r18] = r5
</code></pre>
</blockquote>
<p>The new <code>ld8.a</code> and <code>ld8.c.clr</code> instructions belong together and replace the <code>ld8</code> instruction in the previous code sequence. The <code>ld8.a</code> instruction is the speculative load.  The value cannot be used directly but the processor can start the work.  At the time when the <code>ld8.c.clr</code> instruction is reached the content might have been loaded already (given there is a sufficient number of instructions in the gap).  The arguments for this instruction must match that for the <code>ld8.a</code> instruction.  If the preceding <code>st8</code> instruction does not overwrite the value (i.e., <code>r4</code> and <code>r8</code> are the same), nothing has to be done.  The speculative load does its job and the latency of the load is hidden.  If the store and load do conflict the <code>ld8.c.clr</code> reloads the value from memory and we end up with the semantics of a normal <code>ld8</code> instruction.</p>
<p>Speculative loads are not (yet?) widely used.  But as the example shows it is a very simple yet effective way to hide latencies.  Prefetching is basically equivalent and, for processors with fewer registers, speculative loads probably do not make much sense.  Speculative loads have the (sometimes big) advantage of loading the value directly into the register and not into the cache line where it might be evicted again (for instance, when the thread is descheduled). If speculation is available it should be used.</p>
<p><strong>6.3.4 Helper Threads</strong></p>
<p>When one tries to use software prefetching one often runs into problems with the complexity of the code.  If the code has to iterate over a data structure (a list in our case) one has to implement two independent iterations in the same loop: the normal iteration doing the work and the second iteration, which looks ahead, to use prefetching.  This easily gets complex enough that mistakes are likely.</p>
<p>Furthermore, it is necessary to determine how far to look ahead.  Too little and the memory will not be loaded in time.  Too far and the just loaded data might have been evicted again.  Another problem is that prefetch instructions, although they do not block and wait for the memory to be loaded, take time.  The instruction has to be decoded, which might be noticeable if the decoder is too busy, for instance, due to well written/generated code.  Finally, the code size of the loop is increased.  This decreases the L1i efficiency.  If one tries to avoid parts of this cost by issuing multiple prefetch requests in a row (in case the second load does not depend on the result of the first) one runs into problems with the number of outstanding prefetch requests.</p>
<p>An alternative approach is to perform the normal operation and the prefetch completely separately.  This can happen using two normal threads.  The threads must obviously be scheduled so that the prefetch thread is populating a cache accessed by both threads.  There are two special solutions worth mentioning:</p>
<ul>
<li>
<p>Use hyper-threads (see Figure 3.22) on the  same core.  In this case the prefetch can go into L2 (or even L1d).</p>
</li>
<li>
<p>Use “dumber” threads than SMT threads which can do  nothing but prefetch and other simple operations.  This is an option  processor manufacturers might explore.</p>
</li>
</ul>
<p>The use of hyper-threads is particularly intriguing.  As we have seen on Figure 3.22, the sharing of caches is a problem if the hyper-threads execute independent code.  If, instead, one thread is used as a prefetch helper thread this is not a problem.  To the contrary, it is the desired effect since the lowest level cache is preloaded. Furthermore, since the prefetch thread is mostly idle or waiting for memory, the normal operation of the other hyper-thread is not disturbed much if it does not have to access main memory itself.  The latter is exactly what the prefetch helper thread prevents.</p>
<p>The only tricky part is to ensure that the helper thread is not running too far ahead.  It must not completely pollute the cache so that the oldest prefetched values are evicted again.  On Linux, synchronization is easily done using the <code>futex</code> system call [futexes] or, at a little bit higher cost, using the POSIX thread synchronization primitives.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.53.png" alt="img" /></p>
<p><strong>Figure 6.8: Average with Helper Thread, NPAD=31</strong></p>
</blockquote>
<p>The benefits of the approach can be seen in Figure 6.8. This is the same test as in Figure 6.7 only with the additional result added.  The new test creates an additional helper thread which runs about 100 list entries ahead and reads (not only prefetches) all the cache lines of each list element.  In this case we have two cache lines per list element (<code>NPAD</code>=31 on a 32-bit machine with 64 byte cache line size).</p>
<p>The two threads are scheduled on two hyper-threads of the same core. The test machine has only one core but the results should be about the same if there is more than one core.  The affinity functions, which we will introduce in Section 6.4.3, are used to tie the threads down to the appropriate hyper-thread.</p>
<p>To determine which two (or more) processors the OS knows are hyper-threads, the <code>NUMA_cpu_level_mask</code> interface from libNUMA can be used (see Section 12).</p>
<blockquote>
<pre><code>#include &lt;libNUMA.h&gt;
ssize_t NUMA_cpu_level_mask(size_t destsize,
                            cpu_set_t *dest,
                            size_t srcsize,
                            const cpu_set_t*src,
                            unsigned int level);
</code></pre>
</blockquote>
<p>This interface can be used to determine the hierarchy of CPUs as they are connected through caches and memory.  Of interest here is level 1 which corresponds to hyper-threads.  To schedule two threads on two hyper-threads the libNUMA functions can be used (error handling dropped for brevity):</p>
<blockquote>
<pre><code>  cpu_set_t self;
  NUMA_cpu_self_current_mask(sizeof(self),
                             &amp;self);
  cpu_set_t hts;
  NUMA_cpu_level_mask(sizeof(hts), &amp;hts,
                      sizeof(self), &amp;self, 1);
  CPU_XOR(&amp;hts, &amp;hts, &amp;self);
</code></pre>
</blockquote>
<p>After this code is executed we have two CPU bit sets.  <code>self</code> can be used to set the affinity of the current thread and the mask in <code>hts</code> can be used to set the affinity of the helper thread.  This should ideally happen before the thread is created.  In Section 6.4.3 we will introduce the interface to set the affinity.  If there is no hyper-thread available the <code>NUMA_cpu_level_mask</code> function will return 1.  This can be used as a sign to avoid this optimization.</p>
<p>The result of this benchmark might be surprising (or maybe not).  If the working set fits into L2, the overhead of the helper thread reduces the performance by between 10% and 60% (ignore the smallest working set sizes again, the noise is too high).  This should be expected since, if all the data is already in the L2 cache,  the prefetch helper thread only uses system resources without contributing to the execution.</p>
<p>Once the L2 size is exhausted the picture changes, though.  The prefetch helper thread helps to reduce the runtime by about 25%.  We still see a rising curve simply because the prefetches cannot be processed fast enough.  The arithmetic operations performed by the main thread and the memory load operations of the helper thread do complement each other, though.  The resource collisions are minimal which causes this synergistic effect.</p>
<p>The results of this test should be transferable to many other situations.  Hyper-threads, often not useful due to cache pollution, shine in these situations and should be taken advantage of.  The <code>sys</code> file system allows a program to find the thread siblings (see the <code>thread_siblings</code> column in Table 5.3).  Once this information is available the program just has to define the affinity of the threads and then run the loop in two modes: normal operation and prefetching.  The amount of memory prefetched should depend on the size of the shared cache. In this example the L2 size is relevant and the program can query the size using</p>
<blockquote>
<pre><code>    sysconf(_SC_LEVEL2_CACHE_SIZE)
</code></pre>
</blockquote>
<p>Whether or not the progress of the helper thread must be restricted depends on the program.  In general it is best to make sure there is some synchronization since scheduling details could otherwise cause significant performance degradations.</p>
<p><strong>6.3.5 Direct Cache Access</strong></p>
<p>One sources of cache misses in a modern OS is the handling of incoming data traffic.  Modern hardware, like Network Interface Cards (NICs) and disk controllers, has the ability to write the received or read data directly into memory without involving the CPU.  This is crucial for the performance of the devices we have today, but it also causes problems.  Assume an incoming packet from a network: the OS has to decide how to handle it by looking at the header of the packet.  The NIC places the packet into memory and then notifies the processor about the arrival.  The processor has no chance to prefetch the data since it does not know when the data will arrive, and maybe not even where exactly it will be stored.  The result is a cache miss when reading the header.</p>
<p>Intel has added technology in their chipsets and CPUs to alleviate this problem [directcacheaccess].  The idea is to populate the cache of the CPU which will be notified about the incoming packet with the packet's data.  The payload of the packet is not critical here, this data will, in general, be handled by higher-level functions, either in the kernel or at user level.  The packet header is used to make decisions about the way the packet has to be handled and so this data is needed immediately.</p>
<p>The network I/O hardware already has DMA to write the packet.  That means it communicates directly with the memory controller which potentially is integrated in the Northbridge.  Another side of the memory controller is the interface to the processors through the FSB (assuming the memory controller is not integrated into the CPU itself).</p>
<p>The idea behind Direct Cache Access (DCA) is to extend the protocol between the NIC and the memory controller.  In Figure 6.9 the first figure shows the beginning of the DMA transfer in a regular machine with North- and Southbridge.</p>
<blockquote>
<div class="table-wrapper"><table><thead><tr><th><img src="cpu-memory/assets/cpumemory.54.png" alt="img" /></th><th><img src="cpu-memory/assets/cpumemory.55.png" alt="img" /></th></tr></thead><tbody>
<tr><td>DMA Initiated</td><td>DMA and DCA Executed</td></tr>
</tbody></table>
</div>
<p><strong>Figure 6.9: Direct Cache Access</strong></p>
</blockquote>
<p>The NIC is connected to (or is part of) the Southbridge.  It initiates the DMA access but provides the new information about the packet header which should be pushed into the processor's cache.</p>
<p>The traditional behavior would be, in step two, to simply complete the DMA transfer with the connection to the memory.  For the DMA transfers with the DCA flag set the Northbridge additionally sends the data on the FSB with a special, new DCA flag.  The processor always snoops the FSB and, if it recognizes the DCA flag, it tries to load the data directed to the processor into the lowest cache.  The DCA flag is, in fact, a hint; the processor can freely ignore it. After the DMA transfer is finished the processor is signaled.</p>
<p>The OS, when processing the packet, first has to determine what kind of packet it is.  If the DCA hint is not ignored, the loads the OS has to perform to identify the packet most likely hit the cache.  Multiply this saving of hundreds of cycles per packet with tens of thousands of packets which can be processed per second, and the savings add up to very significant numbers, especially when it comes to latency.</p>
<p>Without the integration of I/O hardware (NIC in this case), chipset, and CPUs such an optimization is not possible.  It is therefore necessary to make sure to select the platform wisely if this technology is needed.</p>
<div style="break-before: page; page-break-before: always;"></div><h3 id="64-multi-thread-optimizations"><a class="header" href="#64-multi-thread-optimizations">6.4 Multi-Thread Optimizations</a></h3>
<p>When it comes to multi-threading, there are three different aspects of cache use which are important:</p>
<ul>
<li>Concurrency</li>
<li>Atomicity</li>
<li>Bandwidth</li>
</ul>
<p>These aspects also apply to multi-process situations but, because multiple processes are (mostly) independent, it is not so easy to optimize for them.  The possible multi-process optimizations are a subset of those available for the multi-thread scenario.  So we will deal exclusively with the latter here.</p>
<p>Concurrency in this context refers to the memory effects a process experiences when running more than one thread at a time.  A property of threads is that they all share the same address space and, therefore, can all access the same memory. In the ideal case, the memory regions used by the threads are distinct, in which case those threads are coupled only lightly (common input and/or output, for instance). If more than one thread uses the same data, coordination is needed; this is when atomicity comes into play.  Finally, depending on the machine architecture, the available memory and inter-processor bus bandwidth available to the processors is limited.  We will handle these three aspects separately in the following sections—although they are, of course, closely linked.</p>
<p><strong>6.4.1 Concurrency Optimizations</strong></p>
<p>Initially, in this section, we will discuss two separate issues which actually require contradictory optimizations.  A multi-threaded application uses common data in some of its threads.  Normal cache optimization calls for keeping data together so that the footprint of the application is small, thus maximizing the amount of memory which fits into the caches at any one time.</p>
<p>There is a problem with this approach, though: if multiple threads write to a memory location, the cache line must be in ‘E’ (exclusive) state in the L1d of each respective core.  This means that a lot of RFO requests are sent, in the worst case one for each write access.  So a normal write will be suddenly very expensive.  If the same memory location is used, synchronization is needed (maybe through the use of atomic operations, which is handled in the next section).  The problem is also visible, though, when all the threads are using different memory locations and are supposedly independent.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.40.png" alt="img" /></p>
<p><strong>Figure 6.10: Concurrent Cache Line Access Overhead</strong></p>
</blockquote>
<p>Figure 6.10 shows the results of this “false sharing”. The test program (shown in Section 9.3) creates a number of threads which do nothing but increment a memory location (500 million times).  The measured time is from the program start until the program finishes after waiting for the last thread.  The threads are pinned to individual processors.  The machine has four P4 processors.  The blue values represent runs where the memory allocations assigned to each thread are on separate cache lines.  The red part is the penalty occurred when the locations for the threads are moved to just one cache line.</p>
<p>The blue measurements (when using individual cache lines) match what one would expect.  The program scales without penalty to many threads. Each processor keeps its cache line in its own L1d and there are no bandwidth issues since not much code or data has to be read (in fact, it is all cached).  The measured slight increase is really system noise and probably some prefetching effects (the threads use sequential cache lines).</p>
<p>The measured overhead, computed by dividing the time needed when using one cache line versus a separate cache line for each thread, is 390%, 734%, and 1,147% respectively.  These large numbers might be surprising at first sight but, when thinking about the cache interaction needed, it should be obvious.  The cache line is pulled from one processor's cache just after it has finished writing to the cache line.  All processors, except the one which has the cache line at any given moment, are delayed and cannot do anything.  Each additional processor will just cause more delays.</p>
<p>It is clear from these measurements that this scenario must be avoided in programs.  Given the huge penalty, this problem is, in many situations, obvious (profiling will show the code location, at least) but there is a pitfall with modern hardware.  Figure 6.11 shows the equivalent measurements when running the code on a single processor, quad core machine (Intel Core 2 QX 6700).  Even with this processor's two separate L2s the test case does not show any scalability issues. There is a slight overhead when using the same cache line more than once but it does not increase with the number of cores. {<em>I cannot explain the lower number when all four cores are used but it is reproducible.</em>} If more than one of these processors were used we would, of course, see results similar to those in Figure 6.10. Despite the increasing use of multi-core processors, many machines will continue to use multiple processors and, therefore, it is important to handle this scenario correctly, which might mean testing the code on real SMP machines.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.41.png" alt="img" /></p>
<p><strong>Figure 6.11: Overhead, Quad Core</strong></p>
</blockquote>
<p>There is a very simple “fix” for the problem: put every variable on its own cache line. This is where the conflict with the previously mentioned optimization comes into play, specifically, the footprint of the application would increase a lot.  This is not acceptable; it is therefore necessary to come up with a more intelligent solution.</p>
<p>What is needed is to identify which variables are used by only one thread at a time, those used by only one thread ever, and maybe those which are contested at times. Different solutions for each of these scenarios are possible and useful. The most basic criterion for the differentiation of variables is: are they ever written to and how often does this happen.</p>
<p>Variables which are never written to and those which are only initialized once are basically constants. Since RFO requests are only needed for write operations, constants can be shared in the cache (‘S’ state). So, these variables do not have to be treated specially; grouping them together is fine.  If the programmer marks the variables correctly with <code>const</code>, the tool chain will move the variables away from the normal variables into the <code>.rodata</code> (read-only data) or <code>.data.rel.ro</code> (read-only after relocation) section {<em>Sections, identified by their names are the atomic units containing code and data in an ELF file.</em>}  No other special action is required.  If, for some reason, variables cannot be marked correctly with <code>const</code>, the programmer can influence their placement by assigning them to a special section.</p>
<p>When the linker constructs the final binary, it first appends the sections with the same name from all input files; those sections are then arranged in an order determined by the linker script. This means that, by moving all variables which are basically constant but are not marked as such into a special section, the programmer can group all of those variables together. There will not be a variable which is often written to between them.  By aligning the first variable in that section appropriately, it is possible to guarantee that no false sharing happens.  Assume this little example:</p>
<pre><code>  int foo = 1;
  int bar __attribute__((section(&quot;.data.ro&quot;))) = 2;
  int baz = 3;
  int xyzzy __attribute__((section(&quot;.data.ro&quot;))) = 4;
</code></pre>
<p>If compiled, this input file defines four variables.  The interesting part is that the variables <code>foo</code> and <code>baz</code>, and <code>bar</code> and <code>xyzzy</code> are grouped together respectively.  Without the attribute definitions the compiler would allocate all four variables in the sequence in which they are defined in the source code the a section named <code>.data</code>. {<em>This  is not guaranteed by the ISO C standard but it is how gcc works.</em>} With the code as-is the variables <code>bar</code> and <code>xyzzy</code> are placed in a section named <code>.data.ro</code>. The section name <code>.data.ro</code> is more or less arbitrary.  A prefix of <code>.data.</code> guarantees that the GNU linker will place the section together with the other data sections.</p>
<p>The same technique can be applied to separate out variables which are mostly read but occasionally written.  Simply choose a different section name.  This separation seems to make sense in some cases like the Linux kernel.</p>
<p>If a variable is only ever used by one thread, there is another way to specify the variable.  In this case it is possible and useful to use thread-local variables (see [mytls]). The C and C++ language in gcc allow variables to be defined as per-thread using the <code>__thread</code> keyword.</p>
<pre><code>  int foo = 1;
  __thread int bar = 2;
  int baz = 3;
  __thread int xyzzy = 4;
</code></pre>
<p>The variables <code>bar</code> and <code>xyzzy</code> are not allocated in the normal data segment; instead each thread has its own separate area where such variables are stored.  The variables can have static initializers.  All thread-local variables are addressable by all other threads but, unless a thread passes a pointer to a thread-local variable to those other threads, there is no way the other threads can find that variable. Due to the variable being thread-local, false sharing is not a problem—unless the program artificially creates a problem.  This solution is easy to set up (the compiler and linker do all the work), but it has its cost. When a thread is created, it has to spend some time on setting up the thread-local variables, which requires time and memory. In addition, addressing thread-local variables is usually more expensive than using global or automatic variables (see [mytls] for explanations of how the costs are minimized automatically, if possible).</p>
<p>One drawback of using thread-local storage (TLS) is that, if the use of the variable shifts over to another thread, the current value of the variable in the old thread is not available to new thread. Each thread's copy of the variable is distinct.  Often this is not a problem at all and, if it is, the shift over to the new thread needs coordination, at which time the current value can be copied.</p>
<p>A second, bigger problem is possible waste of resources. If only one thread ever uses the variable at any one time, all threads have to pay a price in terms of memory. If a thread does not use any TLS variables, the lazy allocation of the TLS memory area prevents this from being a problem (except for TLS in the application itself).  If a thread uses just one TLS variable in a DSO, the memory for all the other TLS variables in this object will be allocated, too. This could potentially add up if TLS variables are used on a large scale.</p>
<p>In general the best advice which can be given is</p>
<ol>
<li>
<p>Separate at least read-only (after initialization) and  read-write variables.  Maybe extend this separation to read-mostly variables as a  third category.</p>
</li>
<li>
<p>Group read-write variables which are used together into a  structure.  Using a structure is the only way to ensure the memory  locations for all of those variables are close together in a way which is translated  consistently by all gcc versions..</p>
</li>
<li>
<p>Move read-write variables which are often written to by  different threads onto their own cache line.  This might mean adding  padding at the end to fill a remainder of the cache line.  If  combined with step 2, this is often not really wasteful.  Extending  the example above, we might end up with code as follows (assuming  barand xyzzy</p>
</li>
</ol>
<p>are meant to be used together):</p>
<pre><code>  int foo = 1;
  int baz = 3;
  struct {
    struct al1 {
      int bar;
      int xyzzy;
    };
    char pad[CLSIZE - sizeof(struct al1)];
  } rwstruct __attribute__((aligned(CLSIZE))) =
    { { .bar = 2, .xyzzy = 4 } };
</code></pre>
<pre><code> Some code changes are needed (references to `bar` have to be  replaced with `rwstruct.bar`, likewise for `xyzzy`) but  that is all.  The compiler and linker do all the rest. {*This code has  to be compiled with `-fms-extensions`} on the command line.*}
</code></pre>
<ol start="4">
<li>If a variable is used by multiple threads, but every use is  independent, move the variable into TLS.</li>
</ol>
<p><strong>6.4.2 Atomicity Optimizations</strong></p>
<p>If multiple threads modify the same memory location concurrently, processors do not guarantee any specific result. This is a deliberate decision made to avoid costs which are unnecessary in 99.999% of all cases. For instance, if a memory location is in the ‘S’ state and two threads concurrently have to increment its value, the execution pipeline does not have to wait for the cache line to be available in the ‘E’ state before reading the old value from the cache to perform the addition.  Instead it reads the value currently in the cache and, once the cache line is available in state ‘E’, the new value is written back. The result is not as expected if the two cache reads in the two threads happen simultaneously; one addition will be lost.</p>
<p>To assure this does not happen, processors provide atomic operations. These atomic operations would, for instance, not read the old value until it is clear that the addition could be performed in a way that the addition to the memory location appears as atomic.  In addition to waiting for other cores and processors, some processors even signal atomic operations for specific addresses to other devices on the motherboard.  All this makes atomic operations slower.</p>
<p>Processor vendors decided to provide different sets of atomic operations.  Early RISC processors, in line with the ‘R’ for reduced, provided very few atomic operations, sometimes only an atomic bit set and test. {<em>HP Parisc still does not provide more…</em>} At the other end of the spectrum, we have x86 and x86-64 which provide a large number of atomic operations. The generally available atomic operations can be categorized in four classes:</p>
<ul>
<li>
<p><strong>Bit Test</strong></p>
<p>These operations set or clear a bit  atomically and return a status indicating whether the bit was set  before or not.</p>
</li>
<li>
<p><strong>Load Lock/Store Conditional (LL/SC)</strong></p>
<p>{<em>Some people use  “linked” instead of “lock”, it is all the same.</em>}   These operations work as a pair where the special load instruction is  used to start an transaction and the final store will only succeed  if the location has not been modified in the meantime.  The store  operation indicates success or failure, so the program can repeat its  efforts if necessary.</p>
</li>
<li>
<p><strong>Compare-and-Swap (CAS)</strong></p>
<p>This is a  ternary operation which writes a value provided as a parameter into  an address (the second parameter) only if the current value  is the same as the third parameter value;</p>
</li>
<li>
<p><strong>Atomic Arithmetic</strong></p>
<p>These operations are only available on  x86 and x86-64, which can perform arithmetic and logic operations on  memory locations.  These processors have support for non-atomic  versions of these operations but RISC architectures do not.  So it  is no wonder that their availability is limited.</p>
</li>
</ul>
<p>An architecture supports either the LL/SC or the CAS instruction, not both. Both approaches are basically equivalent; they allow the implementation of atomic arithmetic operations equally well, but CAS seems to be the preferred method these days.  All other operations can be indirectly implemented using it.  For instance, an atomic addition:</p>
<pre><code>  int curval;
  int newval;
  do {
    curval = var;
    newval = curval + addend;
  } while (CAS(&amp;var, curval, newval));
</code></pre>
<p>The result of the <code>CAS</code> call indicates whether the operation succeeded or not.  If it returns failure (non-zero value), the loop is run again, the addition is performed, and the <code>CAS</code> call is tried again.  This repeats until it is successful.  Noteworthy about the code is that the address of the memory location has to be computed in two separate instructions. {<em>The <code>CAS</code> opcode on x86 and x86-64 can avoid the load of the value in the second and later iterations but, on this platform, we can write the atomic addition in a simpler way, with a single addition opcode.</em>}  For LL/SC the code looks about the same.</p>
<pre><code>  int curval;
  int newval;
  do {
    curval = LL(var);
    newval = curval + addend;
  } while (SC(var, newval));
</code></pre>
<p>Here we have to use a special load instruction (<code>LL</code>) and we do not have to pass the current value of the memory location to <code>SC</code> since the processor knows if the memory location has been modified in the meantime.</p>
<p>The big differentiators are x86 and x86-64 where we have the atomic operations and, here, it is important to select the proper atomic operation to achieve the best result.  Figure 6.12 shows three different ways to implement an atomic increment operation.</p>
<blockquote>
<div class="table-wrapper"><table><thead><tr><th><code>   for (i = 0; i &lt; N; ++i)      __sync_add_and_fetch(&amp;var,1);</code> <strong>1. Add and Read Result</strong></th></tr></thead><tbody>
<tr><td><code>   for (i = 0; i &lt; N; ++i)      __sync_fetch_and_add(&amp;var,1);</code> <strong>2. Add and Return Old Value</strong></td></tr>
<tr><td><code>   for (i = 0; i &lt; N; ++i) {      long v, n;      do {        v = var;        n = v + 1;      } while (!__sync_bool_compare_and_swap(&amp;var, v, n));    }</code> <strong>3. Atomic Replace with New Value</strong></td></tr>
</tbody></table>
</div>
<p><strong>Figure 6.12: Atomic Increment in a Loop</strong></p>
</blockquote>
<p>All three produce different code on x86 and x86-64 while the code might be identical on other architectures.  There are huge performance differences.  The following table shows the execution time for 1 million increments by four concurrent threads.  The code uses the built-in primitives of gcc (<code>__sync_</code>*).</p>
<blockquote>
<div class="table-wrapper"><table><thead><tr><th>1. Exchange Add</th><th>2. Add Fetch</th><th>3. CAS</th></tr></thead><tbody>
<tr><td>0.23s</td><td>0.21s</td><td>0.73s</td></tr>
</tbody></table>
</div></blockquote>
<p>The first two numbers are similar; we see that returning the old value is a little bit faster.  The important piece of information is the highlighted field, the cost when using CAS. It is, unsurprisingly, a lot more expensive. There are several reasons for this: 1. there are two memory operations, 2. the CAS operation by itself is more complicated and requires even conditional operation, and 3. the whole operation has to be done in a loop in case two concurrent accesses cause a CAS call to fail.</p>
<p>Now a reader might ask a question: why would somebody use the complicated and longer code which utilizes CAS? The answer to this is: the complexity is usually hidden.  As mentioned before, CAS is currently the unifying atomic operation across all interesting architectures.  So some people think it is sufficient to define all atomic operations in terms of CAS.  This makes programs simpler. But as the numbers show, the results can be everything but optimal. The memory handling overhead of the CAS solution is huge.  The following illustrates the execution of just two threads, each on its own core.</p>
<blockquote>
<div class="table-wrapper"><table><thead><tr><th>Thread #1</th><th>Thread #2</th><th><code>var</code> Cache State</th></tr></thead><tbody>
<tr><td><code>v = var</code></td><td></td><td>‘E’ on Proc 1</td></tr>
<tr><td><code>n = v + 1</code></td><td><code>v = var</code></td><td>‘S’ on Proc 1+2</td></tr>
<tr><td>CAS(<code>var</code>)</td><td><code>n = v + 1</code></td><td>‘E’ on Proc 1</td></tr>
<tr><td></td><td>CAS(<code>var</code>)</td><td>‘E’ on Proc 2</td></tr>
</tbody></table>
</div></blockquote>
<p>We see that, within this short period of execution, the cache line status changes at least three times; two of the changes are RFOs. Additionally, the second CAS will fail, so that thread has to repeat the whole operation.  During that operation the same can happen again.</p>
<p>In contrast, when the atomic arithmetic operations are used, the processor can keep the load and store operations needed to perform the addition (or whatever) together.  It can ensure that concurrently-issued cache line requests are blocked until the atomic operation is done. Each loop iteration in the example therefore results in, at most, one RFO cache request and nothing else.</p>
<p>What all this means is that it is crucial to define the machine abstraction at a level at which atomic arithmetic and logic operations can be utilized.  CAS should not be universally used as the unification mechanism.</p>
<p>For most processors, the atomic operations are, by themselves, always atomic.  One can avoid them only by providing completely separate code paths for the case when atomicity is not needed. This means more code, a conditional, and further jumps to direct execution appropriately.</p>
<p>For x86 and x86-64 the situation is different: the same instructions can be used in both atomic and non-atomic ways. To make them atomic, a special prefix for the instruction is used: the <code>lock</code> prefix.  This opens the door for atomic operations to avoid the high costs if the atomicity requirement in a given situation is not needed.  Generic code in libraries, for example, which always has to be thread-safe if needed, can benefit from this.  No information is needed when writing the code, the decision can be made at runtime.  The trick is to jump over the <code>lock</code> prefix.  This trick applies to all the instructions which the x86 and x86-64 processor allow to prefix with <code>lock</code>.</p>
<pre><code>      cmpl $0, multiple_threads
      je   1f
      lock
  1:  add  $1, some_var
</code></pre>
<p>If this assembler code appears cryptic, do not worry, it is simple. The first instruction checks whether a variable is zero or not. Nonzero in this case indicates that more than one thread is running. If the value is zero, the second instruction jumps to label <code>1</code>. Otherwise, the next instruction is executed.  This is the tricky part.  If the <code>je</code> instruction does not jump, the <code>add</code> instruction is executed with the <code>lock</code> prefix.  Otherwise it is executed without the <code>lock</code> prefix.</p>
<p>Adding a relatively expensive operation like a conditional jump (expensive in case the branch prediction fails) seems to be counter productive. Indeed it can be: if multiple threads are running most of the time, the performance is further decreased, especially if the branch prediction is not correct.  But if there are many situations where only one thread is in use, the code is significantly faster.  The alternative of using an if-then-else construct introduces an additional unconditional jump in both cases which can be slower. Given that an atomic operation costs on the order of 200 cycles, the cross-over point for using the trick (or the if-then-else block) is pretty low.  This is definitely a technique to be kept in mind.  Unfortunately this means gcc's <code>__sync_</code>* primitives cannot be used.</p>
<p><strong>6.4.3 Bandwidth Considerations</strong></p>
<p>When many threads are used, and they do not cause cache contention by using the same cache lines on different cores, there still are potential problems.  Each processor has a maximum bandwidth to the memory which is shared by all cores and hyper-threads on that processor.  Depending on the machine architecture (e.g., the one in Figure 2.1), multiple processors might share the same bus to memory or the Northbridge.</p>
<p>The processor cores themselves run at frequencies where, at full speed, even in perfect conditions, the connection to the memory cannot fulfill all load and store requests without waiting. Now, further divide the available bandwidth by the number of cores, hyper-threads, and processors sharing a connection to the Northbridge and suddenly parallelism becomes a big problem. Programs which are, in theory, very efficient may be limited by the memory bandwidth.</p>
<p>In Figure 3.32 we have seen that increasing the FSB speed of a processor can help a lot. This is why, with growing numbers of cores on a processor, we will also see an increase in the FSB speed. Still, this will never be enough if the program uses large working sets and it is sufficiently optimized. Programmers have to be prepared to recognize problems due to limited bandwidth.</p>
<p>The performance measurement counters of modern processors allow the observation of FSB contention. On Core 2 processors the <code>NUS_BNR_DRV</code> event counts the number of cycles a core has to wait because the bus is not ready. This indicates that the bus is highly used and loads from or stores to main memory take even longer than usual. The Core 2 processors support more events which can count specific bus actions like RFOs or the general FSB utilization.  The latter might come in handy when investigating the possibility of scalability of an application during development.  If the bus utilization rate is already  close to 1.0 then the scalability opportunities are minimal.</p>
<p>If a bandwidth problem is recognized, there are several things which can be done.  They are sometimes contradictory so some experimentation might be necessary.  One solution is to buy faster computers, if there are some available.  Getting more FSB speed, faster RAM modules, and possibly memory local to the processor, can—and probably will—help. It can cost a lot, though.  If the program in question is only needed on one (or a few machines) the one-time expense for the hardware might cost less than reworking the program.  In general, though, it is better to work on the program.</p>
<p>After optimizing the program itself to avoid cache misses, the only option left to achieve better bandwidth utilization is to place the threads better on the available cores. By default, the scheduler in the kernel will assign a thread to a processor according to its own policy. Moving a thread from one core to another is avoided when possible. The scheduler does not really know anything about the workload, though.  It can gather information from cache misses etc but this is not much help in many situations.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.42.png" alt="img" /></p>
<p><strong>Figure 6.13: Inefficient Scheduling</strong></p>
</blockquote>
<p>One situation which can cause big FSB usage is when two threads are scheduled on different processors (or cores which do not share a cache) and they use the same data set.  Figure 6.13 shows such a situation.  Core 1 and 3 access the same data (indicated by the same color for the access indicator and the memory area).  Similarly core 2 and 4 access the same data.  But the threads are scheduled on different processors.  This means each data set has to be read twice from memory.  This situation can be handled better.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.43.png" alt="img" /></p>
<p><strong>Figure 6.14: Efficient Scheduling</strong></p>
</blockquote>
<p>In Figure 6.14 we see how it should ideally look like.  Now the total cache size in use is reduced since now core 1 and 2 and core 3 and 4 work on the same data.  The data sets have to be read from memory only once.</p>
<p>This is a simple example but, by extension, it applies to many situations.  As mentioned before, the scheduler in the kernel has no insight into the use of data, so the programmer has to ensure that scheduling is done efficiently.  There are not many kernel interfaces available to communicate this requirement.  In fact, there is only one: defining thread affinity.</p>
<p>Thread affinity means assigning a thread to one or more cores. The scheduler will then choose among those cores (only) when deciding where to run the thread.  Even if other cores are idle they will not be considered.  This might sound like a disadvantage, but it is the price one has to pay.  If too many threads exclusively run on a set of cores the remaining cores might mostly be idle and there is nothing one can do except change the affinity.  By default threads can run on any core.</p>
<p>There are a number of interfaces to query and change the affinity of a thread:</p>
<blockquote>
<pre><code>#define _GNU_SOURCE
#include &lt;sched.h&gt;

int sched_setaffinity(pid_t pid, size_t size, const cpu_set_t *cpuset);
int sched_getaffinity(pid_t pid, size_t size, cpu_set_t *cpuset);
</code></pre>
</blockquote>
<p>These two interfaces are meant to be used for single-threaded code. The <code>pid</code> argument specifies which process's affinity should be changed or determined.  The caller obviously needs appropriate privileges to do this.  The second and third parameter specify the bitmask for the cores.  The first function requires the bitmask to be filled in so that it can set the affinity.  The second fills in the bitmask with the scheduling information of the selected thread.  The interfaces are declared in <code>&lt;sched.h&gt;</code>.</p>
<p>The <code>cpu_set_t</code> type is also defined in that header, along with a number of macros to manipulate and use objects of this type.</p>
<blockquote>
<pre><code>#define _GNU_SOURCE
#include &lt;sched.h&gt;

#define CPU_SETSIZE
#define CPU_SET(cpu, cpusetp)
#define CPU_CLR(cpu, cpusetp)
#define CPU_ZERO(cpusetp)
#define CPU_ISSET(cpu, cpusetp)
#define CPU_COUNT(cpusetp)
</code></pre>
</blockquote>
<p><code>CPU_SETSIZE</code> specifies how many CPUs can be represented in the data structure. The other three macros manipulate <code>cpu_set_t</code> objects. To initialize an object <code>CPU_ZERO</code> should be used; the other two macros should be used to select or deselect individual cores. <code>CPU_ISSET</code> tests whether a specific processor is part of the set.  <code>CPU_COUNT</code> returns the number of cores selected in the set.  The <code>cpu_set_t</code> type provide a reasonable default value for the upper limit on the number of CPUs. Over time it certainly will prove too small; at that point the type will be adjusted.  This means programs always have to keep the size in mind.  The above convenience macros implicitly handle the size according to the definition of <code>cpu_set_t</code>.  If more dynamic size handling is needed an extended set of macros should be used:</p>
<blockquote>
<pre><code>#define _GNU_SOURCE
#include &lt;sched.h&gt;

#define CPU_SET_S(cpu, setsize, cpusetp)
#define CPU_CLR_S(cpu, setsize, cpusetp)
#define CPU_ZERO_S(setsize, cpusetp)
#define CPU_ISSET_S(cpu, setsize, cpusetp)
#define CPU_COUNT_S(setsize, cpusetp)
</code></pre>
</blockquote>
<p>These interfaces take an additional parameter with the size.  To be able to allocate dynamically sized CPU sets three macros are provided:</p>
<blockquote>
<pre><code>#define _GNU_SOURCE
#include &lt;sched.h&gt;

#define CPU_ALLOC_SIZE(count)
#define CPU_ALLOC(count)
#define CPU_FREE(cpuset)
</code></pre>
</blockquote>
<p>The <code>CPU_ALLOC_SIZE</code> macro returns the number of bytes which have to be allocated for a <code>cpu_set_t</code> structure which can handle <code>count</code> CPUs.  To allocate such a block the <code>CPU_ALLOC</code> macro can be used. The memory allocated this way should be freed with <code>CPU_FREE</code>.  The functions will likely use <code>malloc</code> and <code>free</code> behind the scenes but this does not necessarily have to remain this way.</p>
<p>Finally, a number of operations on CPU set objects are defined:</p>
<blockquote>
<pre><code>#define _GNU_SOURCE
#include &lt;sched.h&gt;

#define CPU_EQUAL(cpuset1, cpuset2)
#define CPU_AND(destset, cpuset1, cpuset2)
#define CPU_OR(destset, cpuset1, cpuset2)
#define CPU_XOR(destset, cpuset1, cpuset2)
#define CPU_EQUAL_S(setsize, cpuset1, cpuset2)
#define CPU_AND_S(setsize, destset, cpuset1, cpuset2)
#define CPU_OR_S(setsize, destset, cpuset1, cpuset2)
#define CPU_XOR_S(setsize, destset, cpuset1, cpuset2)
</code></pre>
</blockquote>
<p>These two sets of four macros can check two sets for equality and perform logical AND, OR, and XOR operations on sets. These operations come in handy when using some of the libNUMA functions (see Section 12).</p>
<p>A process can determine on which processor it is currently running using the <code>sched_getcpu</code> interface:</p>
<blockquote>
<pre><code>#define _GNU_SOURCE
#include &lt;sched.h&gt;
int sched_getcpu(void);
</code></pre>
</blockquote>
<p>The result is the index of the CPU in the CPU set.  Due to the nature of scheduling this number cannot always be 100% correct.  The thread might have been moved to a different CPU between the time the result was returned and when the thread returns to userlevel.  Programs always have to take this possibility of inaccuracy into account. More important is, in any case, the set of CPUs the thread is allowed to run on. This set can be retrieved using <code>sched_getaffinity</code>.  The set is inherited by child threads and processes.  Threads cannot rely on the set to be stable over the lifetime.  The affinity mask can be set from the outside (see the <code>pid</code> parameter in the prototypes above); Linux also supports CPU hot-plugging which means CPUs can vanish from the system—and, therefore, also from the affinity CPU set.</p>
<p>In multi-threaded programs, the individual threads officially have no process ID as defined by POSIX and, therefore, the two functions above cannot be used.  Instead <code>&lt;pthread.h&gt;</code> declares four different interfaces:</p>
<blockquote>
<pre><code>#define _GNU_SOURCE
#include &lt;pthread.h&gt;

int pthread_setaffinity_np(pthread_t th, size_t size,
                           const cpu_set_t *cpuset);
int pthread_getaffinity_np(pthread_t th, size_t size, cpu_set_t *cpuset);
int pthread_attr_setaffinity_np(pthread_attr_t *at,
                                size_t size, const cpu_set_t *cpuset);
int pthread_attr_getaffinity_np(pthread_attr_t *at, size_t size, 
                                cpu_set_t *cpuset);
</code></pre>
</blockquote>
<p>The first two interfaces are basically equivalent to the two we have already seen, except that they take a thread handle in the first parameter instead of a process ID.  This allows addressing individual threads in a process.  It also means that these interfaces cannot be used from another process, they are strictly for intra-process use.  The third and fourth interfaces use a thread attribute. These attributes are used when creating a new thread. By setting the attribute, a thread can be scheduled from the start on a specific set of CPUs.  Selecting the target processors this early—instead of after the thread already started—can be of advantage on many different levels, including (and especially) memory allocation (see NUMA in Section 6.5).</p>
<p>Speaking of NUMA, the affinity interfaces play a big role in NUMA programming, too.  We will come back to that case shortly.</p>
<p>So far, we have talked about the case where the working set of two threads overlaps such that having both threads on the same core makes sense. The opposite can be true, too.  If two threads work on separate data sets, having them scheduled on the same core can be a problem.  Both threads fight for the same cache, thereby reducing each others effective use of the cache.  Second, both data sets have to be loaded into the same cache; in effect this increases the amount of data that has to be loaded and, therefore, the available bandwidth is cut in half.</p>
<p>The solution in this case is to set the affinity of the threads so that they cannot be scheduled on the same core.  This is the opposite from the previous situation, so it is important to understand the situation one tries to optimize before making any changes.</p>
<p>Optimizing for cache sharing to optimize bandwidth is in reality an aspect of NUMA programming which is covered in the next section.  One only has to extend the notion of “memory” to the caches.  This will become ever more important once the number of levels of cache increases. For this reason, the solution to multi-core scheduling is available in the NUMA support library.  See the code samples in Section 12 for ways to determine the affinity masks without hardcoding system details or diving into the depth of the <code>/sys</code> filesystem.</p>
<h3 id="65-numa-programming"><a class="header" href="#65-numa-programming">6.5 NUMA Programming</a></h3>
<p>For NUMA programming everything said so far about cache optimizations applies as well.  The differences only start below that level.  NUMA introduces different costs when accessing different parts of the address space.  With uniform memory access we can optimize to minimize page faults (see Section 7.5) but that is about it.  All pages are created equal.</p>
<p>NUMA changes this.  Access costs can depend on the page which is accessed.  Differing access costs also increase the importance of optimizing for memory page locality.  NUMA is inevitable for most SMP machines since both Intel with CSI (for x86,x86-64, and IA-64) and AMD (for Opteron) use it.  With an increasing number of cores per processor we are likely to see a sharp reduction of SMP systems being used (at least outside data centers and offices of people with terribly high CPU usage requirements).  Most home machines will be fine with just one processor and hence no NUMA issues.  But this a) does not mean programmers can ignore NUMA and b) it does not mean there are not related issues.</p>
<p>If one thinks about generalizations to NUMA one quickly realizes the concept extends to processor caches as well.  Two threads on cores using the same cache will collaborate faster than threads on cores not sharing a cache.  This is not a fabricated case:</p>
<ul>
<li></li>
<li>
<p>early dual-core processors had no L2 sharing.</p>
</li>
<li>
<p>Intel's Core 2 QX 6700 and QX 6800 quad core chips, for instance, have two separate L2 caches.</p>
</li>
<li>
<p>as speculated early, with more cores on a chip and the desire to  unify caches, we will have more levels of caches.</p>
</li>
</ul>
<p>Caches form their own hierarchy, and placement of threads on cores becomes important for sharing (or not) of caches.  This is not very different from the problems NUMA is facing and, therefore, the two concepts can be unified.  Even people only interested in non-SMP machines should therefore read this section.</p>
<p>In Section 5.3 we have seen that the Linux kernel provides a lot of information which is useful—and needed—in NUMA programming. Collecting this information is not that easy, though.  The currently available NUMA library on Linux is wholly inadequate for this purpose.  A much more suitable version is currently under construction by the author.</p>
<p>The existing NUMA library, <code>libnuma</code>, part of the numactl package, provides no access to system architecture information.  It is only a wrapper around the available system calls together with some convenience interfaces for commonly used operations.  The system calls available on Linux today are:</p>
<ul>
<li>
<p><strong><code>mbind</code></strong></p>
<p>Select binding for specified memory pages to nodes.</p>
</li>
<li>
<p><strong><code>set_mempolicy</code></strong></p>
<p>Set the default memory binding policy.</p>
</li>
<li>
<p><strong><code>get_mempolicy</code></strong></p>
<p>Get the default memory binding policy.</p>
</li>
<li>
<p><strong><code>migrate_pages</code></strong></p>
<p>Migrate all pages of a process on a given set of nodes to a different set of nodes.</p>
</li>
<li>
<p><strong><code>move_pages</code></strong></p>
<p>Move selected pages to given node or request node information about pages.</p>
</li>
</ul>
<p>These interfaces are declared in <code>&lt;numaif.h&gt;</code> which comes along with the <code>libnuma</code> library.  Before we go into more details we have to understand the concept of memory policies.</p>
<p><strong>6.5.1 Memory Policy</strong></p>
<p>The idea behind defining a memory policy is to allow existing code to work reasonably well in a NUMA environment without major modifications.  The policy is inherited by child processes, which makes it possible to use the numactl tool.  This tool can be used to, among other things, start a program with a given policy.</p>
<p>The Linux kernel supports the following policies:</p>
<ul>
<li>
<p><strong><code>MPOL_BIND</code></strong></p>
<p>Memory is allocated only from the given set of nodes.  If this is not possible allocation fails.</p>
</li>
<li>
<p><strong><code>MPOL_PREFERRED</code></strong></p>
<p>Memory is preferably allocated from the given set of nodes.  If this fails memory from other nodes is considered.</p>
</li>
<li>
<p><strong><code>MPOL_INTERLEAVE</code></strong></p>
<p>Memory is allocated equally from the specified nodes.  The node is selected either by the offset in the virtual memory region for VMA-based policies, or through a free-running counter for task-based policies.</p>
</li>
<li>
<p><strong><code>MPOL_DEFAULT</code></strong></p>
<p>Choose the allocation based on the default for the region.</p>
</li>
</ul>
<p>This list seems to recursively define policies.  This is half true. In fact, memory policies form a hierarchy (see Figure 6.15).</p>
<blockquote>
<p><strong>Figure 6.15: Memory Policy Hierarchy</strong></p>
</blockquote>
<p>If an address is covered by a VMA policy then this policy is used.  A special kind of policy is used for shared memory segments.  If no policy for the specific address is present, the task's policy is used. If this is also not present the system's default policy is used.</p>
<p>The system default is to allocate memory local to the thread requesting the memory.  No task and VMA policies are provided by default.  For a process with multiple threads the local node is the “home” node, the one which first ran the process.  The system calls mentioned above can be used to select different policies.</p>
<p><strong>6.5.2 Specifying Policies</strong></p>
<p>The <code>set_mempolicy</code> call can be used to set the task policy for the current thread (task in kernel-speak).  Only the current thread is affected, not the entire process.</p>
<blockquote>
<pre><code>#include &lt;numaif.h&gt;

long set_mempolicy(int mode, 
                   unsigned long *nodemask, 
		   unsigned long maxnode);
</code></pre>
</blockquote>
<p>The <code>mode</code> parameter must be one of the <code>MPOL_*</code> constants introduced in the previous section.  The <code>nodemask</code> parameter specifies the memory nodes to use and <code>maxnode</code> is the number of nodes (i.e., bits) in <code>nodemask</code>.  If <code>MPOL_DEFAULT</code> is used the <code>nodemask</code> parameter is ignored.  If a null pointer is passed as <code>nodemask</code> for <code>MPOL_PREFERRED</code> the local node is selected. Otherwise <code>MPOL_PREFERRED</code> uses the lowest node number with the corresponding bit set in <code>nodemask</code>.</p>
<p>Setting a policy does not have any effect on already-allocated memory. Pages are not automatically migrated; only future allocations are affected. Note the difference between memory allocation and address space reservation: an address space region established using <code>mmap</code> is usually not automatically allocated. The first read or write operation on the memory region will allocate the appropriate page. If the policy changes between accesses to different pages of the same address space region, or if the policy allows allocation of memory from different nodes, a seemingly uniform address space region might be scattered across many memory nodes.</p>
<p><strong>6.5.3 Swapping and Policies</strong></p>
<p>If physical memory runs out, the system has to drop clean pages and save dirty pages to swap. The Linux swap implementation discards node information when it writes pages to swap.  That means when the page is reused and paged in the node which is used will be chosen from scratch.  The policies for the thread will likely cause a node which is close to the executing processors to be chosen, but the node might be different from the one used before.</p>
<p>This changing association means that the node association cannot be stored by a program as a property of the page.  The association can change over time.  For pages which are shared with other processes this can also happen because a process asks for it (see the discussion of <code>mbind</code> below).  The kernel by itself can migrate pages if one node runs out of space while other nodes still have free space.</p>
<p>Any node association the user-level code learns about can therefore be true for only a short time. It is more of a hint than absolute information.  Whenever accurate knowledge is required the <code>get_mempolicy</code> interface should be used (see Section 6.5.5).</p>
<p><strong>6.5.4 VMA Policy</strong></p>
<p>To set the VMA policy for an address range a different interface has to be used:</p>
<blockquote>
<pre><code>#include &lt;numaif.h&gt;

long mbind(void *start, unsigned long len,
           int mode,
           unsigned long *nodemask,
           unsigned long maxnode,
           unsigned flags);
</code></pre>
</blockquote>
<p>This interface registers a new VMA policy for the address range [<code>start</code>, <code>start</code> + <code>len</code>).  Since memory handling operates on pages the start address must be page-aligned.  The <code>len</code> value is rounded up to the next page size.</p>
<p>The <code>mode</code> parameter specifies, again, the policy; the values must be chosen from the list in Section 6.5.1. As with <code>set_mempolicy</code>, the <code>nodemask</code> parameter is only used for some policies.  Its handling is identical.</p>
<p>The semantics of the <code>mbind</code> interface depends on the value of the <code>flags</code> parameter.  By default, if <code>flags</code> is zero, the system call sets the VMA policy for the address range.  Existing mappings are not affected.  If this is not sufficient there are currently three flags to modify this behavior; they can be selected individually or together:</p>
<ul>
<li>
<p><strong><code>MPOL_MF_STRICT</code></strong></p>
<p>The call to <code>mbind</code> will fail if not all pages are on the nodes specified by <code>nodemask</code>.  In case this flag is used together with <code>MPOL_MF_MOVE</code> and/or <code>MPOL_MF_MOVEALL</code> the call will fail if any page cannot be moved.</p>
</li>
<li>
<p><strong><code>MPOL_MF_MOVE</code></strong></p>
<p>The kernel will try to move any page in the address range allocated on a node not in the set specified by <code>nodemask</code>. By default, only pages used exclusively by the current process's page tables are moved.</p>
</li>
<li>
<p><strong><code>MPOL_MF_MOVEALL</code></strong></p>
<p>Like <code>MPOL_MF_MOVE</code> but the kernel will try to move all pages, not just those used by the current process's page tables alone.  This has system-wide implications since it influences the memory access of other processes—which are possibly not owned by the same user—as well.  Therefore <code>MPOL_MF_MOVEALL</code> is a privileged operation (<code>CAP_NICE</code> capability is needed).</p>
</li>
</ul>
<p>Note that support for <code>MPOL_MF_MOVE</code> and <code>MPOL_MF_MOVEALL</code> was added only in the 2.6.16 Linux kernel.</p>
<p>Calling <code>mbind</code> without any flags is most useful when the policy for a newly reserved address range has to be specified before any pages are actually allocated.</p>
<blockquote>
<pre><code>void *p = mmap(NULL, len, PROT_READ|PROT_WRITE, MAP_ANON, -1, 0);
if (p != MAP_FAILED)
  mbind(p, len, mode, nodemask, maxnode, 0);
</code></pre>
</blockquote>
<p>This code sequence reserve an address space range of <code>len</code> bytes and specifies that the policy <code>mode</code> referencing the memory nodes in <code>nodemask</code> should be used.  Unless the <code>MAP_POPULATE</code> flag is used with <code>mmap</code>, no memory will have been allocated by the time of the <code>mbind</code> call and, therefore, the new policy applies to all pages in that address space region.</p>
<p>The <code>MPOL_MF_STRICT</code> flag alone can be used to determine whether any page in the address range described by the <code>start</code> and <code>len</code> parameters to <code>mbind</code> is allocated on nodes other than those specified by <code>nodemask</code>.  No allocated pages are changed. If all pages are allocated on the specified nodes, the VMA policy for the address space region will be changed according to <code>mode</code>.</p>
<p>Sometimes the rebalancing of memory is needed, in which case it might be necessary to move pages allocated on one node to another node. Calling <code>mbind</code> with <code>MPOL_MF_MOVE</code> set makes a best effort to achieve that.  Only pages which are solely referenced by the process's page table tree are considered for moving.  There can be multiple users in the form of threads or other processes which share that part of the page table tree. It is not possible to affect other processes which happen to map the same data. These pages do not share the page table entries.</p>
<p>If both <code>MPOL_MF_STRICT</code> and <code>MPOL_MF_MOVE</code> are passed to <code>mbind</code> the kernel will try to move all pages which are not allocated on the specified nodes.  If this is not possible the call will fail.  Such a call might be useful to determine whether there is a node (or set of nodes) which can house all the pages.  Several combinations can be tried in succession until a suitable node is found.</p>
<p>The use of <code>MPOL_MF_MOVEALL</code> is harder to justify unless running the current process is the main purpose of the computer.  The reason is that even pages that appear in multiple page tables are moved. That can easily affect other processes in a negative way.  This operation should thus be used with caution.</p>
<p><strong>6.5.5 Querying Node Information</strong></p>
<p>The <code>get_mempolicy</code> interface can be used to query a variety of facts about the state of NUMA for a given address.</p>
<blockquote>
<pre><code>#include &lt;numaif.h&gt;
long get_mempolicy(int *policy,
             const unsigned long *nmask,
             unsigned long maxnode,
             void *addr, int flags);
</code></pre>
</blockquote>
<p>When <code>get_mempolicy</code> is called without a flag set in <code>flags</code>, the information about the policy for address <code>addr</code> is stored in the word pointed to by <code>policy</code> and in the bitmask for the nodes pointed to by <code>nmask</code>.  If <code>addr</code> falls into an address space region for which a VMA policy has been specified, information about that policy is returned. Otherwise information about the task policy or, if necessary, system default policy will be returned.</p>
<p>If the <code>MPOL_F_NODE</code> flag is set in <code>flags</code>, and the policy governing <code>addr</code> is <code>MPOL_INTERLEAVE</code>, the value stored in the word pointed to by <code>policy</code> is the index of the node on which the next allocation is going to happen.  This information can potentially be used to set the affinity of a thread which is going to work on the newly-allocated memory. This might be a less costly way to achieve proximity, especially if the thread has yet to be created.</p>
<p>The <code>MPOL_F_ADDR</code> flag can be used to retrieve yet another completely different data item. If this flag is used, the value stored in the word pointed to by <code>policy</code> is the index of the memory node on which the memory for the page containing <code>addr</code> has been allocated.  This information can be used to make decisions about possible page migration, to decide which thread could work on the memory location most efficiently, and many more things.</p>
<p>The CPU—and therefore memory node—a thread is using is much more volatile than its memory allocations. Memory pages are, without explicit requests, only moved in extreme circumstances. A thread can be assigned to another CPU as the result of rebalancing the CPU loads.  Information about the current CPU and node might therefore be short-lived.  The scheduler will try to keep the thread on the same CPU, and possibly even on the same core, to minimize performance losses due to cold caches.  This means it is useful to look at the current CPU and node information; one only must avoid assuming the association will not change.</p>
<p>libNUMA provides two interfaces to query the node information for a given virtual address space range:</p>
<blockquote>
<pre><code>#include &lt;libNUMA.h&gt;

int NUMA_mem_get_node_idx(void *addr);
int NUMA_mem_get_node_mask(void *addr,
                           size_t size,
                           size_t __destsize,
                           memnode_set_t *dest);
</code></pre>
</blockquote>
<p><code>NUMA_mem_get_node_mask</code> sets in <code>dest</code> the bits for all memory nodes on which the pages in the range [<code>addr</code>, <code>addr</code>+<code>size</code>) are (or would be) allocated, according to the governing policy.  <code>NUMA_mem_get_node</code> only looks at the address <code>addr</code> and returns the index of the memory node on which this address is (or would be) allocated.  These interfaces are simpler to use than <code>get_mempolicy</code> and probably should be preferred.</p>
<p>The CPU currently used by a thread can be queried using <code>sched_getcpu</code> (see Section 6.4.3).  Using this information, a  program can determine the memory node(s) which are local to the CPU using the <code>NUMA_cpu_to_memnode</code> interface from libNUMA:</p>
<blockquote>
<pre><code>#include &lt;libNUMA.h&gt;

int NUMA_cpu_to_memnode(size_t cpusetsize,
                        const cpu_set_t *cpuset,
                        size_t memnodesize,
                        memnode_set_t *
                        memnodeset);
</code></pre>
</blockquote>
<p>A call to this function will set (in the memory node set pointed to by the fourth parameter) all the bits corresponding to memory nodes which are local to any of the CPUs in the set pointed to by the second parameter.  Just like CPU information itself, this information is only correct until the configuration of the machine changes (for instance, CPUs get removed and added).</p>
<p>The bits in the <code>memnode_set_t</code> objects can be used in calls to the low-level functions like <code>get_mempolicy</code>. It is more convenient to use the other functions in libNUMA.  The reverse mapping is available through:</p>
<blockquote>
<pre><code>#include &lt;libNUMA.h&gt;

int NUMA_memnode_to_cpu(size_t memnodesize,
                        const memnode_set_t *
                        memnodeset,
                        size_t cpusetsize,
                        cpu_set_t *cpuset);
</code></pre>
</blockquote>
<p>The bits set in the resulting <code>cpuset</code> are those of the CPUs local to any of the memory nodes with corresponding bits set in <code>memnodeset</code>.  For both interfaces, the programmer has to be aware that the information can change over time (especially with CPU hot-plugging). In many situations, a single bit is set in the input bit set, but it is also meaningful, for instance, to pass the entire set of CPUs retrieved by a call to <code>sched_getaffinity</code> to <code>NUMA_cpu_to_memnode</code> to determine which are the memory nodes the thread ever can have direct access to.</p>
<p><strong>6.5.6 CPU and Node Sets</strong></p>
<p>Adjusting code for SMP and NUMA environments by changing the code to use the interfaces described so far might be prohibitively expensive (or impossible) if the sources are not available. Additionally, the system administrator might want to impose restrictions on the resources a user and/or process can use.  For these situations the Linux kernel supports so-called CPU sets.  The name is a bit misleading since memory nodes are also covered.  They also have nothing to do with the <code>cpu_set_t</code> data type.</p>
<p>The interface to CPU sets is, at the moment, a special filesystem.  It is usually not mounted (so far at least).  This can be changed with</p>
<pre><code>     mount -t cpuset none /dev/cpuset
</code></pre>
<p>Of course the mount point <code>/dev/cpuset</code> must exist.  The content of this directory is a description of the default (root) CPU set.  It comprises initially all CPUs and all memory nodes.  The <code>cpus</code> file in that directory shows the CPUs in the CPU set, the <code>mems</code> file the memory nodes, the <code>tasks</code> file the processes.</p>
<p>To create a new CPU set one simply creates a new directory somewhere in the hierarchy.  The new CPU set will inherit all settings from the parent.  Then the CPUs and memory nodes for new CPU set can be changed by writing the new values into the <code>cpus</code> and <code>mems</code> pseudo files in the new directory.</p>
<p>If a process belongs to a CPU set, the settings for the CPUs and memory nodes are used as masks for the affinity and memory policy bitmasks. That means the program cannot select any CPU in the affinity mask which is not in the <code>cpus</code> file for the CPU set the process is using (i.e., where it is listed in the <code>tasks</code> file).  Similarly for the node masks for the memory policy and the <code>mems</code> file.</p>
<p>The program will not experience any errors unless the bitmasks are empty after the masking, so CPU sets are an almost-invisible means to control program execution.  This method is especially efficient on large machines with lots of CPUs and/or memory nodes.  Moving a process into a new CPU set is as simple as writing the process ID into the <code>tasks</code> file of the appropriate CPU set.</p>
<p>The directories for the CPU sets contain a number of other files which can be used to specify details like behavior under memory pressure and exclusive access to CPUs and memory nodes.  The interested reader is referred to the file <code>Documentation/cpusets.txt</code> in the kernel source tree.</p>
<p><strong>6.5.7 Explicit NUMA Optimizations</strong></p>
<p>All the local memory and affinity rules cannot help out if all threads on all the nodes need access to the same memory regions. It is, of course, possible to simply restrict the number of threads to a number supportable by the processors which are directly connected to the memory node.  This does not take advantage of SMP NUMA machines, though, and is therefore not a real option.</p>
<p>If the data in question is read-only there is a simple solution: replication.  Each node can get its own copy of the data so that no inter-node accesses are necessary.  Code to do this can look like this:</p>
<blockquote>
<pre><code>void *local_data(void) {
  static void *data[NNODES];
  int node =
    NUMA_memnode_self_current_idx();
  if (node == -1)
    /* Cannot get node, pick one.  */
    node = 0;
  if (data[node] == NULL)
    data[node] = allocate_data();
  return data[node];
}
void worker(void) {
  void *data = local_data();
  for (...)
    compute using data
}
</code></pre>
</blockquote>
<p>In this code the function <code>worker</code> prepares by getting a pointer to the local copy of the data by a call to <code>local_data</code>.  Then it proceeds with the loop, which uses this pointer.  The <code>local_data</code> function keeps a list of the already allocated copies of the data around.  Each system has a limited number of memory nodes, so the size of the array with the pointers to the per-node memory copies is limited in size.  The <code>NUMA_memnode_system_count</code> function from libNUMA returns this number.  If the pointer for the current node, as determined by the <code>NUMA_memnode_self_current_idx</code> call, is not yet known a new copy is allocated.</p>
<p>It is important to realize that nothing terrible happens if the threads get scheduled onto another CPU connected to a different memory node after the <code>sched_getcpu</code> system call.  It just means that the accesses using the <code>data</code> variable in <code>worker</code> access memory on another memory node.  This slows the program down until <code>data</code> is computed anew, but that is all.  The kernel will always avoid gratuitous rebalancing of the per-CPU run queues.  If such a transfer happens it is usually for a good reason and will not happen again for the near future.</p>
<p>Things are more complicated when the memory area in question is writable.  Simple duplication will not work in this case.  Depending on the exact situation there might a number of possible solutions.</p>
<p>For instance, if the writable memory region is used to accumulate results, it might be possible to first create a separate region for each memory node in which the results are accumulated. Then, when this work is done, all the per-node memory regions are combined to get the total result. This technique can work even if the work never really stops, but intermediate results are needed.  The requirement for this approach is that the accumulation of a result is stateless, i.e., it does not depend on the previously collected results.</p>
<p>It will always be better, though, to have direct access to the writable memory region.  If the number of accesses to the memory region is substantial, it might be a good idea to force the kernel to migrate the memory pages in question to the local node.  If the number of accesses is really high, and the writes on different nodes do not happen concurrently, this could help. But be aware that the kernel cannot perform miracles: the page migration is a copy operation and as such it is not cheap.  This cost has to be amortized.</p>
<p><strong>6.5.8 Utilizing All Bandwidth</strong></p>
<p>The numbers in Figure 5.4 show that access to remote memory when the caches are ineffective is not measurably slower than access to local memory.  This means a program could possibly save bandwidth to the local memory by writing data it does not have to read again into memory attached to another processor. The bandwidth of the connection to the DRAM modules and the bandwidth of the interconnects are mostly independent, so parallel use could improve overall performance.</p>
<p>Whether this is really possible depends on many factors.  One really has to be sure that caches are ineffective since otherwise the slowdown related to remote accesses is measurable.  Another big problem is whether the remote node has any needs for its own memory bandwidth.  This possibility must be examined in detail before the approach is taken.  In theory, using all the bandwidth available to a processor can have positive effects.  A family 10h Opteron processor can be directly connected to up to four other processors.  Utilizing all that additional bandwidth, perhaps coupled with appropriate prefetches (especially <code>prefetchw</code>) could lead to improvements if the rest of the system plays along.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="7-memory-performance-tools"><a class="header" href="#7-memory-performance-tools">7 Memory Performance Tools</a></h2>
<p>A wide variety of tools is available to help programmers understand the cache and memory use of a program.  Modern processors have performance monitoring hardware that can be used.  Some events are hard to measure exactly, so there is also room for simulation.  When it comes to higher-level functionality, there are special tools to monitor the execution of a process.  We will introduce a set of commonly used tools available on most Linux systems.</p>
<h3 id="71-memory-operation-profiling"><a class="header" href="#71-memory-operation-profiling">7.1 Memory Operation Profiling</a></h3>
<p>Profiling memory operations requires collaboration from the hardware. It is possible to gather some information in software alone, but this is either coarse-grained or merely a simulation.  Examples of simulation will be shown in Section 7.2 and Section 7.5.  Here we will concentrate on measurable memory effects.</p>
<p>Access to performance monitoring hardware on Linux is provided by <a href="http://oprofile.sourceforge.net/">oprofile.</a>  Oprofile provides continuous profiling capabilities as first described in [continuous]; it performs statistical, system-wide profiling with an easy-to-use interface. Oprofile is by no means the only way the performance measurement functionality of processors can be used; Linux developers are working on <a href="http://www.hpl.hp.com/research/linux/perfmon/pfmon.php4">pfmon</a> which might at some point be sufficiently widely deployed to warrant being described here, too.</p>
<p>The interface oprofile provides is simple and minimal but also pretty low-level, even if the optional GUI is used.  The user has to select among the events the processor can record.  The architecture manuals for the processors describe the events but, oftentimes, it requires extensive knowledge about the processors themselves to interpret the data.  Another problem is the interpretation of the collected data.  The performance measurement counters are absolute values and can grow arbitrarily.  How high is too high for a given counter?</p>
<p>A partial answer to this problem is to avoid looking at the absolute values and, instead, relate multiple counters to each other. Processors can monitor more than one event; the ratio of the collected absolute values can then be examined.  This gives nice, comparable results.  Often the divisor is a measure of processing time, the number of clock cycles or the number of instructions.  As an initial stab at program performance, relating just these two numbers by themselves is useful.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.45.png" alt="img" /></p>
<p><strong>Figure 7.1: Cycles per Instruction (Follow Random)</strong></p>
</blockquote>
<p>Figure 7.1 shows the Cycles Per Instruction (CPI) for the simple random “Follow” test case for the various working set sizes.  The names of the events to collect this information for most Intel processor are <code>CPU_CLK_UNHALTED</code> and <code>INST_RETIRED</code>.  As the names suggest, the former counts the clock cycles of the CPU and the latter the number of instructions.  We see a picture similar to the cycles per list element measurements we used.  For small working set sizes the ratio is 1.0 or even lower.  These measurements were made on a Intel Core 2 processor, which is multi-scalar and can work on several instructions at once.  For a program which is not limited by memory bandwidth, the ratio can be significantly below 1.0 but, in this case, 1.0 is pretty good.</p>
<p>Once the L1d is no longer large enough to hold the working set, the CPI jumps to just below 3.0.  Note that the CPI ratio averages the penalties for accessing L2 over all instructions, not just the memory instructions.  Using the cycles for list element data, it can be worked out how many instructions per list element are needed.  If even the L2 cache is not sufficient, the CPI ratio jumps to more than 20.  These are expected results.</p>
<p>But the performance measurement counters are supposed to give more insight into what is going on in the processor.  For this we need to think about processor implementations.  In this document, we are concerned with cache handling details, so we have to look at events related to the caches.  These events, their names, and what they count, are processor-specific.  This is where oprofile is currently hard to use, irrespective of the simple user interface: the user has to figure out the performance counter details by her/himself.  In Section 10 we will see details about some processors.</p>
<p>For the Core 2 processor the events to look for are <code>L1D_REPL</code>, <code>DTLB_MISSES</code>, and <code>L2_LINES_IN</code>.  The latter can measure both all misses and misses caused by instructions instead of hardware prefetching.  The results for the random “Follow” test can be seen in Figure 7.2.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.44.png" alt="img" /></p>
<p><strong>Figure 7.2: Measured Cache Misses (Follow Random)</strong></p>
</blockquote>
<p>All ratios are computed using the number of retired instructions (<code>INST_RETIRED</code>).  This means that instructions not touching memory are also counted, which, in turn, means that the number of instructions which do touch memory and which suffer a cache miss is even higher than shown in the graph.</p>
<p>The L1d misses tower over all the others since an L2 miss implies, for Intel processors, an L1d miss due to the use of inclusive caches.  The processor has 32k of L1d and so we see, as expected, the L1d rate go up from zero at about that working set size (there are other uses of the cache beside the list data structure, which means the increase happens between the 16k and 32k mark).  It is interesting to see that the hardware prefetching can keep the miss rate at 1% for a working set size up to and including 64k. After that the L1d rate skyrockets.</p>
<p>The L2 miss rate stays zero until the L2 is exhausted; the few misses due to other uses of L2 do not influence the numbers much.  Once the size of L2 (221 bytes) is exceeded, the miss rates rise. It is important to notice that the L2 demand miss rate is nonzero. This indicates that the hardware prefetcher does not load all the cache lines needed by instructions later.  This is expected, the randomness of the accesses prevents perfect prefetching.  Compare this with the data for the sequential read in Figure 7.3.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.46.png" alt="img" /></p>
<p><strong>Figure 7.3: Measured Cache Misses (Follow Sequential)</strong></p>
</blockquote>
<p>In this graph we can see that the L2 demand miss rate is basically zero (note the scale of this graph is different from Figure 7.2). For the sequential access case, the hardware prefetcher works perfectly: almost all L2 cache misses are caused by the prefetcher. The fact that the L1d and L2 miss rates are the same shows that all L1d cache misses are handled by the L2 cache without further delays.  This is the ideal case for all programs but it is, of course, hardly ever achievable.</p>
<p>The fourth line in both graphs is the DTLB miss rate (Intel has separate TLBs for code and data, DTLB is the data TLB).  For the random access case, the DTLB miss rate is significant and contributes to the delays.  What is interesting is that the DTLB penalties set in before the L2 misses.  For the sequential access case the DTLB costs are basically zero.</p>
<p>Going back to the matrix multiplication example in Section 6.2.1 and the example code in Section 9.1, we can make use of three more counters.  The <code>SSE_PRE_MISS</code>, <code>SSE_PRE_EXEC</code>, and <code>LOAD_HIT_PRE</code> counters can be used to see how effective the software prefetching is.  If the code in Section 9.1 is run we get the following results:</p>
<blockquote>
<div class="table-wrapper"><table><thead><tr><th>Description</th><th>Ratio</th></tr></thead><tbody>
<tr><td>Useful NTA prefetches</td><td>2.84%</td></tr>
<tr><td>Late NTA prefetches</td><td>2.65%</td></tr>
</tbody></table>
</div></blockquote>
<p>The low useful NTA (non-temporal aligned) prefetch ratio indicates that many prefetch instructions are executed for cache lines which are already loaded, so no work is needed.  This means the processor wastes time to decode the prefetch instruction and look up the cache. One cannot judge the code too harshly, though. Much depends on the size of the caches of the processor used; the hardware prefetcher also plays a role.</p>
<p>The low late NTA prefetch ratio is misleading.  The ratio means that 2.65% of all prefetch instructions are issued too late.  The instruction which needs the data is executed before the data could be prefetched into the cache.  It must be kept in mind that only 2.84%+2.65%=5.5% of the prefetch instructions were of any use.  Of the NTA prefetch instructions which are useful, 48% did not finish in time.  The code therefore can be optimized further:</p>
<ul>
<li>most of the prefetch instructions are not needed.</li>
<li>the use of the prefetch instruction can be adjusted to match the  hardware better.</li>
</ul>
<p>It is left as an exercise to the reader to determine the best solution for the available hardware.  The exact hardware specification plays a big role.  On Core 2 processors the latency of the SSE arithmetic operations is 1 cycle. Older versions had a latency of 2 cycles, meaning that the hardware prefetcher and the prefetch instructions had more time to bring in the data.</p>
<p>To determine where prefetches might be needed—or are unnecessary—one can use the opannotate program.  It lists the source or assembler code of the program and shows the instructions where the event was recognized.  Note that there are two sources of vagueness:</p>
<ol>
<li>Oprofile performs stochastic profiling.  Only every  N</li>
</ol>
<p>th</p>
<pre><code>event (where N is a per-event threshold with an  enforced minimum) is recorded to avoid slowing down operation of the  system too much. There might be lines which cause 100 events and yet  they might not show up in the report.
</code></pre>
<ol start="2">
<li>Not all events are recorded accurately. For example, the instruction counter  at the time a specific event was recorded might be incorrect. Processors being  multi-scalar makes it hard to give a 100% correct answer.  A few  events on some processors are exact, though.</li>
</ol>
<p>The annotated listings are useful for more than determining the prefetching information.  Every event is recorded with the instruction pointer;  it is therefore also possible to pinpoint other hot spots in the program.  Locations which are the source of many <code>INST_RETIRED</code> events are executed frequently and deserve to be tuned.  Locations where many cache misses are reported might warrant a prefetch instruction to avoid the cache miss.</p>
<p>One type of event which can be measured without hardware support is page faults. The OS is responsible for resolving page faults and, on those occasions, it also counts them. It distinguishes two kinds of page faults:</p>
<ul>
<li>
<p><strong>Minor Page Faults</strong></p>
<p>These are page faults for anonymous (i.e.,  not file-backed) pages which have not been used so far, for  copy-on-write pages, and for other pages whose content is already in  memory somewhere.</p>
</li>
<li>
<p><strong>Major Page Faults</strong></p>
<p>Resolving them requires access to disk to  retrieve the file-backed (or swapped-out) data.</p>
</li>
</ul>
<p>Obviously, major page faults are significantly more expensive than minor page faults. But the latter are not cheap either. In either case an entry into the kernel is necessary, a new page must be found, the page must be cleared or populated with the appropriate data, and the page table tree must be modified accordingly.  The last step requires synchronization with other tasks reading or modifying the page table tree, which might introduce further delays.</p>
<p>The easiest way to retrieve information about the page fault counts is to use the time tool.  Note: use the real tool, not the shell builtin.  The output can be seen in Figure 7.4. {<em>The leading backslash prevents the use of the built-in command.</em>}</p>
<blockquote>
<pre><code>$ \time ls /etc
[...]
0.00user 0.00system 0:00.02elapsed 17%CPU (0avgtext+0avgdata 0maxresident)k
0inputs+0outputs (1major+335minor)pagefaults 0swaps
</code></pre>
<p><strong>Figure 7.4: Output of the time utility</strong></p>
</blockquote>
<p>The interesting part here is the last line.  The time tool reports one major and 335 minor page faults. The exact numbers vary; in particular, repeating the run immediately will likely show that there are now no major page faults at all. If the program performs the same action, and nothing changes in the environment, the total page fault count will be stable.</p>
<p>An especially sensitive phase with respect to page faults is program start-up. Each page which is used will produce a page fault; the visible effect (especially for GUI applications) is that the more pages that are used, the longer it takes for the program to start working.  In Section 7.5 we will see a tool to measure this effect specifically.</p>
<p>Under the hood, the time tool uses the <code>rusage</code> functionality.  The <code>wait4</code> system call fills in a <code>struct rusage</code> object when the parent waits for a child to terminate; that is exactly what is needed for the time tool.  But it is also possible for a process to request information about its own resource usage (that is where the name <code>rusage</code> comes from) or the resource usage of its terminated children.</p>
<blockquote>
<pre><code>#include &lt;sys/resource.h&gt;
int getrusage(__rusage_who_t who, struct rusage *usage)
</code></pre>
</blockquote>
<p>The <code>who</code> parameter specifies which process the information is requested for. Currently, <code>RUSAGE_SELF</code> and <code>RUSAGE_CHILDREN</code> are defined.  The resource usage of the child processes is accumulated when each child terminates. It is a total value, not the usage of an individual child process.  Proposals to allow requesting thread-specific information exist, so it is likely that we will see <code>RUSAGE_THREAD</code> in the near future. The <code>rusage</code> structure is defined to contain all kinds of metrics, including execution time, the number of IPC messages sent and memory used, and the number of page faults. The latter information is available in the <code>ru_minflt</code> and <code>ru_majflt</code> members of the structure.</p>
<p>A programmer who tries to determine where her program loses performance due to page faults could regularly request the information and then compare the returned values with the previous results.</p>
<p>From the outside, the information is also visible if the requester has the necessary privileges.  The pseudo file <code>/proc/&lt;PID&gt;/stat</code>, where <code>&lt;PID&gt;</code> is the process ID of the process we are interested in, contains the page fault numbers in the tenth to fourteenth fields. They are pairs of the process's and its childrens' cumulative minor and major page faults, respectively.</p>
<h3 id="72-simulating-cpu-caches"><a class="header" href="#72-simulating-cpu-caches">7.2 Simulating CPU Caches</a></h3>
<p>While the technical description of how a cache works is relatively easy to understand, it is not so easy to see how an actual program behaves with respect to cache. Programmers are not directly concerned with the values of addresses, be they absolute nor relative. Addresses are determined, in part, by the linker and, in part, at runtime by the dynamic linker and the kernel. The generated assembly code is expected to work with all possible addresses and, in the source language, there is not even a hint of absolute address values left. So it can be quite difficult to get a sense for how a program is making use of memory. {<em>When programming close to the hardware this might be  different, but this is of no concern to normal programming and, in any  case, is only possible for special addresses such as memory-mapped  devices.</em>}</p>
<p>CPU-level profiling tools such as oprofile (as described in Section 7.1) can help to understand the cache use.  The resulting data corresponds to the actual hardware, and it can be collected relatively quickly if fine-grained collection is not needed. As soon as more fine-grained data is needed, oprofile is not usable anymore; the thread would have to be interrupted too often.  Furthermore, to see the memory behavior of the program on different processors, one actually has to have such machines and execute the program on them.  This is sometimes (often) not possible.  One example is the data from Figure 3.8.  To collect such data with oprofile one would have to have 24 different machines, many of which do not exist.</p>
<p>The data in that graph was collected using a cache simulator.  This program, cachegrind, uses the valgrind framework, which was initially developed to check for memory handling related problems in a program. The valgrind framework simulates the execution of a program and, while doing this, it allows various extensions, such as cachegrind, to hook into the execution framework.  The cachegrind tool uses this to intercept all uses of memory addresses; it then simulates the operation of L1i, L1d, and L2 caches with a given size, cache line size, and associativity.</p>
<p>To use the tool a program must be run using valgrind as a wrapper:</p>
<blockquote>
<pre><code>valgrind --tool=cachegrind command arg
</code></pre>
</blockquote>
<p>In this simplest form the program <code>command</code> is executed with the parameter <code>arg</code> while simulating the three caches using sizes and associativity corresponding to that of the processor it is running on.  One part of the output is printed to standard error when the program is running; it consists of statistics of the total cache use as can be seen in Figure 7.5.</p>
<blockquote>
<pre><code>==19645== I   refs:      152,653,497
==19645== I1  misses:         25,833
==19645== L2i misses:          2,475
==19645== I1  miss rate:        0.01%
==19645== L2i miss rate:        0.00%
==19645==
==19645== D   refs:       56,857,129  (35,838,721 rd + 21,018,408 wr)
==19645== D1  misses:         14,187  (    12,451 rd +      1,736 wr)
==19645== L2d misses:          7,701  (     6,325 rd +      1,376 wr)
==19645== D1  miss rate:         0.0% (       0.0%   +        0.0%  )
==19645== L2d miss rate:         0.0% (       0.0%   +        0.0%  )
==19645==
==19645== L2 refs:            40,020  (    38,284 rd +      1,736 wr)
==19645== L2 misses:          10,176  (     8,800 rd +      1,376 wr)
==19645== L2 miss rate:          0.0% (       0.0%   +        0.0%  )
</code></pre>
<p><strong>Figure 7.5: Cachegrind Summary Output</strong></p>
</blockquote>
<p>The total number of instructions and memory references is given, along with the number of misses they produce for the L1i/L1d and L2 cache, the miss rates, etc.  The tool is even able to split the L2 accesses into instruction and data accesses, and all data cache uses are split in read and write accesses.</p>
<p>It becomes even more interesting when the details of the simulated caches are changed and the results compared. Through the use of the <code>—I1</code>, <code>—D1</code>, and <code>—L2</code> parameters, cachegrind can be instructed to disregard the processor's cache layout and use that specified on the command line.  For example:</p>
<blockquote>
<pre><code>  valgrind --tool=cachegrind --L2=8388608,8,64 command arg
</code></pre>
</blockquote>
<p>would simulate an 8MB L2 cache with 8-way set associativity and 64 byte cache line size.  Note that the <code>—L2</code> option appears on the command line before the name of the program which is simulated.</p>
<p>This is not all cachegrind can do.  Before the process exits cachegrind writes out a file named <code>cachegrind.out.XXXXX</code> where <code>XXXXX</code> is the PID of the process.  This file contains the summary information and detailed information about the cache use in each function and source file.  The data can be viewed using the cg_annotate program.</p>
<p>The output this program produces contains the cache use summary which was printed when the process terminated, along with a detailed summary of the cache line use in each function of the program. Generating this per-function data requires that cg_annotate is able to match addresses to functions.  This means debug information should be available for best results. Failing that, the ELF symbol tables can help a bit but, since internal symbols are not listed in the dynamic symbol table, the results are not complete. Figure 7.6 shows part of the output for the same program run as Figure 7.5.</p>
<pre><code>--------------------------------------------------------------------------------
        Ir  I1mr I2mr         Dr  D1mr D2mr        Dw D1mw D2mw  file:function
--------------------------------------------------------------------------------
53,684,905     9    8  9,589,531    13    3 5,820,373   14    0  ???:_IO_file_xsputn@@GLIBC_2.2.5
36,925,729 6,267  114 11,205,241    74   18 7,123,370   22    0  ???:vfprintf
11,845,373    22    2  3,126,914    46   22 1,563,457    0    0  ???:__find_specmb
 6,004,482    40   10    697,872 1,744  484         0    0    0  ???:strlen
 5,008,448     3    2  1,450,093   370  118         0    0    0  ???:strcmp
 3,316,589    24    4    757,523     0    0   540,952    0    0  ???:_IO_padn
 2,825,541     3    3    290,222     5    1   216,403    0    0  ???:_itoa_word
 2,628,466     9    6    730,059     0    0   358,215    0    0  ???:_IO_file_overflow@@GLIBC_2.2.5
 2,504,211     4    4    762,151     2    0   598,833    3    0  ???:_IO_do_write@@GLIBC_2.2.5
 2,296,142    32    7    616,490    88    0   321,848    0    0  dwarf_child.c:__libdw_find_attr
 2,184,153 2,876   20    503,805    67    0   435,562    0    0  ???:__dcigettext
 2,014,243     3    3    435,512     1    1   272,195    4    0  ???:_IO_file_write@@GLIBC_2.2.5
 1,988,697 2,804    4    656,112   380    0    47,847    1    1  ???:getenv
 1,973,463    27    6    597,768    15    0   420,805    0    0  dwarf_getattrs.c:dwarf_getattrs
</code></pre>
<p><strong>Figure 7.6: cg_annotate Output</strong></p>
<p>The Ir, Dr, and  Dw columns show the total cache use, not cache misses, which are shown in the following two columns. This data can be used to identify the code which produces the most cache misses. First, one probably would concentrate on L2 cache misses, then proceed to optimizing L1i/L1d cache misses.</p>
<p>cg_annotate can provide the data in more detail. If the name of a source file is given, it also annotates (hence the program's name) each line of the source file with the number of cache hits and misses corresponding to that line. This information allows the programmer to drill down to the exact line where cache misses are a problem.  The program interface is a bit raw: as of this writing, the cachegrind data file and the source file must be in the same directory.</p>
<p>It should, at this point, be noted again: cachegrind is a simulator which does <em>not</em> use measurements from the processor.  The actual cache implementation in the processor might very well be quite different. cachegrind simulates Least Recently Used (LRU) eviction, which is likely to be too expensive for caches with large associativity.  Furthermore, the simulation does not take context switches and system calls into account, both of which can destroy large parts of L2 and must flush L1i and L1d.  This causes the total number of cache misses to be lower than experienced in reality. Nevertheless, cachegrind is a nice tool to learn about a program's memory use and its problems with memory.</p>
<h3 id="73-measuring-memory-usage"><a class="header" href="#73-measuring-memory-usage">7.3 Measuring Memory Usage</a></h3>
<p>Knowing how much memory a program allocates and possibly where the allocation happens is the first step to optimizing its memory use There are, fortunately, some easy-to-use programs available which do not even require that the program be recompiled or specifically modified.</p>
<p>For the first tool, called massif, it is sufficient to not strip the debug information which the compiler can automatically generate. It provides an overview of the accumulated memory use over time.  Figure 7.7 shows an example of the generated output.</p>
<blockquote>
<p><img src="cpu-memory/assets/massif.png" alt="img" /></p>
<p><strong>Figure 7.7: Massif Output</strong></p>
</blockquote>
<p>Like cachegrind (Section 7.2), massif is a tool using the valgrind infrastructure.  It is started using</p>
<pre><code>      valgrind --tool=massif command arg
</code></pre>
<p>where <code>command arg</code> is the program which is observed and its parameter(s), The program will be simulated and all calls to memory allocation functions are recognized. The call site is recorded along with a timestamp value; the new allocation size is added to both the whole-program total and total for the specific call site. The same applies to the functions which free memory where, obviously, the size of the freed block is subtracted from the appropriated sums.  This information can then be used to create a graph showing the memory use over the lifetime of the program, splitting each time value according to the location which requested the allocation.  Before the process is terminated massif creates two files: <code>massif.XXXXX.txt</code> and <code>massif.XXXXX.ps</code>, where <code>XXXXX</code> in both cases is the PID of the process.  The <code>.txt</code> file is a summary of the memory use for all call sites and the <code>.ps</code> is what can be seen in Figure 7.7.</p>
<p>Massif can also record the program's stack usage, which can be useful to determine the total memory footprint of an application.  But this is not always possible. In some situations (some thread stacks or when <code>signaltstack</code> is used) the valgrind runtime cannot know about the limits of the stack . In these situations, it also does not make much sense to add these stacks' sizes to the total. There are several other situations where it makes no sense. If a program is affected by this, massif should be started with the addition option <code>—stacks=no</code>. Note, this is an option for valgrind and therefore must come before the name of the program which is being observed.</p>
<p>Some programs provide their own memory allocation functions or wrapper functions around the system's allocation functions. In the first case, allocations are normally missed; in the second case, the recorded call sites hide information, since only the address of the call in the wrapper function is recorded. For this reason, it is possible to add additional functions to the list of allocation functions.  The <code>—alloc-fn=xmalloc</code> parameter would specify that the function <code>xmalloc</code> is also an allocation function, which is often the case in GNU programs. Calls to <code>xmalloc</code> are recorded, but not the allocation calls made from within <code>xmalloc</code>.</p>
<p>The second tool is called memusage; it is part of the GNU C library.  It is a simplified version of massif (but existed a long time before massif).  It only records the total memory use for heap (including possible calls to <code>mmap</code> etc. if the <code>-m</code> option is given) and, optionally, the stack. The results can be shown as a graph of the total memory use over time or, alternatively, linearly over the calls made to allocation functions.  The graphs are created separately by the memusage script which, just as with valgrind, has to be used to start the application:</p>
<pre><code>     memusage command arg
</code></pre>
<p>The <code>-p IMGFILE</code> option must be used to specify that the graph should be generated in the file <code>IMGFILE</code>, which will be a PNG file.  The code to collect the data is run in the actual program itself, it is not an simulation like valgrind.  This means memusage is much faster than massif and usable in situations where massif would be not useful. Besides total memory consumption, the code also records allocation sizes and, on program termination, it shows a histogram of the used allocation sizes.  This information is written to standard error.</p>
<p>Sometimes it is not possible (or feasible) to call the program which is supposed to be observed directly.  An example is the compiler stage of gcc, which is started by the gcc driver program.  In this case the name of the program which should be observed must be provided to the memusage script using the <code>-n NAME</code> parameter.  This parameter is also useful if the program which is observed starts other programs. If no program name is specified all started programs will be profiled.</p>
<p>Both programs, massif and memusage, have additional options.  A programmer finding herself in the position needing more functionality should first consult the manual or help messages to make sure the additional functionality is not already implemented.</p>
<p>Now that we know how the data about memory allocation can be captured, it is necessary to discuss how this data can be interpreted in the context of memory and cache use.  The main aspects of efficient dynamic memory allocation are linear allocation and compactness of the used portion.  This goes back to making prefetching efficient and reducing cache misses.</p>
<p>A program which has to read in an arbitrary amount of data for later processing could do this by creating a list where each of the list elements contains a new data item.  The overhead for this allocation method might be minimal (one pointer for a single-linked list) but the cache effects when using the data can reduce the performance dramatically.</p>
<p>One problem is, for instance, that there is no guarantee that sequentially allocated memory is laid out sequentially in memory. There are many possible reasons for this:</p>
<ul>
<li>memory blocks inside a large memory chunk administrated by the  memory allocator are actually returned from the back to the front;</li>
<li>a memory chunk is exhausted and a new one is started in a  different part of the address space;</li>
<li>the allocation requests are for different sizes which are served  from different memory pools;</li>
<li>the interleaving of allocations in the  various threads of multi-threaded programs.</li>
</ul>
<p>If data must be allocated up front for later processing, the linked-list approach is clearly a bad idea.  There is no guarantee (or even likelihood) that the consecutive elements in the list are laid out consecutively in memory.  To ensure contiguous allocations, that memory must not be allocated in small chunks.  Another layer of memory handling must be used; it can easily be implemented by the programmer.  An alternative is to use the obstack implementation available in the GNU C library.  This allocator requests large blocks of memory from the system's allocator and then hands arbitrarily large or small blocks of memory out.  These allocations are always sequential unless the large memory chunk is exhausted, which is, depending on the requested allocation sizes, pretty rare. Obstacks are not a complete replacement for a memory allocator, they have limited abilities to free objects. See the GNU C library manual for details.</p>
<p>So, how can a situation where the use of obstacks (or similar techniques) is advisable be recognized from the graphs?  Without consulting the source, possible candidates for the changes cannot be identified, but the graph can provide an entry point for the search.  If many allocations are made from the same location, this could mean that allocation in bulk might help. In Figure 7.7, we can see such a possible candidate in the allocations at address 0x4c0e7d5.  From about 800ms into the run until 1,800ms into the run this is the only area (except the top, green one) which grows.  Moreover, the slope is not steep, which means we have a large number of relatively small allocations. This is, indeed, a candidate for the use of obstacks or similar techniques.</p>
<p>Another problem the graphs can show is when the total number of allocations is high.  This is especially easy to see if the graph is not drawn linearly over time but, instead, linearly over the number of calls (the default with memusage). In that case, a gentle slope in the graph means a lot of small allocations.  memusage will not say where the allocations took place, but the comparison with massif's output can say that, or the programmer might recognize it right away. Many small allocations should be consolidated to achieve linear memory use.</p>
<p>But there is another, equally important, aspect to this latter class of cases: many allocations also means higher overhead in administrative data.  This by itself might not be that problematic.  The red area named “heap-admin” represents this overhead in the massif graph and it is quite small.  But, depending on the <code>malloc</code> implementation, this administrative data is allocated along with the data blocks, in the same memory.  For the current <code>malloc</code> implementation in the GNU C library, this is the case: every allocated block has at least a 2-word header (8 bytes for 32-bit platforms, 16 bytes for 64-bit platforms). In addition, block sizes are often a bit larger than necessary due to the way memory is administrated (rounding up block sizes to specific multiples).</p>
<p>This all means that memory used by the program is interspersed with memory only used by the allocator for administrative purposes. We might see something like this:</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.39.png" alt="img" /></p>
</blockquote>
<p>Each block represents one memory word and, in this small region of memory, we have four allocated blocks. The overhead due to the block header and padding is 50%. Due to the placement of the header, this automatically means that the effective prefetch rate of the processor is lowered by up to 50% as well. If the blocks were be processed sequentially (to take maximum advantage of prefetching), the processor would read all the header and padding words into the cache, even though they are never supposed to be read from or written to by the application itself.  Only the runtime uses the header words, and the runtime only comes into play when the block is freed.</p>
<p>Now, one could argue that the implementation should be changed to put the administrative data somewhere else.  This is indeed done in some implementations, and it might prove to be a good idea.  There are many aspects to be kept in mind, though, security not being the least of them. Regardless of whether we might see a change in the future, the padding issue will never go away (amounting to 16% of the data in the example, when ignoring the headers).  Only if the programmer directly takes control of allocations can this be avoided. When alignment requirements come into play there might still be holes, but this is also something under control of the programmer.</p>
<h3 id="74-improving-branch-prediction"><a class="header" href="#74-improving-branch-prediction">7.4 Improving Branch Prediction</a></h3>
<p>In Section 6.2.2, two methods to improve L1i use through branch prediction and block reordering were mentioned: static prediction through <code>__builtin_expect</code> and profile guided optimization (PGO). Correct branch prediction has performance impacts, but here we are interested in the memory usage improvements.</p>
<p>The use of <code>__builtin_expect</code> (or better the <code>likely</code> and <code>unlikely</code> macros) is simple.  The definitions are placed in a central header and the compiler takes care of the rest.  There is a little problem, though: it is easy enough for a programmer to use <code>likely</code> when really <code>unlikely</code> was meant and vice versa. Even if somebody uses a tool like oprofile to measure incorrect branch predictions and L1i misses these problems are hard to detect.</p>
<p>There is one easy method, though.  The code in Section 9.2 shows an alternative definition of the <code>likely</code> and <code>unlikely</code> macros which measure actively, at runtime, whether the static predictions are correct or not.  The results can then be examined by the programmer or tester and adjustments can be made.  The measurements do not actually take the performance of the program into account, they simply test the static assumptions made by the programmer. More details can be found, along with the code, in the section referenced above.</p>
<p>PGO is quite easy to use with gcc these days.  It is a three-step process, though, and certain requirements must be fulfilled.  First, all source files must be compiled with the additional <code>-fprofile-generate</code> option.  This option must be passed to all compiler runs and to the command which links the program.   Mixing object files compiled with and without this option is possible, but PGO will not do any good for those that do not have it enabled.</p>
<p>The compiler generates a binary which behaves normally except that it is significantly larger and slower since it records (and emits) all kinds of information about branches taken or not.  The compiler also emits a file with the extension <code>.gcno</code> for each input file.  This file contains information related to the branches in the code.  It must be preserved for later.</p>
<p>Once the program binary is available, it should be used to run a representative set of workloads.  Whatever workload is used, the final binary will be optimized to do this task well.  Consecutive runs of the program are possible and, in general necessary; all the runs will contribute to the same output file. Before the program terminates, the data collected during the program run is written out into files with the extension <code>.gcda</code>.  These files are created in the directory which contains the source file. The program can be executed from any directory, and the binary can be copied, but the directory with the sources must be available and writable.  Again, one output file is created for each input source file.  If the program is run multiple times, it is important that the <code>.gcda</code> files of the previous run are found in the source directories since otherwise the data of the runs cannot be accumulated in one file.</p>
<p>When a representative set of tests has been run, it is time to recompile the application.  The compiler has to be able to find the <code>.gcda</code> files in the same directory which holds the source files. The files cannot be moved since the compiler would not find them and the embedded checksum for the files would not match anymore. For the recompilation, replace the <code>-fprofile-generate</code> parameter with <code>-fprofile-use</code>.  It is essential that the sources do not change in any way that would change the generated code. That means: it is OK to change white spaces and edit comments, but adding more branches or basic blocks invalidates the collected data and the compilation will fail.</p>
<p>This is all the programmer has to do; it is a fairly simple process. The most important thing to get right is the selection of representative tests to perform the measurements.  If the test workload does not match the way the program is actually used, the performed optimizations might actually do more harm than good.  For this reason, is it often hard to use PGO for libraries. Libraries can be used in many—sometimes widely different—scenarios. Unless the use cases are indeed similar, it is usually better to rely exclusively on static branch prediction using <code>__builtin_expect</code>.</p>
<p>A few words on the <code>.gcno</code> and <code>.gcda</code> files.  These are binary files which are not immediately usable for inspection.  It is possible, though, to use the gcov tool, which is also part of the gcc package, to examine them. This tool is mainly used for coverage analysis (hence the name) but the file format used is the same as for PGO.  The gcov tool generates output files with the extension <code>.gcov</code> for each source file with executed code (this might include system headers).  The files are source listings which are annotated, according to the parameters given to gcov, with branch counter, probabilities, etc.</p>
<h3 id="75-page-fault-optimization"><a class="header" href="#75-page-fault-optimization">7.5 Page Fault Optimization</a></h3>
<p>On demand-paged operating systems like Linux, an <code>mmap</code> call only modifies the page tables. It makes sure that, for file-backed pages, the underlying data can be found and, for anonymous memory, that, on access, pages initialized with zeros are provided. No actual memory is allocated at the time of the <code>mmap</code> call. {<em>If you want to say “Wrong!”  wait a second, it will be qualified later that there are  exceptions.</em>}</p>
<p>The allocation part happens when a memory page is first accessed, either by reading or writing data, or by executing code. In response to the ensuing page fault, the kernel takes control and determines, using the page table tree, the data which has to be present on the page.  This resolution of the page fault is not cheap, but it happens for every single page which is used by a process.</p>
<p>To minimize the cost of page faults, the total number of used pages has to be reduced. Optimizing the code for size will help with this.  To reduce the cost of a specific code path (for instance, the start-up code), it is also possible to rearrange code so that, in that code path, the number of touched pages is minimized. It is not easy to determine the right order, though.</p>
<p>The author wrote a tool, based on the valgrind toolset, to measure page faults as they happen.  Not the number of page faults, but the reason why they happen.  The <a href="http://people.redhat.com/drepper/pagein.html">pagein</a> tool emits information about the order and timing of page faults. The output, written to a file named <code>pagein.&lt;PID&gt;</code>, looks as in Figure 7.8. </p>
<blockquote>
<pre><code>   0 0x3000000000 C            0 0x3000000B50: (within /lib64/ld-2.5.so)
   1 0x 7FF000000 D         3320 0x3000000B53: (within /lib64/ld-2.5.so)
   2 0x3000001000 C        58270 0x3000001080: _dl_start (in /lib64/ld-2.5.so)
   3 0x3000219000 D       128020 0x30000010AE: _dl_start (in /lib64/ld-2.5.so)
   4 0x300021A000 D       132170 0x30000010B5: _dl_start (in /lib64/ld-2.5.so)
   5 0x3000008000 C     10489930 0x3000008B20: _dl_setup_hash (in /lib64/ld-2.5.so)
   6 0x3000012000 C     13880830 0x3000012CC0: _dl_sysdep_start (in /lib64/ld-2.5.so)
   7 0x3000013000 C     18091130 0x3000013440: brk (in /lib64/ld-2.5.so)
   8 0x3000014000 C     19123850 0x3000014020: strlen (in /lib64/ld-2.5.so)
   9 0x3000002000 C     23772480 0x3000002450: dl_main (in /lib64/ld-2.5.so)
</code></pre>
<p><strong>Figure 7.8: Output of the pagein Tool</strong></p>
</blockquote>
<p>The second column specifies the address of the page which is paged-in. Whether it is a code or data page is indicated in the third column, which contains <code>C' or </code>D' respectively.  The fourth column specifies the number of cycles which passed since the first page fault.  The rest of the line is valgrind's attempt to find a name for the address which caused the page fault. The address value itself is correct but the name is not always accurate if no debug information is available.</p>
<p>In the example in Figure 7.8, execution starts at address 0x3000000B50, which forces the page at address 0x3000000000 to be paged in. Shortly after that, the page after this is also brought in; the function called on that page is <code>_dl_start</code>. The initial code accesses a variable on page 0x7FF000000.  This happens just 3,320 cycles after the first page fault and is most likely the second instruction of the program (just three bytes after the first instruction).  If one looks at the program, one will notice that there is something peculiar about this memory access.  The instruction in question is a <code>call</code> instruction, which does not explicitly load or store data.  It does store the return address on the stack, though, and this is exactly what happens here.  This is not the official stack of the process, though, it is valgrind's internal stack of the application.  This means when interpreting the results of pagein it is important to keep in mind that valgrind introduces some artifacts.</p>
<p>The output of pagein can be used to determine which code sequences should ideally be adjacent in the program code.  A quick look at the <code>/lib64/ld-2.5.so</code> code shows that the first instructions immediately call the function <code>_dl_start</code>, and that these two places are on different pages.  Rearranging the code to move the code sequences onto the same page can avoid—or at least delay—a page fault. It is, so far, a cumbersome process to determine what the optimal code layout should be. Since the second use of a page is, by design, not recorded, one needs to use trial and error to see the effects of a change. Using call graph analysis, it is possible to guess about possible call sequences; this might help speed up the process of sorting the functions and variables.</p>
<p>At a very coarse level, the call sequences can be seen by looking a the object files making up the executable or DSO. Starting with one or more entry points (i.e., function names), the chain of dependencies can be computed.  Without much effort this works well at the object file level.  In each round, determine which object files contain needed functions and variables.  The seed set has to be specified explicitly.  Then determine all undefined references in those object files and add them to the set of needed symbols.  Repeat until the set is stable.</p>
<p>The second step in the process is to determine an order.  The various object files have to be grouped together to fill as few pages as possible. As an added bonus, no function should cross over a page boundary. A complication in all this is that, to best arrange the object files, it has to be known what the linker will do later. The important fact here is that the linker will put the object files into the executable or DSO in the same order in which they appear in the input files (e.g., archives), and on the command line. This gives the programmer sufficient control.</p>
<p>For those who are willing to invest a bit more time, there have been successful attempts at reordering made using automatic call tracing via the <code>__cyg_profile_func_enter</code> and <code>__cyg_profile_func_exit</code> hooks gcc inserts when called with the <code>-finstrument-functions</code> option [oooreorder].  See the gcc manual for more information on these <code>__cyg_*</code> interfaces. By creating a trace of the program execution, the programmer can more accurately determine the call chains. The results in [oooreorder] are a 5% decrease in start-up costs, just through reordering of the functions.  The main benefit is the reduced number of page faults, but the TLB cache also plays a role—an increasingly important role given that, in virtualized environments, TLB misses become significantly more expensive.</p>
<p>By combining the analysis of the pagein tool with the call sequence information, it should be possible to optimize certain phases of the program (such as start-up) to minimize the number of page faults.</p>
<p>The Linux kernel provides two additional mechanisms to avoid page faults.  The first one is a flag for <code>mmap</code> which instructs the kernel to not only modify the page table but, in fact, to pre-fault all the pages in the mapped area. This is achieved by simply adding the <code>MAP_POPULATE</code> flag to the fourth parameter of the <code>mmap</code> call.  This will cause the <code>mmap</code> call to be significantly more expensive, but, if all pages which are mapped by the call are being used right away, the benefits can be large. Instead of having a number of page faults, which each are pretty expensive due to the overhead incurred by synchronization requirements etc., the program would have one, more expensive, <code>mmap</code> call.  The use of this flag has disadvantages, though, in cases where a large portion of the mapped pages are not used soon (or ever) after the call.  Mapped, unused pages are obviously a waste of time and memory.  Pages which are immediately pre-faulted and only much later used also can clog up the system.  The memory is allocated before it is used and this might lead to shortages of memory in the meantime.  On the other hand, in the worst case, the page is simply reused for a new purpose (since it has not been modified yet), which is not that expensive but still, together with the allocation, adds some cost.</p>
<p>The granularity of <code>MAP_POPULATE</code> is simply too coarse.  And there is a second possible problem: this is an optimization; it is not critical that all pages are, indeed, mapped in. If the system is too busy to perform the operation the pre-faulting can be dropped.  Once the page is really used the program takes the page fault, but this is not worse than artificially creating resource scarcity. An alternative is to use the <code>POSIX_MADV_WILLNEED</code> advice with the <code>posix_madvise</code> function. This is a hint to the operating system that, in the near future, the program will need the page described in the call. The kernel is free to ignore the advice, but it also can pre-fault pages. The advantage here is that the granularity is finer.  Individual pages or page ranges in any mapped address space area can be pre-faulted. For memory-mapped files which contain a  lot of data which is not used at runtime, this can have huge advantages over using <code>MAP_POPULATE</code>.</p>
<p>Beside these active approaches to minimizing the number of page faults, it is also possible to take a more passive approach which is popular with the hardware designers.  A DSO occupies neighboring pages in the address space, one range of pages each for the code and the data.  The smaller the page size, the more pages are needed to hold the DSO. This, in turn, means more page faults, too. Important here is that the opposite is also true. For larger page sizes, the number of necessary pages for the mapping (or anonymous memory) is reduced; with it falls the number of page faults.</p>
<p>Most architectures support page sizes of 4k.  On IA-64 and PPC64, page sizes of 64k are also popular.  That means the smallest unit in which memory is given out is 64k.  The value has to be specified when compiling the kernel and cannot be changed dynamically (at least not at the moment).  The ABIs of the multiple-page-size architectures are designed to allow running an application with either page size.  The runtime will make the necessary adjustments, and a correctly-written program will not notice a thing.  Larger page sizes mean more waste through partially-used pages, but, in some situations, this is OK.</p>
<p>Most architectures also support very large page sizes of 1MB or more. Such pages are useful in some situations, too, but it makes no sense to have all memory given out in units that large.  The waste of physical RAM would simply be too large. But very large pages have their advantages: if huge data sets are used, storing them in 2MB pages on x86-64 would require 511 fewer page faults (per large page) than using the same amount of memory with 4k pages.  This can make a big difference.  The solution is to selectively request memory allocation which, just for the requested address range, uses huge memory pages and, for all the other mappings in the same process, uses the normal page size.</p>
<p>Huge page sizes come with a price, though.  Since the physical memory used for large pages must be continuous, it might, after a while, not be possible to allocate such pages due to memory fragmentation. prevent this.  People are working on memory defragmentation and fragmentation avoidance, but it is very complicated.  For large pages of, say, 2MB the necessary 512 consecutive pages are always hard to come by, except at one time: when the system boots up.  This is why the current solution for large pages requires the use of a special filesystem, <code>hugetlbfs</code>.  This pseudo filesystem is allocated on request by the system administrator by writing the number of huge pages which should be reserved to</p>
<pre><code>    /proc/sys/vm/nr_hugepages
</code></pre>
<p>the number of huge pages which should be reserved.  This operation might fail if not enough continuous memory can be located.  The situation gets especially interesting if virtualization is used.  A system virtualized using the VMM model does not directly access physical memory and, therefore, cannot by itself allocate the <code>hugetlbfs</code>. It has to rely on the VMM, and this feature is not guaranteed to be supported. For the KVM model, the Linux kernel running the KVM module can perform the <code>hugetlbfs</code> allocation and possibly pass a subset of the pages thus allocated on to one of the guest domains.</p>
<p>Later, when a program needs a large page, there are multiple possibilities:</p>
<ul>
<li>
<p>the program can use System V shared memory with the  SHM_HUGETLBflag.</p>
</li>
<li>
<p>the hugetlbfsfilesystem can actually be mounted and the  program can then create a file under the mount point and use  mmap to map one or more pages as anonymous memory.</p>
</li>
</ul>
<p>In the first case, the <code>hugetlbfs</code> need not be mounted.  Code requesting one or more large pages could look like this:</p>
<blockquote>
<pre><code>key_t k = ftok(&quot;/some/key/file&quot;, 42);
int id = shmget(k, LENGTH, SHM_HUGETLB|IPC_CREAT|SHM_R|SHM_W);
void *a = shmat(id, NULL, 0);
</code></pre>
</blockquote>
<p>The critical parts of this code sequence are the use of the <code>SHM_HUGETLB</code> flag and the choice of the right value for <code>LENGTH</code>, which must be a multiple of the huge page size for the system.  Different architectures have different values.  The use of the System V shared memory interface has the nasty problem of depending on the key argument to differentiate (or share) mappings.  The <code>ftok</code> interface can easily produce conflicts which is why, if possible, it is better to use other mechanisms.</p>
<p>If the requirement to mount the <code>hugetlbfs</code> filesystem is not a problem, it is better to use it instead of System V shared memory.  The only real problems with using the special filesystem are that the kernel must support it, and that there is no standardized mount point yet. Once the filesystem is mounted, for instance at <code>/dev/hugetlb</code>, a program can make easy use of it:</p>
<blockquote>
<pre><code>int fd = open(&quot;/dev/hugetlb/file1&quot;, O_RDWR|O_CREAT, 0700);
void *a = mmap(NULL, LENGTH, PROT_READ|PROT_WRITE, fd, 0);
</code></pre>
</blockquote>
<p>By using the same file name in the <code>open</code> call, multiple processes can share the same huge pages and collaborate.  It is also possible to make the pages executable, in which case the <code>PROT_EXEC</code> flag must also be set in the <code>mmap</code> call.  As in the System V shared memory example, the value of <code>LENGTH</code> must be a multiple of the system's huge page size.</p>
<p>A defensively-written program (as all programs should be) can determine the mount point at runtime using a function like this:</p>
<blockquote>
<pre><code>char *hugetlbfs_mntpoint(void) {
  char *result = NULL;
  FILE *fp = setmntent(_PATH_MOUNTED, &quot;r&quot;);
  if (fp != NULL) {
    struct mntent *m;
    while ((m = getmntent(fp)) != NULL)
       if (strcmp(m-&gt;mnt_fsname, &quot;hugetlbfs&quot;) == 0) {
         result = strdup(m-&gt;mnt_dir);
         break;
       }
    endmntent(fp);
  }
  return result;
}
</code></pre>
</blockquote>
<p>More information for both these cases can be found in the hugetlbpage.txt file which comes as part of the kernel source tree. The file also describes the special handling needed for IA-64.</p>
<blockquote>
<p><img src="cpu-memory/assets/cpumemory.68.png" alt="img" /></p>
<p><strong>Figure 7.9: Follow with Huge Pages, NPAD=0</strong></p>
</blockquote>
<p>To illustrate the advantages of huge pages, Figure 7.9 shows the results of running the random Follow test for <code>NPAD</code>=0.  This is the same data shown in Figure 3.15, but, this time, we measure the data also with memory allocated in huge pages.  As can be seen the performance advantage can be huge.  For 220 bytes the test using huge pages is 57% faster.  This is due to the fact that this size still fits completely into one single 2MB page and, therefore, no DTLB misses occur.</p>
<p>After this point, the winnings are initially smaller but grow again with increasing working set size.  The huge pages test is 38% faster for the 512MB working set size.  The curve for the huge page test has a plateau at around 250 cycles.  Beyond working sets of 227 bytes, the numbers rise significantly again.  The reason for the plateau is that 64 TLB entries for 2MB pages cover 227 bytes.</p>
<p>As these numbers show, a large part of the costs of using large working set sizes comes from TLB misses.  Using the interfaces described in this section can pay off big-time. The numbers in the graph are, most likely, upper limits, but even real-world programs show a significant speed-up.  Databases, since they use large amounts of data, are among the programs which use huge pages today.</p>
<p>There is currently no way to use large pages to map file-backed data. There is interest in implementing this capability, but the proposals made so far all involve explicitly using large pages, and they rely on the <code>hugetlbfs</code> filesystem. This is not acceptable: large page use in this case must be transparent.  The kernel can easily determine which mappings are large and automatically use large pages.  A big problem is that the kernel does not always know about the use pattern.  If the memory, which could be mapped as a large page, later requires 4k-page granularity (for instance, because the protection of parts of the memory range is changed using <code>mprotect</code>) a lot of precious resources, in particular the linear physical memory, will have been wasted. So it will certainly be some more time before such an approach is successfully implemented.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="8-upcoming-technology"><a class="header" href="#8-upcoming-technology">8 Upcoming Technology</a></h2>
<p>In the preceding sections about multi-processor handling we have seen that significant performance problems must be expected if the number of CPUs or cores is scaled up. But this scaling-up is exactly what has to be expected in the future. Processors will get more and more cores, and programs must be ever more parallel to take advantage of the increased potential of the CPU, since single-core performance will not rise as quickly as it used to.</p>
<h3 id="81-the-problem-with-atomic-operations"><a class="header" href="#81-the-problem-with-atomic-operations">8.1 The Problem with Atomic Operations</a></h3>
<p>Synchronizing access to shared data structures is traditionally done in two ways:</p>
<ul>
<li>
<p>through mutual exclusion, usually by using functionality of the  system runtime to achieve just that;</p>
</li>
<li>
<p>by using lock-free data structures.</p>
</li>
</ul>
<p>The problem with lock-free data structures is that the processor has to provide primitives which can perform the entire operation atomically.  This support is limited.  On most architectures support is limited to atomically read and write a word.  There are two basic ways to implement this (see Section 6.4.2):</p>
<ul>
<li>
<p>using atomic compare-and-exchange (CAS) operations;</p>
</li>
<li>
<p>using a load lock/store conditional (LL/SC) pair.</p>
</li>
</ul>
<p>It can be easily seen how a CAS operation can be implemented using LL/SC instructions.  This makes CAS operations the building block for most atomic operations and lock free data structures.</p>
<p>Some processors, notably the x86 and x86-64 architectures, provide a far more elaborate set of atomic operations.  Many of them are optimizations of the CAS operation for specific purposes.  For instance, atomically adding a value to a memory location can be implemented using CAS and LL/SC operations, but the native support for atomic increments on x86/x86-64 processors is faster. It is important for programmers to know about these operations, and the intrinsics which make them available when programming, but that is nothing new.</p>
<p>The extraordinary extension of these two architectures is that they have double-word CAS (DCAS) operations.  This is significant for some applications but not all (see [dcas]).  As an example of how DCAS can be used, let us try to write a lock-free array-based stack/LIFO data structure.  A first attempt using gcc's intrinsics can be seen in Figure 8.1.</p>
<blockquote>
<pre><code>  struct elem {
    data_t d;
    struct elem *c;
  };
  struct elem *top;
  void push(struct elem *n) {
    n-&gt;c = top;
    top = n;
  }
  struct elem *pop(void) {
    struct elem *res = top;
    if (res != NULL)
      top = res-&gt;c;
    return res;
  }
</code></pre>
<p><strong>Figure 8.1: Not Thread-Safe LIFO</strong></p>
</blockquote>
<p>This code is clearly not thread-safe.  Concurrent accesses in different threads will modify the global variable <code>top</code> without consideration of other thread's modifications.  Elements could be lost or removed elements can magically reappear.  It is possible to use mutual exclusion but here we will try to use only atomic operations.</p>
<p>The first attempt to fix the problem uses CAS operations when installing or removing list elements.  The resulting code looks like Figure 8.2.</p>
<blockquote>
<pre><code>  #define CAS __sync_bool_compare_and_swap
  struct elem {
    data_t d;
    struct elem *c;
  };
  struct elem *top;
  void push(struct elem *n) {
    do
      n-&gt;c = top;
    while (!CAS(&amp;top, n-&gt;c, n));
  }
  struct elem *pop(void) {
    struct elem *res;
    while ((res = top) != NULL)
      if (CAS(&amp;top, res, res-&gt;c))
        break;
    return res;
  }
</code></pre>
<p><strong>Figure 8.2: LIFO using CAS</strong></p>
</blockquote>
<p>At first glance this looks like a working solution.  <code>top</code> is never modified unless it matches the element which was at the top of the LIFO when the operation started.  But we have to take concurrency at all levels into account.  It might be that another thread working on the data structure is scheduled at the worst possible moment.  One such case here is the so-called ABA problem.  Consider what happens if a second thread is scheduled right before the CAS operation in <code>pop</code> and it performs the following operation:</p>
<ol>
<li><code>l = pop()</code></li>
<li><code>push(newelem)</code></li>
<li><code>push(l)</code></li>
</ol>
<p>The end effect of this operation is that the former top element of the LIFO is back at the top but the second element is different.  Back in the first thread, because the top element is unchanged, the CAS operation will succeed.  But the value <code>res-&gt;c</code> is not the right one.  It is a pointer to the second element of the original LIFO and not <code>newelem</code>.  The result is that this new element is lost.</p>
<p>In the literature [lockfree] you find suggestions to use a feature found on some processors to work around this problem.  Specifically, this is about the ability of the x86 and x86-64 processors to perform DCAS operations.  This is used in the third incarnation of the code in Figure 8.3.</p>
<blockquote>
<pre><code>  #define CAS __sync_bool_compare_and_swap
  struct elem {
    data_t d;
    struct elem *c;
  };
  struct lifo {
    struct elem *top;
    size_t gen;
  } l;
  void push(struct elem *n) {
    struct lifo old, new;
    do {
      old = l;
      new.top = n-&gt;c = old.top;
      new.gen = old.gen + 1;
    } while (!CAS(&amp;l, old, new));
  }
  struct elem *pop(void) {
    struct lifo old, new;
    do {
      old = l;
      if (old.top == NULL) return NULL;
      new.top = old.top-&gt;c;
      new.gen = old.gen + 1;
    } while (!CAS(&amp;l, old, new));
    return old.top;
  }
</code></pre>
<p><strong>Figure 8.3: LIFO using double-word CAS</strong></p>
</blockquote>
<p>Unlike the other two examples, this is (currently) pseudo-code since gcc does not grok the use of structures in the CAS intrinsics. Regardless, the example should be sufficient understand the approach. A generation counter is added to the pointer to the top of the LIFO. Since it is changed on every operation, <code>push</code> or <code>pop</code>, the ABA problem described above is no longer a problem.  By the time the first thread is resuming its work by actually exchanging the <code>top</code> pointer, the generation counter has been incremented three times. The CAS operation will fail and, in the next round of the loop, the correct first and second element of the LIFO are determined and the LIFO is not corrupted.  Voilà.</p>
<p>Is this really the solution?  The authors of [lockfree] certainly make it sound like it and, to their credit, it should be mentioned that it is possible to construct data structures for the LIFO which would permit using the code above.  But, in general, this approach is just as doomed as the previous one.  We still have concurrency problems, just now in a different place.  Let us assume a thread executes <code>pop</code> and is interrupted after the test for <code>old.top == NULL</code>. Now a second thread uses <code>pop</code> and receives ownership of the previous first element of the LIFO.  It can do anything with it, including changing all values or, in case of dynamically allocated elements, freeing the memory.</p>
<p>Now the first thread resumes.  The <code>old</code> variable is still filled with the previous top of the LIFO.  More specifically, the <code>top</code> member points to the element popped by the second thread.  In <code>new.top = old.top-&gt;c</code> the first thread dereferences a pointer in the element.  But the element this pointer references might have been freed. That part of the address space might be inaccessible and the process could crash. This cannot be allowed for a generic data type implementation.  Any fix for this problem is terribly expensive: memory must never be freed, or at least it must be verified that no thread is referencing the memory anymore before it is freed.  Given that lock-free data structures are supposed to be faster and more concurrent, these additional requirements completely destroy any advantage. In languages which support it, memory handling through garbage collection can solve the problem, but this comes with its price.</p>
<p>The situation is often worse for more complex data structures.  The same paper cited above also describes a FIFO implementation (with refinements in a successor paper).  But this code has all the same problems.  Because CAS operations on existing hardware (x86, x86-64) are limited to modifying two words which are consecutive in memory, they are no help at all in other common situations. For instance, atomically adding or removing elements anywhere in a double-linked list is not possible.  {<em>As a side note, the developers of the IA-64 did</em> not <em>include this feature. They allow</em> comparing <em>two words, but replacing only one.</em>}</p>
<p>The problem is that more than one memory address is generally involved, and only if none of the values of these addresses is changed concurrently can the entire operation succeed.  This is a well-known concept in database handling, and this is exactly where one of the most promising proposals to solve the dilemma comes from.</p>
<h3 id="82-transactional-memory"><a class="header" href="#82-transactional-memory">8.2 Transactional Memory</a></h3>
<p>In their groundbreaking 1993 paper [transactmem] Herlihy and Moss propose to implement transactions for memory operations in hardware since software alone cannot deal with the problem efficiently. Digital Equipment Corporation, at that time, was already battling with scalability problems on their high-end hardware, which featured a few dozen processors.  The principle is the same as for database transactions: the result of a transaction becomes visible all at once or the transaction is aborted and all the values remain unchanged.</p>
<p>This is where memory comes into play and why the previous section bothered to develop algorithms which use atomic operations. Transactional memory is meant as a replacement for—and extension of—atomic operations in many situations, especially for lock-free data structures.  Integrating a transaction system into the processor sounds like a terribly complicated thing to do but, in fact, most processors, to some extent, already have something similar.</p>
<p>The LL/SC operations implemented by some processors form a transaction. The SC instruction aborts or commits the transaction based on whether the memory location was touched or not.  Transactional memory is an extension of this concept.  Now, instead of a simple pair of instructions, multiple instructions take part in the transaction.  To understand how this can work, it is worthwhile to first see how LL/SC instructions can be implemented. {<em>This does not mean it is actually implemented  like this.</em>}</p>
<p><strong>8.2.1 Load Lock/Store Conditional Implementation</strong></p>
<p>If the LL instruction is issued, the value of the memory location is loaded into a register. As part of that operation, the value is loaded into L1d. The SC instruction later can only succeed if this value has not been tampered with.  How can the processor detect this?  Looking back at the description of the MESI protocol in Figure 3.18 should make the answer obvious. If another processor changes the value of the memory location, the copy of the value in L1d of the first processor must be revoked.  When the SC instruction is executed on the first processor, it will find it has to load the value again into L1d.  This is something the processor must already detect.</p>
<p>There are a few more details to iron out with respect to context switches (possible modification on the same processor) and accidental reloading of the cache line after a write on another processor.  This is nothing that policies (cache flush on context switch) and extra flags, or separate cache lines for LL/SC instructions, cannot fix. In general, the LL/SC implementation comes almost for free with the implementation of a cache coherence protocol like MESI.</p>
<p><strong>8.2.2 Transactional Memory Operations</strong></p>
<p>For transactional memory to be generally useful, a transaction must not be finished with the first store instruction.  Instead, an implementation should allow a certain number of load and store operations; this means we need separate commit and abort instructions.  In a bit we will see that we need one more instruction which allows checking on the current state of the transaction and whether it is already aborted or not.</p>
<p>There are three different memory operations to implement:</p>
<ul>
<li>Read memory</li>
<li>Read memory which is written to later</li>
<li>Write memory</li>
</ul>
<p>When looking at the MESI protocol it should be clear how this special second type of read operation can be useful.  The normal read can be satisfied by a cache line in the <code>E' and </code>S' state.  The second type of read operation needs a cache line in state `E'.  Exactly why the second type of memory read is necessary can be glimpsed from the following discussion, but, for a more complete description, the interested reader is referred to literature about transactional memory, starting with [transactmem].</p>
<p>In addition, we need transaction handling which mainly consists of the commit and abort operation we are already familiar with from database transaction handling.  There is one more operation, though, which is optional in theory but required for writing robust programs using transactional memory.  This instruction lets a thread test whether the transaction is still on track and can (perhaps) be committed later, or whether the transaction already failed and will in any case be aborted.</p>
<p>We will discuss how these operations actually interact with the CPU cache and how they match to bus operation.  But before we do that we take a look at some actual code which uses transactional memory.  This will hopefully make the remainder of this section easier to understand.</p>
<p><strong>8.2.3 Example Code Using Transactional Memory</strong></p>
<p>For the example we revisit our running example and show a LIFO implementation which uses transactional memory.</p>
<blockquote>
<pre><code>  struct elem {
    data_t d;
    struct elem *c;
  };
  struct elem *top;
  void push(struct elem *n) {
    while (1) {
      n-&gt;c = LTX(top);
      ST(&amp;top, n);
      if (COMMIT())
        return;
      ... delay ...
    }
  }
  struct elem *pop(void) {
    while (1) {
      struct elem *res = LTX(top);
      if (VALIDATE()) {
        if (res != NULL)
          ST(&amp;top, res-&gt;c);
        if (COMMIT())
          return res;
      }
      ... delay ...
    }
  }
</code></pre>
<p><strong>Figure 8.4: LIFO Using Transactional Memory</strong></p>
</blockquote>
<p>This code looks quite similar to the not-thread-safe code, which is an additional bonus as it makes writing code using transactional memory easier.  The new parts of the code are the <code>LTX</code>, <code>ST</code>, <code>COMMIT</code>, and <code>VALIDATE</code> operations.  These four operations are the way to request accesses to transactional memory.  There is actually one more operation, <code>LT</code>, which is not used here. <code>LT</code> requests non-exclusive read access, <code>LTX</code> requests exclusive read access, and <code>ST</code> is a store into transactional memory. The <code>VALIDATE</code> operation is the operation which checks whether the transaction is still on track to be committed.  It returns true if this transaction is still OK.  If the transaction is already marked as aborting, it will be actually aborted and the next transactional memory instruction will start a new transaction. For this reason, the code uses a new <code>if</code> block in case the transaction is still going on.</p>
<p>The <code>COMMIT</code> operation finishes the transaction; if the transaction is finished successfully the operation returns true.  This means that this part of the program is done and the thread can move on. If the operation returns a false value, this usually means the whole code sequence must be repeated. This is what the outer <code>while</code> loop is doing here.  This is not absolutely necessary, though, in some cases giving up on the work is the right thing to do.</p>
<p>The interesting point about the <code>LT</code>, <code>LTX</code>, and <code>ST</code> operations is that they can fail without signaling this failure in any direct way. The way the program can request this information is through the <code>VALIDATE</code> or <code>COMMIT</code> operation. For the load operation, this can mean that the value actually loaded into the register might be bogus; that is why it is necessary in the example above to use <code>VALIDATE</code> before dereferencing the pointer. In the next section, we will see why this is a wise choice for an implementation.  It might be that, once transactional memory is actually widely available, the processors will implement something different. The results from [transactmem] suggest what we describe here, though.</p>
<p>The <code>push</code> function can be summarized as this: the transaction is started by reading the pointer to the head of the list.  The read requests exclusive ownership since, later in the function, this variable is written to. If another thread has already started a transaction, the load will fail and mark the still-born transaction as aborted; in this case, the value actually loaded might be garbage. This value is, regardless of its status, stored in the <code>next</code> field of the new list member. This is fine since this member is not yet in use, and it is accessed by exactly one thread.  The pointer to the head of the list is then assigned the pointer to the new element. If the transaction is still OK, this write can succeed. This is the normal case, it can only fail if a thread uses some code other than the provided <code>push</code> and <code>pop</code> functions to access this pointer. If the transaction is already aborted at the time the <code>ST</code> is executed, nothing at all is done. Finally, the thread tries to commit the transaction. If this succeeds the work is done; other threads can now start their transactions. If the transaction fails, it must be repeated from the beginning. Before doing that, however, it is best to insert an delay. If this is not done the thread might run in a busy loop (wasting energy, overheating the CPU).</p>
<p>The <code>pop</code> function is slightly more complex.  It also starts with reading the variable containing the head of the list, requesting exclusive ownership.  The code then immediately checks whether the <code>LTX</code> operation succeeded or not.  If not, nothing else is done in this round except delaying the next round.  If the <code>top</code> pointer was read successfully, this means its state is good; we can now dereference the pointer.  Remember, this was exactly the problem with the code using atomic operations; with transactional memory this case can be handled without any problem.  The following <code>ST</code> operation is only performed when the LIFO is not empty, just as in the original, thread-unsafe code. Finally the transaction is committed. If this succeeds the function returns the old pointer to the head; otherwise we delay and retry.  The one tricky part of this code is to remember that the <code>VALIDATE</code> operation aborts the transaction if it has already failed.  The next transactional memory operation would start a new transaction and, therefore, we must skip over the rest of the code in the function.</p>
<p>How the delay code works will be something to see when implementations of transactional memory are available in hardware.  If this is done badly system performance might suffer significantly.</p>
<p><strong>8.2.4 Bus Protocol for Transactional Memory</strong></p>
<p>Now that we have seen the basic principles behind transactional memory, we can dive into the details of the implementation.  Note that this is <em>not</em> based on actual hardware.  It is based on the original design of transactional memory and knowledge about the cache coherency protocol. Some details are omitted, but it still should be possible to get insight into the performance characteristics.</p>
<p>Transactional memory is not actually implemented as separate memory; that would not make any sense given that transactions on any location in a thread's address space are wanted. Instead, it is implemented at the first cache level. The implementation could, in theory, happen in the normal L1d but, as [transactmem] points out, this is not a good idea.  We will more likely see the transaction cache implemented in parallel to L1d. All accesses will use the higher level cache in the same way they use L1d. The transaction cache is likely much smaller than L1d.  If it is fully associative its size is determined by the number of operations a transaction can comprise.  Implementations will likely have limits for the architecture and/or specific processor version.  One could easily imagine a transaction cache with 16 elements or even less.  In the above example we only needed one single memory location; algorithms with a larger transaction working sets get very complicated. It is possible that we will see processors which support more than one active transaction at any one time. The number of elements in the cache then multiplies, but it is still small enough to be fully associative.</p>
<p>The transaction cache and L1d are exclusive.  That means a cache line is in, at most, one of the caches but never in both. Each slot in the transaction cache is in, at any one time, one of the four MESI protocol states. In addition to this, a slot has a transaction state. The states are as follows (names according to [transactmem]):</p>
<ul>
<li>
<p><strong>EMPTY</strong> </p>
<p>the cache slot contains no data.  The MESI state is  always 'I'.</p>
</li>
<li>
<p><strong>NORMAL</strong></p>
<p>the cache slot contains committed data.  The data could  as well exist in L1d.  The MESI state can be 'M', 'E', and 'S'.  The  fact that the 'M' state is allowed means that transaction commits do  <em>not</em> force the data to be written into the main memory (unless  the memory region is declared as uncached or write-through).  This  can significantly help to increase performance.</p>
</li>
<li>
<p><strong>XABORT</strong></p>
<p>the cache slot contains data which is discarded on  abort.  This is obviously the opposite of XCOMMIT.  All the data  created during a transaction is kept in the transaction cache,  nothing is written to main memory before a commit.  This limits the  maximum transaction size but it means that, beside the transaction  cache, no other memory has to be aware of the XCOMMIT/XABORT duality  for a single memory location.  The possible MESI states are 'M',  'E', and 'S'.</p>
</li>
<li>
<p><strong>XCOMMIT</strong></p>
<p>the cache slot contains data which is discarded on  commit. This is a possible optimization processors could implement.  If a memory location is changed using a transaction operation, the  old content cannot be just dropped: if the transaction fails the old  content needs to be restored.  The MESI states are the same as for  XABORT. One difference with regard to XABORT is that, if the transaction  cache is full, any XCOMMIT entries in the 'M' state could be written back to memory  and then, for all states, discarded.</p>
</li>
</ul>
<p>When an <code>LT</code> operation is started, the processor allocates two slots in the cache.  Victims are chosen by first looking for NORMAL slots for the address of the operation, i.e., a cache hit.  If such an entry is found, a second slot is located, the value copied, one entry is marked XABORT, and the other one is marked XCOMMIT.</p>
<p>If the address is not already cached, EMPTY cache slots are located. If none can be found, NORMAL slots are looked for. The old content must then be flushed to memory if the MESI state is 'M'. If no NORMAL slot is available either, it is possible to victimize XCOMMIT entries. This is likely going to be an implementation detail, though.  The maximum size of a transaction is determined by the size of the transaction cache, and, since the number of slots which are needed for each operation in the transaction is fixed, the number of transactions can be capped before having to evict XCOMMIT entries.</p>
<p>If the address is not found in the transactional cache, a T_READ request is issued on the bus.  This is just like the normal READ bus request, but it indicates that this is for the transactional cache. Just like for the normal READ request, the caches in all other processors first get the chance to respond.  If none does the value is read from the main memory.  The MESI protocol determines whether the state of the new cache line is 'E' or 'S'.  The difference between T_READ and READ comes into play when the cache line is currently in use by an active transaction on another processor or core.  In this case the T_READ operation plainly fails, no data is transmitted.  The transaction which generated the T_READ bus request is marked as failed and the value used in the operation (usually a simple register load) is undefined. Looking back to the example, we can see that this behavior does not cause problems if the transactional memory operations are used correctly.  Before a value loaded in a transaction is used, it must be verified with <code>VALIDATE</code>. This is, in almost no cases, an extra burden. As we have seen in the attempts to create a FIFO implementation using atomic operations, the check which we added is the one missing feature which would make the lock-free code work.</p>
<p>The <code>LTX</code> operation is almost identical to <code>LT</code>.  The one difference is that the bus operation is T_RFO instead of T_READ. T_RFO, like the normal RFO bus request, requests exclusive ownership of the cache line. The state of the resulting cache line is 'E'. Like the T_READ bus request, T_RFO can fail, in which case the used value is undefined, too.  If the cache line is already in the local transaction cache with 'M' or 'E' state, nothing has to be done.  If the state in the local transaction cache is 'S' the bus request has to go out to invalidate all other copies.</p>
<p>The <code>ST</code> operation is similar to <code>LTX</code>.  The value is first made available exclusively in the local transaction cache.  Then the <code>ST</code> operation makes a copy of the value into a second slot in the cache and marks the entry as XCOMMIT.  Lastly, the other slot is marked as XABORT and the new value is written into it.  If the transaction is already aborted, or is newly aborted because the implicit <code>LTX</code> fails, nothing is written.</p>
<p>Neither the <code>VALIDATE</code> nor <code>COMMIT</code> operations automatically and implicitly create bus operations.  This is the huge advantage transactional memory has over atomic operations. With atomic operations, concurrency is made possible by writing changed values back into main memory. If you have read this document thus far, you should know how expensive this is. With transactional memory, no accesses to the main memory are forced.  If the cache has no EMPTY slots, current content must be evicted, and for slots in the 'M' state, the content must be written to main memory. This is not different from regular caches, and the write-back can be performed without special atomicity guarantees. If the cache size is sufficient, the content can survive for a long time.  If transactions are performed on the same memory location over and over again, the speed improvements can be astronomical since, in the one case, we have one or two main memory accesses in each round while, for transactional memory, all accesses hit the transactional cache, which is as fast as L1d.</p>
<p>All the <code>VALIDATE</code> and <code>COMMIT</code> operations do for an aborted transaction is to mark the cache slots marked XABORT as empty and mark the XCOMMIT slots as NORMAL. Similarly, when <code>COMMIT</code> successfully finishes a transaction, the XCOMMIT slots are marked empty and the XABORT slots are marked NORMAL. These are very fast operations on the transaction cache. No explicit notification to other processors which want to perform transactions happens; those processors just have to keep trying.  Doing this efficiently is another matter.  In the example code above we simply have <code>...delay...</code> in the appropriate place.  We might see actual processor support for delaying in a useful way.</p>
<p>To summarize, transactional memory operations cause bus operation only when a new transaction is started and when a new cache line, which is not already in the transaction cache, is added to a still-successful transaction.  Operations in aborted transactions do not cause bus operations.  There will be no cache line ping-pong due to multiple threads trying to use the same memory.</p>
<p><strong>8.2.5 Other Considerations</strong></p>
<p>In Section 6.4.2, we already discussed how the <code>lock</code> prefix, available on x86 and x86-64, can be used to avoid the coding of atomic operations in some situations.  The proposed tricks falls short, though, when there are multiple threads in use which do not contend for the same memory. In this case, the atomic operations are used unnecessarily. With transactional memory this problem goes away. The expensive RFO bus requests are issued only if memory is used on different CPUs concurrently or in succession; this is only the case when they are needed. It is almost impossible to do any better.</p>
<p>The attentive reader might have wondered about delays. What is the expected worst case scenario? What if the thread with the active transaction is descheduled, or if it receives a signal and is possibly terminated, or decides to use <code>siglongjmp</code> to jump to an outer scope? The answer to this is: the transaction will be aborted. It is possible to abort a transaction whenever a thread makes a system call or receives a signal (i.e., a ring level change occurs).  It might also be that aborting the transaction is part of the OS's duties when performing system calls or handling signals.  We will have to wait until implementations become available to see what is actually done.</p>
<p>The final aspect of transactional memory which should be discussed here is something which people might want to think about even today. The transaction cache, like other caches, operates on cache lines. Since the transaction cache is an exclusive cache, using the same cache line for transactions and non-transaction operation will be a problem.  It is therefore important to</p>
<ul>
<li>
<p>move non-transactional data off of the cache line</p>
</li>
<li>
<p>have separate cache lines for data used in separate transactions</p>
</li>
</ul>
<p>The first point is not new, the same effort will pay off for atomic operations today. The second is more problematic since today objects are hardly ever aligned to cache lines due to the associated high cost. If the data used, along with the words modified using atomic operations, is on the same cache line, one less cache line is needed.  This does not apply to mutual exclusion (where the mutex object should always have its own cache line), but one can certainly make cases where atomic operations go together with other data.  With transactional memory, using the cache line for two purposes will most likely be fatal.  Every normal access to data {<em>From the cache line in question.  Access to arbitrary other cache lines does not influence the transaction.</em>} would remove the cache line from the transactional cache, aborting the transaction in the process. Cache alignment of data objects will be in future not only a matter of performance but also of correctness.</p>
<p>It is possible that transactional memory implementations will use more precise accounting and will, as a result, not suffer from normal accesses to data on cache lines which are part of a transaction.  This requires a lot more effort, though, since then the MESI protocol information is not sufficient anymore.</p>
<h3 id="83-increasing-latency"><a class="header" href="#83-increasing-latency">8.3 Increasing Latency</a></h3>
<p>One thing about future development of memory technology is almost certain: latency will continue to creep up. We already discussed, in Section 2.2.4, that the upcoming DDR3 memory technology will have higher latency than the current DDR2 technology.  FB-DRAM, if it should get deployed, also has potentially higher latency, especially when FB-DRAM modules are daisy-chained.  Passing through the requests and results does not come for free.</p>
<p>The second source of latency is the increasing use of NUMA.  AMD's Opterons are NUMA machines if they have more than one processor. There is some local memory attached to the CPU with its own memory controller but, on SMP motherboards, the rest of the memory has to be accessed through the Hypertransport bus.  Intel's CSI technology will use almost the same technology.  Due to per-processor bandwidth limitations and the requirement to keep (for instance) multiple 10Gb/s Ethernet ports busy, multi-socket motherboards will not vanish, even if the number of cores per socket increases.</p>
<p>A third source of latency are co-processors.  We thought that we got rid of them after math co-processors for commodity processors were no longer necessary at the beginning of the 1990's, but they are making a comeback.  Intel's Geneseo and AMD's Torrenza are extensions of the platform to allow third-party hardware developers to integrate their products into the motherboards.  I.e., the co-processors will not have to sit on a PCIe card but, instead, are positioned much closer to the CPU.  This gives them more bandwidth.</p>
<p>IBM went a different route (although extensions like Intel's and AMD's are still possible) with the Cell CPU.  The Cell CPU consists, beside the PowerPC core, of 8 Synergistic Processing Units (SPUs) which are specialized processors mainly for floating-point computation.</p>
<p>What co-processors and SPUs have in common is that they, most likely, have even slower memory logic than the real processors. This is, in part, caused by the necessary simplification: all the cache handling, prefetching etc is complicated, especially when cache coherency is needed, too.  High-performance programs will increasingly rely on co-processors since the performance differences can be dramatic. Theoretical peak performance for a Cell CPU is 210 GFLOPS, compared to 50-60 GFLOPS for a high-end CPU.  The Graphics Processing Units (GPUs, processors on graphics cards) in use today achieve even higher numbers (north of 500 GFLOPS) and those could probably, with not too much effort, be integrated into the Geneseo/Torrenza systems.</p>
<p>As a result of all these developments, a programmer must conclude that prefetching will become ever more important.  For co-processors it will be absolutely critical.  For CPUs, especially with more and more cores, it is necessary to keep the FSB busy all the time instead of piling on the requests in batches.  This requires giving the CPU as much insight into future traffic as possible through the efficient use of prefetching instructions.</p>
<h3 id="84-vector-operations"><a class="header" href="#84-vector-operations">8.4 Vector Operations</a></h3>
<p>The multi-media extensions in today's mainstream processors implement vector operations only in a limited fashion.  Vector instructions are characterized by large numbers of operations which are performed together. Compared with scalar operations, this can be said about the multi-media instructions, but it is a far cry from what vector computers like the Cray-1 or vector units for machines like the IBM 3090 did.</p>
<p>To compensate for the limited number of operations performed for one instruction (four <code>float</code> or two <code>double</code> operations on most machines) the surrounding loops have to be executed more often.  The example in Section 9.1 shows this clearly, each cache line requires <code>SM</code> iterations.</p>
<p>With wider vector registers and operations, the number of loop iterations can be reduced. This results in more than just improvements in the instruction decoding etc.; here we are more interested in the memory effects.  With a single instruction loading or storing more data, the processor has a better picture about the memory use of the application and does not have to try to piece together the information from the behavior of individual instructions.  Furthermore, it becomes more useful to provide load or store instructions which do not affect the caches. With 16 byte wide loads of an SSE register in an x86 CPU, it is a bad idea to use uncached loads since later accesses to the same cache line have to load the data from memory again (in case of cache misses). If, on the other hand, the vector registers are wide enough to hold one or more cache lines, uncached loads or stores do not have negative impacts.  It becomes more practical to perform operations on data sets which do not fit into the caches.</p>
<p>Having large vector registers does not necessarily mean the latency of the instructions is increased; vector instructions do not have to wait until all data is read or stored.  The vector units could start with the data which has already been read if it can recognize the code flow. That means, if, for instance, a vector register is to be loaded and then all vector elements multiplied by a scalar, the CPU could start the multiplication operation as soon as the first part of the vector has been loaded.  It is just a matter of sophistication of the vector unit. What this shows is that, in theory, the vector registers can grow really wide, and that programs could potentially be designed today with this in mind. In practice, there are limitations imposed on the vector register size by the fact that the processors are used in multi-process and multi-thread OSes.  As a result, the context switch times, which include storing and loading register values, is important.</p>
<p>With wider vector registers there is the problem that the input and output data of the operations cannot be sequentially laid out in memory.  This might be because a matrix is sparse, a matrix is accessed by columns instead of rows, and many other factors.  Vector units provide, for this case, ways to access memory in non-sequential patterns.  A single vector load or store can be parametrized and instructed to load data from many different places in the address space.  Using today's multi-media instructions, this is not possible at all.  The values would have to be explicitly loaded one by one and then painstakingly combined into one vector register.</p>
<p>The vector units of the old days had different modes to allow the most useful access patterns:</p>
<ul>
<li>
<p>using striding, the program can specify how big the gap  between two neighboring vector elements is.  The gap between all  elements must be the same but this would, for instance, easily allow  to read the column of a matrix into a vector register in one  instruction instead of one instruction per row.</p>
</li>
<li>
<p>using indirection, arbitrary access patterns can be created.  The  load or store instruction would receive a pointer to an array which  contains addresses or offsets of the real memory locations which  have to be loaded.</p>
</li>
</ul>
<p>It is unclear at this point whether we will see a revival of true vector operations in future versions of mainstream processors.  Maybe this work will be relegated to co-processors.  In any case, should we get access to vector operations, it is all the more important to correctly organize the code performing such operations.  The code should be self-contained and replaceable, and the interface should be general enough to efficiently apply vector operations.  For instance, interfaces should allow adding entire matrixes instead of operating on rows, columns, or even groups of elements.  The larger the building blocks, the better the chance of using vector operations.</p>
<p>In [vectorops] the authors make a passionate plea for the revival of vector operations. They point out many advantages and try to debunk various myths. They paint an overly simplistic image, though. As mentioned above, large register sets mean high context switch times, which have to be avoided in general purpose OSes. See the problems of the IA-64 processor when it comes to context switch-intensive operations.  The long execution time for vector operations is also a problem if interrupts are involved.  If an interrupt is raised, the processor must stop its current work and start working on handling the interrupt. After that, it must resume executing the interrupted code.  It is generally a big problem to interrupt an instruction in the middle of the work; it is not impossible, but it is complicated. For long running instructions this has to happen, or the instructions must be implemented in a restartable fashion, since otherwise the interrupt reaction time is too high.  The latter is not acceptable.</p>
<p>Vector units also were forgiving as far as alignment of the memory access is concerned, which shaped the algorithms which were developed. Some of today's processors (especially RISC processors) require strict alignment so the extension to full vector operations is not trivial.  There are big potential upsides to having vector operations, especially when striding and indirection are supported, so that we can hope to see this functionality in the future.</p>
<h2 id="appendices-and-bibliography"><a class="header" href="#appendices-and-bibliography">Appendices and bibliography</a></h2>
<p>The  <a href="https://lwn.net/Articles/258188/">appendices and bibliography page</a> contains, among other things, the source code for a number of the benchmark programs used for this document, more information on oprofile, some discussion of memory types, an introduction to libNUMA, and the bibliography</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
